$(document).ready(function () {indexDict['en'] = [{ "title" : "Overview", 
"url" : "overview.html", 
"breadcrumbs" : "Unravel 4.5 \/ Overview", 
"snippet" : "Unravel provides full stack coverage and a unified, end-to-end view of everything going on in your environment. Unravel helps you to understand and optimize performance across every application and business unit both in the cloud and on-premises. It provides visibility across the entire data stack, ...", 
"body" : "Unravel provides full stack coverage and a unified, end-to-end view of everything going on in your environment. Unravel helps you to understand and optimize performance across every application and business unit both in the cloud and on-premises. It provides visibility across the entire data stack, collecting data from every pipeline, every job, wherever those workloads are running. Unravel creates a correlated data model that provides the full context you need on your apps and resources to properly plan, manage, and improve performance. Unravel enables you to: \n \n Uncover \n \n Understand \n \n Unravel Unravel goes beyond raw visibility to provide concrete, providing AI-driven advice such as: Code you can use. Specific settings you can tweak. Recommendations you can immediately implement or automate to fix issues and optimize performance. The Unravel Data Operations Platform helps operations engineers, application developers, and enterprise architects reduce the complexity of delivering reliable application performance – providing unified visibility and operational intelligence to optimize your entire ecosystem. In summary, problems solved by Unravel: APM for Big Data AI for DataOps Cloud Migration Resource and Cost Optimization Troubleshooting and Root Cause Analysis " }, 
{ "title" : "Where Does Unravel Reside", 
"url" : "overview.html#UUID-98969c33-8b3d-d5d9-2229-78ffd5600eae_id_Overview-WhereDoesUnravelReside", 
"breadcrumbs" : "Unravel 4.5 \/ Overview \/ Where Does Unravel Reside", 
"snippet" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less. Once deployed, Unravel Server begins to collect information from relevant services ...", 
"body" : "Unravel Server is positioned in a Hadoop cluster on its own Gateway\/Edge node to communicate with HDFS and TaskTracker nodes, and to enable client access to Unravel Web UI. A basic deployment takes two hours or less. Once deployed, Unravel Server begins to collect information from relevant services like YARN, Hive, Oozie, Spark, Pig, and MapReduce jobs via HDFS. Unravel analyzes this information and store its analyses in its database. Unravel Web UI displays information from the database, as well as real-time event streams it receives directly from Unravel Server. After basic deployment, advanced setup steps are optional, but may include: Deploying Unravel Sensors for Hive and Spark, which can push additional metadata to Unravel Server. In YARN, enabling log aggregation. In Oozie, pushing information via the Oozie REST API. Installing OnDemand for report generation. " }, 
{ "title" : "Features", 
"url" : "overview.html#UUID-98969c33-8b3d-d5d9-2229-78ffd5600eae_id_Overview-Features", 
"breadcrumbs" : "Unravel 4.5 \/ Overview \/ Features", 
"snippet" : "Unravel provides a unified \"single pane of glass\" for the monitoring and management of your cluster whether on-premises, running in the cloud, or a hybrid of both. Dashboards Application Program Manager (APM) Auto-Actions & Alerts Reporting Insights and Recommendations Support Role Based Access Cont...", 
"body" : "Unravel provides a unified \"single pane of glass\" for the monitoring and management of your cluster whether on-premises, running in the cloud, or a hybrid of both. \n Dashboards \n Application Program Manager (APM) \n Auto-Actions & Alerts \n Reporting \n Insights and Recommendations \n Support Role Based Access Control See here " }, 
{ "title" : "Installation Guides", 
"url" : "install.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides", 
"snippet" : "Cloudera Hortonworks Amazon Elastic MapReduce (EMR) Amazon Athena MapR Azure HDInsight MySQL OnDemand...", 
"body" : " Cloudera Hortonworks Amazon Elastic MapReduce (EMR) Amazon Athena MapR Azure HDInsight MySQL OnDemand " }, 
{ "title" : "Cloudera", 
"url" : "install/install-cdh.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Cloudera", 
"snippet" : "This section explains how to deploy Unravel Server and Sensors on the Cloudera distribution of Apache Hadoop (CDH), with Cloudera Manager (CM). The following reports require that you install the OnDemand service: Sessions Cluster Optimization Capacity Forecasting Small Files For installation instruc...", 
"body" : "This section explains how to deploy Unravel Server and Sensors on the Cloudera distribution of Apache Hadoop (CDH), with Cloudera Manager (CM). The following reports require that you install the OnDemand service: Sessions Cluster Optimization Capacity Forecasting Small Files For installation instructions, see OnDemand " }, 
{ "title" : "Cloudera Pre-Installation Check", 
"url" : "install/install-cdh/install-cdh-pre.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Cloudera \/ Cloudera Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility On-premises CDH 6.0, 5.15, 5.14 Hadoop 1.x - 2.x Kafka 0.9, 0.10, 0.11, and 1.0 (Apache version equivalent) Kerberos Hive versions 0.10.0 through 1.2.x Spark versions 1.3, 1.5, 1.6, 2...", 
"body" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility On-premises CDH 6.0, 5.15, 5.14 Hadoop 1.x - 2.x Kafka 0.9, 0.10, 0.11, and 1.0 (Apache version equivalent) Kerberos Hive versions 0.10.0 through 1.2.x Spark versions 1.3, 1.5, 1.6, 2.0 Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum, 128GB for medium volume (>25,000 jobs per day), 256GB for high volume (>50,000 jobs per day) Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 60,000+ MR jobs per day, two or more gateway\/edge nodes are recommended (\"multi-host Unravel\") Make sure vm.max_map_count=262144 Software Operating System: RedHat\/Centos 6.6-7.5 If you're running Red Hat Enterprise Linux (RHEL) 6.x, you must set the following property in your elasticsearch.yml boostrap.system_call_filter: false Reference: https:\/\/github.com\/elastic\/elasticsearch\/issues\/22899 installed libaio.x86_64 (or disabled) should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN+Spark client\/gateway, Hadoop and Hive commands in PATH If Spark2 service is installed, Unravel host should be a client\/gateway LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication (Open signup by default) Zookeeper is not NTP should be running and in-sync with the cluster MySQL: 5.5, 5.6, 5.7 (recommended) MySQL driver (connector): 5.1.47 Access Permissions Local user unravel:unravel is created during installation, but can be changed later If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to: YARN's \"done\" directory in HDFS YARN's log aggregation directory in HDFS Spark event log directory in HDFS File sizes under Hive warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) For Impala support, a read-only account for Cloudera Manager API is needed Network The following ports must be open on the Unravel edge node: Port(s) Direction Description 3000 Both Non- HTTPS traffic to and from Unravel UI If you plan to use Cloudera Manager to install Unravel Sensors, the Cloudera Manager service must also be able to reach the Unravel edge node on port 3000. 3003, 3005 Both HTTPS traffic to and from Unravel UI. Open these ports if localhost 3316 Both Database traffic 4020 Both Unravel APIs 4021 Both Host monitoring of JMX on localhost 4031 Both Database traffic 4043 In UDP and TCP ingest traffic from the entire cluster to Unravel Server(s) 4044-4049 In UDP and TCP ingest spares for unravel_lr* 4091-4099 Both Kafka brokers 4171-4174, 4176-4179 Both ElasticSearch; localhost communication between Unravel daemons or Unravel Servers (if multi-host Unravel deployment) 4181-4189 Both Zookeeper daemons 4210 Both Cluster access service HDFS ports In Traffic from the cluster to Unravel Server(s) Hive Metadata DB port Out For YARN only. Traffic from Hive to Unravel Server(s) for partition reporting 7180 (or 7183 for HTTPS) Out Traffic from Unravel Server(s) to Cloudera Manager 8032 Out Traffic from Unravel Server(s) to the Resource Manager (RM) server(s) 8188 Out Traffic from Unravel Server(s) to the ATS server(s) 11000 Out For Oozie only. Traffic from Unravel Server(s) to the Oozie server " }, 
{ "title" : "Step 1: Install Unravel Server on CDH+CM", 
"url" : "install/install-cdh/install-cdh-part1.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Cloudera \/ Step 1: Install Unravel Server on CDH+CM", 
"snippet" : "This topic explains how to deploy Unravel Server 4.4 on a CDH gateway\/edge node. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Configure the Host Machine Allocate a cluster gateway\/edge\/client host with HDFS access. Use Cloudera Manager to create the ...", 
"body" : "This topic explains how to deploy Unravel Server 4.4 on a CDH gateway\/edge node. For instructions that correspond to older versions of Unravel Server, contact Unravel Support. Configure the Host Machine Allocate a cluster gateway\/edge\/client host with HDFS access. Use Cloudera Manager to create the gateway configuration for the Unravel server(s) that has client roles for HDFS, YARN, Spark, Hive, and optionally, Spark2. Install the Unravel Server RPM on the Host Machine Get the Unravel Server RPM. See Download Unravel Software Make symlinks if required. If you want the two disk areas used by Unravel to be on different volumes, you can make symlinks to specific areas before installing (or do a mv symlink \/usr\/local\/unravel \/srv\/unravel Install the Unravel Server RPM. # sudo rpm -U unravel-4.*.x86_64.rpm*\n# \/usr\/local\/unravel\/install_bin\/await_fixups.sh The precise filename can vary, depending on how it was fetched or copied. The rpm rpm -U Run the specified await_fixups.sh await_fixups.sh DONE The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh externally managed MySQL Do host-specific post-installation actions. For CDH, there are no host-specific post-installation actions. Update site-specific properties in \/usr\/local\/unravel\/etc\/unravel.properties \/\/ Repoint Unravel application logs directory\ncom.unraveldata.job.collector.done.log.base=\/mr-history\/done\ncom.unraveldata.job.collector.log.aggregation.base=\/app-logs\/*\/logs\/\ncom.unraveldata.spark.eventlog.location=hdfs:\/\/\/spark-history\/ Configure Unravel Server (Basic\/Core Optional for CDH) Enable optional daemons. Depending on your workload volume or kind of activity, you can enable optional daemons at this point. For more information, see Creating Multiple Workers for High Volume Data If Kerberos is enabled, add authentication for HDFS: Create Add properties for Kerberos in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab You can verify the principal in a keytab by using klist -kt KETYAB_FILE unravel Run Unravel Daemons with Custom User If Sentry is enabled, add these permissions: Define your own alt principal with narrow privileges. The alt principal can be unravel rpm X Resource Principal Access Purpose hdfs:\/\/user\/spark\/applicationHistory Your alt principal read Spark event log hdfs:\/\/user\/spark\/spark2ApplicationHistory Your alt principal read Spark 2 event log hdfs:\/\/user\/history Your alt principal read MapReduce logs hdfs:\/\/tmp\/logs Your alt principal read YARN aggregation folder hdfs:\/\/user\/hive\/warehouse Your alt principal read Obtain table partition sizes with \"stat\" only For information on how to configure permissions for unravel with a Sentry enforced cluster, see Configure Permission for Unravel daemons on CDH Sentry Secured Cluster You can find the principal by running the klist -kt KEYTAB_FILE If you are using KMS and HDFS encryption and are using the hdfs principal, you might need to adjust kms-acls.xml If you are using \"JNI\" based groups for HDFS (a setting in CM), you need to add export LD_LIBRARY_PATH=\/opt\/cloudera\/parcels\/CDH\/lib\/hadoop\/lib\/native \/usr\/local\/unravel\/etc\/unravel.ext.sh Switch user. Depending on your cluster security configuration, you will need to run the switch_to_user # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh x y where X Y switch_to_user Configure Hive metastore access. Hive metastore is accessed by Unravel server to analyze table usage in conjunction with Hive job instrumentation. Information is gathered using a Hive API that works very much like beeline connections which leverage the jdbc database connection protocol. As a quick-start approach, you can set Unravel to use the already-defined 'hive' user that is also used by HiveServer2. Alternatively, you can define a read-only metastore database user. If you want a custom user, then do the following steps for the particular kind of database that is used for Hive metastore: Connect to the Hive metastore using the normal conversational interface (mysql or psql, etc.) as an admin that can create new users. Create a user, unravel Grant select on all table in the hive database. As the new user, use the conversational interface (mysql or psql, etc.) from the Unravel server to verify their access. To complete the integration of the Hive metastore with the user, follow the steps in Hive Metastore Access Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60 Run the echo # echo \"http:\/\/( UNRAVEL_HOST_IP This completes the basic\/core configuration. Log into Unravel Web UI Using a web browser, navigate to http:\/\/ UNRAVEL_HOST_IP admin unraveldata For the free trial version, use the Chrome browser. Congratulations! Unravel Server is up and running. Unravel UI displays collected data. For instructions on using Unravel UI, see the User Guide Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Step 2: Install Unravel Sensor and Configure Impala " }, 
{ "title" : "Step 2: Install Unravel Sensor and Configure Impala", 
"url" : "install/install-cdh/install-cdh-part2-impala.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Cloudera \/ Step 2: Install Unravel Sensor and Configure Impala", 
"snippet" : "This topic explains how to install the Unravel Sensor parcel on CDH clusters using Cloudera Manager (CM). The parcel includes Hive Hook and Spark instrumentation JARs. Hive Hook is used to collect information about Hive queries in Hadoop. The Spark instrumentation JARs measure Spark job performance....", 
"body" : "This topic explains how to install the Unravel Sensor parcel on CDH clusters using Cloudera Manager (CM). The parcel includes Hive Hook and Spark instrumentation JARs. Hive Hook is used to collect information about Hive queries in Hadoop. The Spark instrumentation JARs measure Spark job performance. Before following these instructions, follow the steps in Step 1: Install Unravel Server on CDH+CM To Upgrade the Unravel Sensor Check the UNRAVEL_SENSOR If an upgrade is available complete steps 3 through 5 When Active Directory Kerberos is used, UNRAVEL_HOST_IP Obtain and Distribute the Parcel from Unravel Server In Cloudera Manager (CM), go to the Parcels ) on the top of the page. Click Configuration Parcel Settings In the Remote Parcel Repository URLs Parcel Settings + Add http:\/\/{UNRAVEL_HOST_IP}:3000\/parcels\/cdh{X.Y}\/ X.Y UNRAVEL_HOST_IP unravel_lr If you are running more than one version of CDH (multiple clusters) in Cloudera Manager, you can add more than one parcel directory from the UNRAVEL_HOST_IP Click Save Click Check for New Parcels On the Parcels Location In the list of Parcel Names UNRAVEL_SENSOR Download Click Distribute If you have an old parcel from Unravel, deactivate it now. Click Activate Put the Hive Hook JAR in AUX_CLASSPATH In Cloudera Manager, for the target cluster, click Hive Configuration hive-env In Gateway Client Environment Advanced Configuration Snippet (Safety Valve) hive-env.sh AUX_CLASSPATH=${AUX_CLASSPATH}:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar In Cloudera Manager, click YARN Configuration hadoop-env In Gateway Client Environment Advanced Configuration Snippet (Safety Valve) hadoop-env.sh HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar If Sentry is enabled, grant privileges on the JAR files to the Sentry roles that run Hive queries. Sentry commands may also be needed to enable access to the Hive Hook JAR file. Grant privileges on the JAR files to the roles that run hive queries. Log into Beeline as user hive SQL GRANT ROLE # GRANT ALL ON URI 'file:\/\/\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar' TO ROLE ROLE Deploy the Hive Hook Instrumentation Use Cloudera Manager to deploy the hive-site.xml \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip On a multi-host Unravel Server deployment, use the \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip In Cloudera Manager, go to the Hive service. Select the Configuration Search for hive-site.xml Add the snippet to Hive Client Advanced Configuration Snippet for hive-site.xml To edit, click View as XML If cluster has been configured with Cloudera Navigator, the hive.exec.post.hooks com.cloudera.navigator.audit.hive.HiveExecHookContext,org.apache.hadoop.hive.ql.hooks.LineageLogger,com.unraveldata.dataflow.hive.hook.UnravelHiveHook The Hive Client Advanced Configuration Snippet for hive-site.xml Add the snippet to HiveServer2 Advanced Configuration Snippet for hive-site.xml To edit, click View as XML Like the step above, if the hive.exec.post.hooks com.cloudera.navigator.audit.hive.HiveExecHookContext,org.apache.hadoop.hive.ql.hooks.LineageLogger,com.unraveldata.dataflow.hive.hook.UnravelHiveHook The HiveServer2 Advanced Configuration Snippet for hive-site.xml Save the changes with optional comment Unravel snippet in hive-site.xml Deploy the Hive Client configuration by clicking the deploy glyph ( Actions Restart the Hive service. Cloudera Manager will specify a restart which is not necessary for activating these changes. You may act on CM's recommendation at a later time. Check Unravel UI to see if all Hive queries are running. If queries are running fine and appearing in Unravel UI, you are done. If queries are failing with a class not found Undo the hive-site.xml Deploy the hive client configuration. Restart the Hive service. Follow the steps in Troubleshooting Deploy the Spark Instrumentation In Cloudera Manager, select the target cluster, then select the Spark service. Select Configuration Search for spark-defaults In Spark Client Advanced Configuration Snippet (Safety Valve) for spark-conf\/spark-defaults.conf On a multi-host Unravel Server deployment, use host2's FQDN or logical hostname UNRAVEL_HOST_IP Copy the text below and paste it into the Cloudera Manager's Spark Client Advanced Configuration Snippet (Safety Valve) spark-conf\/spark-defaults.conf Modify the value of UNRAVEL_HOST_IP SPARK_VERSION-X.Y has the following possible values: SPARK_VERSION-X.Y for Spark 1.3.x spark-1.3 for Spark 1.5.x spark-1.5 for Spark 1.6.x spark-1.6 for Spark 2.0.x spark-2.0 spark.unravel.server.hostport={UNRAVEL_HOST_IP}:4043 \nspark.driver.extraJavaOptions=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=driver,libs={SPARK_VERSION-X.Y}\nspark.executor.extraJavaOptions=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=executor,libs={SPARK_VERSION-X.Y}\nspark.eventLog.enabled=true Save changes. Deploy the client configuration by clicking the deploy glyph ( Actions Enable Spark Streaming. The Spark Streaming probe is disabled by default and you must enable it manually by editing spark-defaults.conf Search for spark.driver.extraJavaOptions X.Y javaagent:${UNRAVEL_SENSOR_PATH}\/btrace-agent.jar=script=DriverProbe.class:SQLProbe.class:StreamingProbe.class,libs=spark- X.Y Support for Spark apps using the Structured Streaming API introduced in Spark 2 is limited. Check Unravel UI to see if all Spark jobs are running. If jobs are running fine and appearing in Unravel UI, you are done. If queries are failing with a class not found Undo the spark-defaults.conf Deploy the client configuration. Investigate and fix the issue. Follow the steps in Troubleshooting In the case of yarn-client mode applications, the default Spark configuration is not sufficient, because the driver JVM starts before the configuration set through the SparkConf is applied. For more information, see httpssparkapacheorgdocslatestconfigurationhtml\/id_runtime-environment FIXLINK. For instructions on setting up Unravel Sensor for Spark to profile specific Spark applications only (in other words, per-application profiling cluster-wide profiling Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide For instructions, see CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR) Enable Impala APM with CM as the Data Source Configure Unravel Server to retrieve Impala query data from Cloudera Manager (CM) as follows: Add com.unraveldata.data.source=cm \/usr\/local\/unravel\/etc\/unravel.properties Tell Unravel Server some information about your CM: URL, port number, login credentials, and so on. You do this by adding the following properties to \/usr\/local\/unravel\/etc\/unravel.properties Property Description com.unraveldata.cloudera.manager.url CM internal URL. Must start with http:\/\/ com.unraveldata.cloudera.manager.port (Optional) CM port number. You only need to specify this if your Cloudera Manager is not on port 7180. com.unraveldata.cloudera.manager.username CM username com.unraveldata.cloudera.manager.password CM password For example: com.unraveldata.data.source=cm \ncom.unraveldata.cloudera.manager.url=http:\/\/mycm.somewhere.secret \ncom.unraveldata.cloudera.manager.port=9997 \ncom.unraveldata.cloudera.manager.username= mycmname mycmpassword Make sure that the CM user in com.unraveldata.cloudera.manager.username You can verify this by running a curl curl --user mycmname mycmpassword mycmname mycmpassword By default, the ImpalaSensor task is enabled. To disable it, specify the following option in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.sensor.tasks.disabled=iw (Optional) Change the Impala lookback window By default, when Unravel Server starts, it retrieves the last 5 days of Impala queries. To change this, do the following: Open \/usr\/local\/unravel\/etc\/unravel.properties Change com.unraveldata.cloudera.manager.impala.look.back.day For example, to set the lookback to 7 days: com.unraveldata.cloudera.manager.impala.look.back.days=-7 Include a minus sign in front of the new value. Restart the unravel_us (Optional) Advanced Configuration Configure Unravel Server for high volume data: see Creating Multiple Workers for High Volume Data Add LDAP users: see Enabling LDAP Authentication for Unravel Web UI Troubleshooting Symptom Problem Remedy indicates that the directory does not exist hadoop fs -ls \/user\/unravel\/HOOK_RESULT_DIR\/ Unravel Server RPM is not yet installed, or Unravel Server RPM is installed on a different HDFS cluster, or HDFS home directory for Unravel does not exist, or kerberos\/sentry actions are needed Install Unravel RPM on Unravel service host: sudo rpm -U unravel*.rpm* OR Verify that unravel \/user\/unravel\/ error for ClassNotFound com.unraveldata.dataflow.hive.hook.UnravelHiveHook Unravel hive hook JAR was not found in in $HIVE_HOME\/lib\/ Check whether UNRAVEL_SENSOR parcel was distributed and activated in CM. OR Put the Unravel hive-hook JAR corresponding to HIVE_VER JAR_DEST cd \/usr\/local\/unravel\/hive-hook\/;\ncp unravel-hive-HIVE_VER*hook.jar JAR_DEST References For more information on creating permanent functions, see " }, 
{ "title" : "For Oozie, Copy the Hive Hook and BTrace JARs to the HDFS Shared Library Path", 
"url" : "install/install-cdh/install-cdh-part2-impala.html#UUID-b7a067b9-94ce-3425-d6e5-be3a0e3bc0d3_step3", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Cloudera \/ Step 2: Install Unravel Sensor and Configure Impala \/ For Oozie, Copy the Hive Hook and BTrace JARs to the HDFS Shared Library Path", 
"snippet" : "Copy the Hive Hook JAR, \/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar \/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar oozie.libpath...", 
"body" : "Copy the Hive Hook JAR, \/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar \/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar oozie.libpath " }, 
{ "title" : "Hortonworks", 
"url" : "install/install-hdp.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Hortonworks", 
"snippet" : "This section explains how to deploy Unravel Server and Sensors on Hortonworks Data Platform (HDP). The following reports require that you install the OnDemand service: Sessions Cluster Optimization Capacity Forecasting Small Files For installation instructions, see OnDemand...", 
"body" : "This section explains how to deploy Unravel Server and Sensors on Hortonworks Data Platform (HDP). The following reports require that you install the OnDemand service: Sessions Cluster Optimization Capacity Forecasting Small Files For installation instructions, see OnDemand " }, 
{ "title" : "HDP Pre-Installation Check", 
"url" : "install/install-hdp/install-hdp-pre.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Hortonworks \/ HDP Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility On-premises HDP 3.0, 2.6.5, 2.6.4 Hadoop 1.x - 2.x Kafka 0.9-1.0 (Apache version equivalent) Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.2.x Hardware Architecture: x86_64 Cores: 8 RAM...", 
"body" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility On-premises HDP 3.0, 2.6.5, 2.6.4 Hadoop 1.x - 2.x Kafka 0.9-1.0 (Apache version equivalent) Kerberos Hive 0.9.x - 1.2.x Spark 1.3.x - 2.2.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum, 128GB for medium volume (>25,000 jobs per day), 256GB for high volume (>50,000 jobs per day) Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 60,000+ MR jobs per day, two or more gateway\/edge nodes are recommended (\"multi-host Unravel\") Make sure vm.max_map_count=262144 Software Operating System: RedHat\/Centos 6.4 - 7.4 If you're running Red Hat Enterprise Linux (RHEL) 6.x, you must set the following property in your elasticsearch.yml boostrap.system_call_filter: false Reference: https:\/\/github.com\/elastic\/elasticsearch\/issues\/22899 installed libaio.x86_64 (or disabled) should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN+Spark client\/gateway, Hadoop and Hive commands in PATH If Spark is in use, Spark client gateway Verify that all clients are installed by checking that RPMs are installed; Unravel utilizes clients and associated libraries You need to register edge node to Ambari LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication (Open signup by default) Zookeeper is not MySQL: 5.5, 5.6, 5.7 (recommended) MySQL driver (connector): 5.1.47 Access Permissions If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to: YARN's \"done\" directory in HDFS YARN's log aggregation directory in HDFS Spark event log directory in HDFS File sizes under Hive warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active If you plan to use the Auto Actions feature (move \/ kill), you'll need to add the Unravel username to YARN yarn.admin.acl JDBC access to the Hive Metastore (read-only user is sufficient) Application Timeline Server (ATS) read-only Network The following ports must be open on Unravel's host(s): Port(s) Direction Description 3000 Both Non- HTTPS traffic to and from Unravel UI 3003, 3005 Both HTTPS traffic to and from Unravel UI. Open these ports if localhost 3316 Both Database traffic 4020 Both Unravel APIs 4021 Both Host monitoring of JMX on localhost 4031 Both Database traffic 4043 In UDP and TCP ingest traffic from the entire cluster to Unravel Server(s) 4044-4049 In UDP and TCP ingest spares for unravel_lr* 4091-4099 Both Kafka brokers 4171-4174, 4176-4179 Both ElasticSearch; localhost communication between Unravel daemons or Unravel Servers (if multi-host Unravel deployment) 4181-4189 Both Zookeeper daemons 4210 Both Cluster access service HDFS ports In Traffic from the cluster to Unravel Server(s) Hive Metadata DB port Out For YARN only. Traffic from Hive to Unravel Server(s) for partition reporting 7180 (or 7183 for HTTPS) Out Traffic from Unravel Server(s) to Cloudera Manager 8032 Out Traffic from Unravel Server(s) to the Resource Manager (RM) server(s) 8188 Out Traffic from Unravel Server(s) to the ATS server(s) 11000 Out For Oozie only. Traffic from Unravel Server(s) to the Oozie server " }, 
{ "title" : "Step 1: Install Unravel Server on HDP", 
"url" : "install/install-hdp/install-hdp-part1.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Hortonworks \/ Step 1: Install Unravel Server on HDP", 
"snippet" : "This topic explains how to deploy Unravel Server on HDP. For this installation you need to allocate a Cluster Gateway\/Edge\/Client host with HDFS access; use Ambari Web UI to create a gateway node configuration. 1. Configure the Host Machine Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Acces...", 
"body" : "This topic explains how to deploy Unravel Server on HDP. For this installation you need to allocate a Cluster Gateway\/Edge\/Client host with HDFS access; use Ambari Web UI to create a gateway node configuration. 1. Configure the Host Machine Allocate a Cluster Gateway\/Edge\/Client Host with HDFS Access. For HDP, use Ambari Web UI to create a Gateway node configuration. 2. Install the Unravel Server RPM on the Host Machine Get the Unravel Server RPM See Download Unravel Software Make symlinks if required mv symlink \/usr\/local\/unravel \/srv\/unravel Install the Unravel Server RPM # sudo rpm -U unravel-4.*.x86_64.rpm*\n# \/usr\/local\/unravel\/install_bin\/await_fixups.sh The precise filename can vary, depending on how it was fetched or copied. The rpm rpm -U Run the specified await_fixups.sh await_fixups.sh Done The installation creates \/usr\/local\/unravel\/ unravel \/srv\/unravel\/ The master configuration file is in \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ unravel \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh The RPM installation also creates an HDFS directory for Hive Hook information collection. During initial install, a bundled database is used. This can be switched to use an externally managed MySQL for production. (The bundled database root mysql password will be stored in \/root\/unravel.install.include Do Host-Specific Post-Installation Actions For HDP, there are no host-specific post-installation actions. Configure Unravel Server (Basic\/Core Options) Enable optional daemons. Depending on your workload volume or kind of activity, you can enable optional daemons at this point. For more information, see Creating Multiple Workers for High Volume Data Edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Example Values com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. http:\/\/LAN_DNS:3000 com.unraveldata.customer.organization Identifies your installation for reporting purposes. Company_and_org com.unraveldata.tmpdir Location where Unravel's temp file will reside \/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Sets retention for search data. 26 com.unraveldata.job.collector.done.log.base Only modifiable through Unravel Web UI's configuration wizard. \/mr-history\/done com.unraveldata.job.collector.log.aggregation.base Only modifiable through Unravel Web UI's configuration wizard. \/app-logs\/*\/logs\/ com.unraveldata.login.admins Defines the usernames that can access Unravel Web UI's admin pages. Default is admin admin com.unraveldata.s3.batch.monitoring.interval.sec Optional. Defines the monitoring frequency. Default is 300 seconds (5 minutes). Set this property to 60 for lower latency. 120 com.unraveldata.spark.eventlog.location Where to find Spark event logs hdfs:\/\/\/user\/spark\/applicationHistory\/, hdfs:\/\/\/user\/spark\/sparkApplicationHistory\/, hdfs:\/\/\/user\/spark\/spark2ApplicationHistory\/ yarn.resourcemanager.webapp.address YARN resource manager web address URL http:\/\/example.localdomain:8088\" oozie.server.url Oozie URL http:\/\/example.localdomain:11000\/oozie If Kerberos is Enabled, Add Authentication for HDFS: Create or identify a principal and keytab for Unravel daemons to access HDFS and REST when Kerberos is enabled. Add properties for Kerberos in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab You can verify the principal in a keytab by using properties klist -kt KEYTAB_FILE The keytab file should have chmod bits 500 and be owned by unravel Run Unravel Daemons with Custom User If Ranger is Enabled, Add These Permissions: Define your own alt principal with narrow prvileges. The alt user can be unravel rpm X in the switch user section below. Resource Principal Access Purpose hdfs:\/\/spark-history Your alt principal read+execute Spark event log hdfs:\/\/spark2-history Your alt principal read+execute Spark2 event log hdfs:\/\/mr-history\/done Your alt principal read+execute MapReduce logs hdfs:\/\/app-logs Your alt principal read+execute YARN aggregation folder hdfs:\/\/apps\/hive\/warehouse (Default value of hive.metastore.warehouse.dir Your alt principal read+execute Obtain table partition sizes Hive Metastore database GRANT hive read+execute Hive table information You must set yarn properties in Ambari. Log into Ambari and from the dashboard select YARN Configs Advanced Set yarn.acl.enable Add the Unravel user specified in com.unraveldata.kerberos.principal above yarn.admin.acl Save your changes. Disable Impala Sensor Impala is not officially supported on HDP clusters. Therefore you should disable the Impala Sensor by setting\/adding com.unraveldata.sensor.tasks.disabled=iw \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.sensor.tasks.disabled=iw Convert Your Unravel Installation to HDP Run the following commands on Unravel Server. # sudo \/etc\/init.d\/unravel_all.sh stop\n# sudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh This change is persistent through subsequent RPM upgrades; it does not need to be done each time. Switch \"user\" Depending on your cluster security configuration, you will need to run the switch_to_user # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh X Y where X Y switch_to_user Restart Unravel Server. After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties echo UNRAVEL_HOST_IP # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60\n# echo \"http:\/\/$(hostname -f):3000\/\" This completes the basic\/core configuration. Log into Unravel Web UI Using a web browser, navigate to http:\/\/ UNRAVEL_HOST_IP admin unraveldata For the free trial version, use the Chrome browser. Congratulations! Unravel Server is up and running. Unravel UI displays collected data. For instructions on using Unravel UI, see the User Guide Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Step 2: Enable Additional Data Collection \/ Instrumentation for HDP " }, 
{ "title" : "Step 2: Enable Additional Data Collection \/ Instrumentation for HDP", 
"url" : "install/install-hdp/install-hdp-part2.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Hortonworks \/ Step 2: Enable Additional Data Collection \/ Instrumentation for HDP", 
"snippet" : "Table of Contents Introduction 1. Convert Your Unravel Installation to HDP 2. Update Site-Specific HDP properties in \/usr\/local\/unravel\/etc\/unravel.properties 3. Start Unravel Server 4. Install Unravel hive hook and spark sensor onto HDP servers 5. For Oozie, Copy Unravel Hive Hook and BTrace JARs t...", 
"body" : "\n Table of Contents \n Introduction \n 1. Convert Your Unravel Installation to HDP \n 2. Update Site-Specific HDP properties in \/usr\/local\/unravel\/etc\/unravel.properties \n 3. Start Unravel Server \n 4. Install Unravel hive hook and spark sensor onto HDP servers \n 5. For Oozie, Copy Unravel Hive Hook and BTrace JARs to the HDFS Shared Library Path. \n 6. Add Unravel Hive Hook hive-site settings to all of HDP's Servers in the cluster using AWU. \n 7. Optionally for Tez, enable Unravel Tez instrumentation on all of HDP's Services in the cluster. \n 8. Optionally for Spark on YARN \n 9. Optionally for YARN \n 10. Confirm that Unravel Web UI Shows Tez Data Introduction This topic explains how to configure Unravel Sensor for Tez ( unravel_us \n HIGHLIGHTED \n UNRAVEL_HOST_IP 1. Convert Your Unravel Installation to HDP Note: This change remains after RPM upgrades. # sudo \/etc\/init.d\/unravel_all.sh stop\n# sudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh 2. Update Site-Specific HDP properties in \/usr\/local\/unravel\/etc\/unravel.properties Add these properties to\/usr\/local\/unravel\/etc\/unravel.properties \n \n \n Property \n Description \n Default Value \n \n com.unraveldata.yarn.timeline-service.webapp.address \n The http address of the Timeline service web application \n \n http:\/\/localhost \n \n com.unraveldata.yarn.timeline-service.port \n Timeline service port \n 8188 Example \n \n \n \n com.unraveldata.yarn.timeline-service.webapp.address= http:\/\/172.16.1.101 If the Application Timeline Server requires user authentication the following properties need also be specified in: \/usr\/local\/unravel\/etc\/unravel.properties \n \n \n Property \n Description \n Default Value \n \n yarn.ats.webapp.username \n Username required for authentication to the Application Timeline Server (if authentication is required) \n \n yarn.ats.webapp.password \n Password required for authentication to the Application Timeline Server (if authentication is required) Open \/usr\/local\/unravel\/etc\/unravel.properties switch_to_hdp.sh If you are using Spark1 and Spark2 you must com.unraveldata.spark.eventlog.location com.unraveldata.spark.eventlog.location = hdfs:\/\/\/spark2-history\/ \n \n \n \n \/\/ Repoint Unravel application logs directory com.unraveldata.job.collector.done.log.base=\/mr-history\/done com.unraveldata.job.collector.log.aggregation.base=\/app-logs\/*\/logs\/ com.unraveldata.spark.eventlog.location=hdfs:\/\/\/spark-history\/ \n \/\/ Add Hive Metastore database information for Unravel Hive Config javax.jdo.option.ConnectionURL=jdbc:mysql:\/\/UNRAVEL_HOST_IP:3306\/database_name javax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver javax.jdo.option.ConnectionUserName=HiveMetastoreUserName javax.jdo.option.ConnectionPassword=HiveMetastorePassword Log into Ambari Web UI (AWU) to verify the above properties have been set correctly in unravel.properties On the left-hand side of AWU's dashboard, click MapReduce2 Configs Advanced Advanced mapred-site Verify com.unraveldata.job.collector.done.log.base mapreduce.jobhistory.done-dir. On the left-hand side of AWU's dashboard, click YARN Configs Advanced Node Manager Verify com.unraveldata.job.collector.done.log. aggregation.base yarn.nodemanager.remote-app-log-dir On the left-hand side of AWU's dashboard, click Hive Configs Advanced Hive Metastore Verify javax.jdo.option.ConnectionURL Database Host Database URL Verify javax.jdo.option.ConnectionDriverName JDBC Driver Class Verify javax.jdo.option.ConnectionUserName Database Username 3. Start Unravel Server Note Unravel must be up for the next step to complete. # sudo \/etc\/init.d\/unravel_all.sh start 4. Install Unravel hive hook and spark sensor onto HDP servers Install Unravel Hive Hook and Spark Sensor onto HDP servers (JAR files) as follows (substitute the correct fully qualified host name or IP address for UNRAVEL_HOST_IP Login as root wget # yum install -y wget From Unravel server (eg. edge node) run on each server that will use instrumentation. Be sure to substitute valid values for: \n UNRAVEL_HOST_IP \n SPARK_VERSION_X.Y.Z \n HIVE_VERSION # cd \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/\n# sudo python2 unravel_hdp_setup.py --sensor-only --unravel-server {Unravel Host}:3000 --spark-version {SPARK_VERSION} --hive-version {HIVE_VERSION} --ambari-server {Ambari Host} Files are installed locally on the edge node under: \n \/usr\/local\/unravel_client (Hive hook jar) \n \/usr\/local\/unravel-agent\/jars\/ (Resource metrics sensor jars) \n Manually copy host2 5. For Oozie, Copy Unravel Hive Hook and BTrace JARs to the HDFS Shared Library Path. Copy the Hive Hook JAR, \/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar \/usr\/local\/unravel-agent\/jars\/btrace-agent.jar oozie.libpath 6. Add Unravel Hive Hook hive-site settings to all of HDP's Servers in the cluster using AWU. Completion of this step requires a restart of all affected Hive services in Ambari UI. In AWU, on the left-hand side, click Hive Configs Advanced Custom hive-site Add Property Bulk property add mode. \n \n \n \n hive.exec.driver.run.hooks=com.unraveldata.dataflow.hive.hook.UnravelHiveHook com.unraveldata.hive.hdfs.dir=\/user\/unravel\/HOOK_RESULT_DIR com.unraveldata.hive.hook.tcp=true com.unraveldata.host={add unravel gateway internal IP hostname} \n \/\/ Find below properties as it may already exists, concatenate it with a comma & no spaces hive.exec.pre.hooks=com.unraveldata.dataflow.hive.hook.UnravelHiveHook hive.exec.post.hooks=com.unraveldata.dataflow.hive.hook.UnravelHiveHook hive.exec.failure.hooks=com.unraveldata.dataflow.hive.hook.UnravelHiveHook If LLAP is enabled copy the above settings in Custom hive-interactive-site Manual edit hive-site.xml (no AWU) \n hive-site.xml \/etc\/hive\/conf\/ Add AUX_CLASSPATH hive-env In AWU, on the left-hand side, click Hive Configs Advanced Advanced hive-env Next, inside the Advanced hive-env \n \n \n \n export AUX_CLASSPATH=${AUX_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar If LLAP is enabled copy above line of code into Advanced hive-interactive-env You can manually edit hive-env.sh without using AWU. The hive-env.sh \/etc\/hive\/conf\/ Add HADOOP_CLASSPATH hadoop-env In AWU, on the left-hand side, click HDFS Configs Advanced Advanced hadoop-env Next, inside the hadoop-env export HADOOP_CLASSPATH \n \n \n \n export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar 7. Optionally for Tez, enable Unravel Tez instrumentation on all of HDP's Services in the cluster. Completion of this step requires a restart of all affected Hive services in Ambari UI. Confirm that hive-execution.engine is set to tez. \n \n \n set hive.execution.engine=tez; Using the Ambari Web UI (AWU), configure the Btrace agent for Tez: Append the below java options to tez.am.launch.cmd-opts tez.task.launch.cmd-opts -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr,config=tez -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043 Restart the affected component(s). The screenshot below illustrates this change. In a Kerberos environment you need to modify tez.am.view-acls 8. Optionally for Spark on YARN Enable Unravel Spark Instrumentation on All of HDP's Servers in the Cluster Completion of this step requires a restart of all affected Spark services in Ambari UI. Be sure to substitute valid values.for UNRAVEL_HOST_IP SPARK_VERSION_X.Y. \n SPARK_VERSION_X.Y \n spark-1.3 \n \n spark-1.5 \n \n spark-1.6 \n \n spark-2.0 spark- \n \n 2.2 spark- \n \n 2.3 Add Spark properties into AWU's Custom spark-defaults In AWU, on the left-hand side, click Spark Configs Custom spark-defaults You can manually edit spark-defaults.conf without using AWU. The default location for spark-defaults.conf \/usr\/hdp\/current\/SPARK_VERSION_X.Y The cluster only has one spark 1.X version: \/usr\/hdp\/current\/spark-client\/conf For spark 2.X version: \/usr\/hdp\/current\/spark2-client\/conf Inside Custom spark-defaults Add Property Bulk property add mode \n \n \n \n spark.unravel.server.hostport=UNRAVEL_HOST_IP spark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=config=driver,libs=SPARK_VERSION_X.Yspark.executor.extraJavaOptions=-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=config=executor,libs=SPARK_VERSION_X.Yspark.eventLog.enabled=true Enable Spark Streaming The Spark Streaming probe is disabled by default and you must enable it manually by editing spark-defaults.conf Search for spark.driver.extraJavaOptions X.Y. javaagent:${UNRAVEL_SENSOR_PATH}\/btrace-agent.jar=script=DriverProbe.class:SQLProbe.class:StreamingProbe.class,libs=spark-X.Y. Support for Spark apps using the Structured Streaming API introduced in Spark 2 is limited.\\ 9. Optionally for YARN If yarn.acl.enable=true \n yarn.acl.enable=false, \n yarn.admin.acl=userName Unravel only requires yarn.admin.acl 10. Confirm that Unravel Web UI Shows Tez Data Run \/usr\/local\/unravel\/install_bin\/hive_test_simple.sh hive.execution.engine=tez Check Unravel Web UI for Tez data. For instructions, see Tez Application Manager Unravel Web UI may take a few seconds to load Tez data. " }, 
{ "title" : "Amazon Elastic MapReduce (EMR)", 
"url" : "install/install-emr.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR)", 
"snippet" : "The following reports require that you install the OnDemand service: Sessions Cluster Optimization Capacity Forecasting Small Files For installation instructions, see OnDemand...", 
"body" : " The following reports require that you install the OnDemand service: Sessions Cluster Optimization Capacity Forecasting Small Files For installation instructions, see OnDemand " }, 
{ "title" : "Introduction", 
"url" : "install/install-emr/install-emr-intro.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Introduction", 
"snippet" : "Unravel offers full-stack performance management for modern data apps and systems. Use Unravel to monitor, manage, and optimize any modern data stack in the cloud (AWS or Azure) or on-premises (with Cloudera, HDP, or MapR). With full-stack visibility and AI-powered guidance, Unravel helps customers ...", 
"body" : "Unravel offers full-stack performance management for modern data apps and systems. Use Unravel to monitor, manage, and optimize any modern data stack in the cloud (AWS or Azure) or on-premises (with Cloudera, HDP, or MapR). With full-stack visibility and AI-powered guidance, Unravel helps customers to: Run apps more reliably, improve performance (improving SLA) Run apps more efficiently (lowering costs) Detect, troubleshoot and fix issues quickly (lowering MTTR) Dashboards and Reports (usage, chargeback\/show back, resource usage etc.) For a detailed overview, see here This guide is focussed on the Unravel for AWS EMR Unravel deployment involves creating a new EC2 instance and setting up RDS (optionally), installing Unravel Server on the new EC2 instance, configuring Unravel Server, and connecting it to the EMR cluster you want to monitor. You have three options for creating a new EC2 instance: Provision an Unravel EC2 instance using our CloudFormation Template Provision an Unravel EC2 instance using our Amazon Machine Image (AMI) Provision and configure an Unravel EC2 instance manually (RPM based deployment) These options are explained later in this guide. A typical deployment and configuration of Unravel Server takes less than an hour. However, in some cases, it could take a bit longer depending on the complexity of the setup in terms of security\/VPC settings, various permissions setup, and so on. " }, 
{ "title" : "Prerequisites and Requirements", 
"url" : "install/install-emr/install-emr-reqs.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Prerequisites and Requirements", 
"snippet" : "Unravel can monitor, manage, and optimize modern data applications running on AWS EMR. We expect that someone operating the AWS EMR will have the requisite skills to setup Unravel. Specifically, the expectation is that the person deploying and configuring Unravel has sufficient knowledge to be able ...", 
"body" : "Unravel can monitor, manage, and optimize modern data applications running on AWS EMR. We expect that someone operating the AWS EMR will have the requisite skills to setup Unravel. Specifically, the expectation is that the person deploying and configuring Unravel has sufficient knowledge to be able to: Provision EC2 instances, RDS instances Create and configure the required IAM roles, Security groups etc. Working knowledge of AWS networking concepts e.g VPC, subnets etc. Run Ansible scripts, basic Unix commands, AWS CLI To deploy Unravel, you don't need to create any scripts or be familiar with any specific programming\/scripting language. These instructions are self-contained and someone with basic knowledge of AWS should be able to follow them. Expert-level knowledge of AWS isn't required. Since this solution is for monitoring, managing and optimizing modern data applications running on AWS EMR, so the requirement for an AWS account to be in place will implicitly be met. There are no specific requirements for a client operating system to work on the Unravel deployment process. The person deploying just needs to be able to connect to AWS for the deployment process. As for AWS permissions and access needed for the person deploying, they should be able to create EC2 instances, connect to those and install software on them. Create security groups, IAM roles and update those for the EMR cluster and the corresponding S3 storage. In case they want to deploy Unravel for a new EMR cluster, they should also have the required permissions to create an EMR cluster and necessary S3 buckets, create and configure VPC etc. As for licensing, Unravel comes with a 30-day trial license and so the deployment can be completed and the product can be used for 30 days without any additional licenses. The process to obtain licenses post this time period is listed in the Planning Guidance#Costs " }, 
{ "title" : "Architecture", 
"url" : "install/install-emr/install-emr-arch.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Architecture", 
"snippet" : "In order to manage, monitor, and optimize the modern data applications running on a customer’s EMR cluster, Unravel Server needs data corresponding to the EMR cluster as well as about the modern data apps running on the cluster. This information includes metrics, configuration information, and logs....", 
"body" : " In order to manage, monitor, and optimize the modern data applications running on a customer’s EMR cluster, Unravel Server needs data corresponding to the EMR cluster as well as about the modern data apps running on the cluster. This information includes metrics, configuration information, and logs. Some of this data is pushed to Unravel, and some is pulled by the daemons in Unravel Server. In order for this to work, you must allow both inbound and outbound traffic between Unravel Server (on the EC2 instance) and the EMR cluster. For details, see PlanningGuidance\/id_PlanningGuidance-Security FIXLINK. About backups and snapshots: Backing up Unravel amounts to RDS backup (if that is the chosen database) and backing up the state of the Unravel Server. For more information, see OperationalGuidance\/id_OperationalGuidance-BackupandRecovery FILINK " }, 
{ "title" : "Planning Guidance", 
"url" : "install/install-emr/install-emr-planning.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Planning Guidance", 
"snippet" : "Security About settings\/configurations required related to IAM roles, Security Groups etc. Security Aspects related to the Unravel Application itself Unravel (Application) Users - Authentication and Authorization Risk Audit Mechanism Costs Sizing Security Note: here About settings\/configurations req...", 
"body" : " Security About settings\/configurations required related to IAM roles, Security Groups etc. Security Aspects related to the Unravel Application itself Unravel (Application) Users - Authentication and Authorization Risk Audit Mechanism Costs Sizing Security Note: here About settings\/configurations required related to IAM roles, Security Groups etc. In order to manage, monitor and optimize the modern data applications running on a customer’s EMR cluster, Unravel Server needs data corresponding to the EMR cluster as well as about the data apps running on the cluster. This information includes metrics data, configuration information as well as logs. Parts of this data are pushed to Unravel and some are pulled by the daemons in the Unravel server. In order for this to available, there is a need for both inbound and outbound access between the Unravel Server (on the EC2 instance) and the EMR cluster. Unravel server is set up on an EC2 instance and connects to the customer’s EMR cluster. The Unravel EC2 instance must be in the same region with the target EMR clusters which Unravel EC2 node will be monitoring. There are two possible scenarios: Both EMR cluster and Unravel node are created in the same VPC, same subnet; and the security group allows all traffic from the same subnet or The EMR cluster is located on a different VPC than the Unravel server, then you must configure VPC peering, route table creation, and security policy update. Instructions for that are available here The Unravel application has a UI which is what Unravel users connect to and use. It is configured to be available on port 3000. A TCP & UDP connection is needed from the EMR master node to Unravel EC2 node Create a security group that allows port 3000 and port 4043 from EMR cluster nodes IP address (The security group on the Unravel EC2 instance allows traffic via TCP ports 3000 for EMR cluster nodes) or Put the member of security group used on EMR cluster in this rule. The Unravel EC2 instance and EMR cluster(s) allow all outbound traffic. EMR cluster nodes need to allow all traffic from the Unravel node. If it is not possible to Unravel EC2 access all traffic to EMR cluster, you must minimally allow the Unravel EC2 node to access cluster nodes' TCP port 8020, 50010 and 50020. Create a S3 ReadAccess only IAM role and assign it to Unravel EC2 node to READ the archive logs on the S3 bucket configured for the EMR cluster (create an IAM role that contains the policy that can only READ the specific S3 bucket used on EMR cluster. Then create a EC2 instance profile and add the IAM role to it) All the above points are adequately placed and discussed in the context of the appropriate setup and configuration steps. Refer to this this Security Aspects related to the Unravel Application itself As mentioned previously, Unravel UI and API are the user-facing Unravel components of this setup. Instructions for enabling TLS (SSL) for the Unravel UI are shared here Unravel (Application) Users - Authentication and Authorization Users access Unravel via the Unravel UI. There is one default admin account created automatically on installation. Follow this process For integrating LDAP authentication for Unravel UI, follow these steps here Unravel also allows for Role Based Access Control (RBAC) to restrict the views and applications various users can see when they login to the Unravel UI. Steps for setting and configuring RBAC are described here Risk Audit Mechanism The logging of actions done on the Unravel applications is recorded on the logs on the same instance that the Unravel server is deployed on. Specifically, this is the file to look into to find out various actions that have occurred in case you need to trace down an incident: .\/usr\/local\/unravel\/logs\/unravel_ngui.log Costs There are two components of the cost to the user in using Unravel: Cost of AWS Components: Amazon EC2 Pricing Amazon EBS Pricing Amazon RDS Pricing Cost of Unravel license contact Unravel Note: AWS generated tags user-defined tags here Sizing EC2 Instance Specifications: Minimum: r4.2xlarge (61 GiB RAM) Maximum: r4.8xlarge (244 GiB RAM) Recommended: r4.4xlarge (122 GiB RAM) Virtualization type: HVM EBS Volume Specifications: Provision 200 GiB or above with Volume Type = Provisioned IOPS SSD ( io1 The Baseline IOPS (3 IOPS per GiB with a minimum of 100 IOPS, burstable to 3000 IOPS) is sufficient for Unravel (Optional) RDS Specifications (in case the user chooses this): DB instance class: db.r3.xlarge — 4 vCPU, 30.5 GiB RAM Storage type: Provisioned IOPS (SSD) Allocated storage: 200 GiB or above Provisioned IOPS: 1000 Note: AWS Service Limits Virtual Private Cloud (Amazon VPC) Limits Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template " }, 
{ "title" : "Step 1: Provision and Configure an Unravel EC2 Instance", 
"url" : "install/install-emr/install-emr-part1.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 1: Provision and Configure an Unravel EC2 Instance", 
"snippet" : "This section describes how to create a new EC2 instance, install Unravel Server on it, configure it, and connect it to a new or existing EMR cluster. Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template Option 2: Provision an Unravel EC2 Instance from Our Amazon Machine Image...", 
"body" : "This section describes how to create a new EC2 instance, install Unravel Server on it, configure it, and connect it to a new or existing EMR cluster. Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template Option 2: Provision an Unravel EC2 Instance from Our Amazon Machine Image Option 3: Provision and Configure an Unravel EC2 Instance Manually " }, 
{ "title" : "Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template", 
"url" : "install/install-emr/install-emr-part1/install-emr-part1-option1.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 1: Provision and Configure an Unravel EC2 Instance \/ Option 1: Provision an Unravel EC2 Instance from Our CloudFormation Template", 
"snippet" : "This topic explains how to provision an EC2 instance with Unravel Server 4.5.X preinstalled on it, using our CloudFormation template. Requirements Checklist Platform Compatibility EC2 Instance Type Security Group \/ IAM Role AWS Permissions AWS EMR 5.17, 5.18, 5.19 Minimum: R4.2xlarge (61 GiB RAM) Ma...", 
"body" : "This topic explains how to provision an EC2 instance with Unravel Server 4.5.X preinstalled on it, using our CloudFormation template. Requirements Checklist Platform Compatibility EC2 Instance Type Security Group \/ IAM Role AWS Permissions AWS EMR 5.17, 5.18, 5.19 Minimum: R4.2xlarge (61 GiB RAM) Maximum: R4.8xlarge (244 GiB RAM) Recommended: R4.4xlarge (122 GiB RAM) Allowed ports for inbound access to Unravel EC2 node: Port 3000 Port 4043 Unravel EC2 node can access EMR cluster all ports Unravel EC2 node has Read permission on the S3 bucket by EMR clusters. Allowed port for inbound access to EMR master node: Port 8020 (Name node access for spark event log) Allowed port for inbound access to EMR core node: Port 50010 (Data node access for spark event log) Port 50020 AmazonEC2FullAccess IAMFullAccess You need to create an IAM role that has S3 read, write, and list bucket permission for the specific S3 bucket that the EMR cluster will use for logging. AmazonS3FullAccess AmazonVPCFullAccess AmazonSNSReadOnlyAccess AWSMarketplaceManageSubscriptions AWSMarketplaceListBuilds AWSMarketplaceStartBuild AWSCloudFormation: Permissions set as follows {\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Allow\",\n \"Action\": [\n \"cloudformation:*\"\n ],\n \"Resource\": \"*\"\n }\n ]\n} EC2 Instance Settings Name Default Value Description Stack name The deployed application together with its virtual resources is called a CloudFormation \"stack\". We recommend you name this stack UnravelEC2number UnravelEC2date InstanceType r4.4xlarge Instance type for the Unravel EC2 instance. Supported values: r4.2xlarge,r4.4xlarge,r4.8xlarge KeyName mysshkey EC2 key name for SSH access to the Unravel EC2 instance. This name can include uppercase letters, lowercase letters, numbers, dashes, and underscores only and must be 1-64 characters long. TrustedSshIPBlock 10.10.0.0\/16 Trusted IP block for SSH, port 3000, and port 4043 access to the Unravel EC2 instance, in CIDR format (x.x.x.x\/x). UnravelInstanceCount 1 Leave this as 1. UnravelVPCBlock 10.10.0.0\/16 Internal subnet (VPC) IP block for the Unravel EC2 instance, in CIDR format (x.x.x.x\/x). Zone us-east-1a Availability zone for the Unravel EC2 instance. The zone must Create an EC2 Instance from Our CloudFormation Template Download our CloudFormation template from the following public S3 bucket into your local \/tmp folder: https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel_aws_marketspace_01.json Use AWS CLI or the AWS Marketplace console to create the Unravel EC2 instance: From AWS CLI, run the aws cloudformation Syntax: aws cloudformation create-stack --stack-name UnravelEC2 \\ \n--template-body file:\/\/\/tmp\/unravel_aws_marketspace_01.json \\ \n--parameters ParameterKey=Zone,ParameterValue= AWS_ZONE_NAME AWS_INSTANCE_TYPE my_ssh_key AWS_VPC_BLOCK AWS_SSH_BLOCK Example: aws cloudformation create-stack --stack-name UnravelEC2number \\ \n--template-body file:\/\/\/tmp\/unravel_aws_marketspace_01.json \\ \n--parameters ParameterKey=Zone,ParameterValue=us-east-1d \\ \nParameterKey=InstanceType,ParameterValue=r4.2xlarge \\ \nParameterKey=KeyName,ParameterValue=topcat \\ \nParameterKey=UnravelVPCBlock,ParameterValue=10.12.0.0\/16 \\ \nParameterKey=TrustedSshIPBlock, ParameterValue=0.0.0.0\/0 \\ \n--capabilities CAPABILITY_IAM | tee UnravelEC2-stack.json From AWS Marketplace ( https:\/\/aws.amazon.com\/marketplace Type unravel In the search results, select Unravel APM for EMR Continue to Subscribe Click Continue to Configuration On the Configure this software Fulfillment Option Software Version Region Click Continue to Launch On the Launch this software Usage Instructions In the Choose Action Launch CloudFormation Click Launch You are now on the CloudFormation portal ( https:\/\/console.aws.amazon.com\/cloudformation\/home....). In the Choose a template Specify an Amazon S3 template URL Next You are now on the Specify Details On the Specify Details settings for this EC2 instance Next On the Options Next On the Review Create The deployed application together with its virtual resources is called a CloudFormation \"stack\" and is named UnravelEC2number If you don't see your new stack in the list, wait 30-60 seconds and refresh the page. When the status of the stack changes to CREATE_COMPLETE UnravelEC2number Outputs UnravelBackupS3 UnravelAutoScalingGroupId VPC SecurityGroup For example: Get the public IP address and private IP address of this EC2 instance. You can do this through aws CLI or through the EC2 console, ( https:\/\/console.aws.amazon.com\/ec2\/ To use aws CLI, run the ec2 describe-instances UnravelAutoScalingGroupId Outputs AUTOSCALING_GROUP=UnravelAutoScalingGroupId_value_from_Outputs_tab \n aws ec2 describe-instances --filters \"Name=tag:aws:autoscaling:groupName,Values=$AUTOSCALING_GROUP\" |grep PublicIpAddress |head -1 \n aws ec2 describe-instances --filters \"Name=tag:aws:autoscaling:groupName,Values=$AUTOSCALING_GROUP\" |grep PrivateIpAddress |head -1 To use the EC2 console, highlight this EC2 instance and look at its IP addresses in its Description For example: Log into Unravel UI Using a web browser, navigate to http:\/\/ EC2_Public_IP admin unraveldata For the free trial version, use the Chrome browser. Congratulations! Unravel Server is up and running. Proceed to connect to your existing or new EMR clusters " }, 
{ "title" : "Option 2: Provision an Unravel EC2 Instance from Our Amazon Machine Image", 
"url" : "install/install-emr/install-emr-part1/install-emr-part1-option2.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 1: Provision and Configure an Unravel EC2 Instance \/ Option 2: Provision an Unravel EC2 Instance from Our Amazon Machine Image", 
"snippet" : "This topic explains how to provision an EC2 instance with Unravel Server 4.5.X preinstalled on it, using our Amazon Machine Image (AMI). Requirements Checklist Platform Compatibility EC2 Instance Type Security Group \/ IAM Role AWS Permissions AWS EMR 5.17, 5.18, 5.19 Minimum: R4.2xlarge (61 GiB RAM)...", 
"body" : "This topic explains how to provision an EC2 instance with Unravel Server 4.5.X preinstalled on it, using our Amazon Machine Image (AMI). Requirements Checklist Platform Compatibility EC2 Instance Type Security Group \/ IAM Role AWS Permissions AWS EMR 5.17, 5.18, 5.19 Minimum: R4.2xlarge (61 GiB RAM) Maximum: R4.8xlarge (244 GiB RAM) Recommended: R4.4xlarge (122 GiB RAM) Allowed ports for inbound access to Unravel EC2 node: Port 3000 Port 4043 Unravel EC2 node can access EMR cluster all ports Unravel EC2 node has Read permission on the S3 bucket by EMR clusters. AmazonEC2FullAccess IAMFullAccess You need to create an IAM role that has S3 read, write, and list bucket permission for the specific S3 bucket that the EMR cluster will use for logging. AmazonS3FullAccess AmazonVPCFullAccess AmazonSNSReadOnlyAccess AWSMarketplaceManageSubscriptions AWSMarketplaceListBuilds AWSMarketplaceStartBuild AWSCloudFormation: Permissions set as follows {\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n {\n \"Effect\": \"Allow\",\n \"Action\": [\n \"cloudformation:*\"\n ],\n \"Resource\": \"*\"\n }\n ]\n} EC2 Instance Settings Name Default Value Description Stack name The deployed application together with its virtual resources is called a CloudFormation \"stack\". We recommend you name this stack UnravelEC2number UnravelEC2date InstanceType r4.4xlarge Instance type for the Unravel EC2 instance. Supported values: r4.2xlarge,r4.4xlarge,r4.8xlarge KeyName mysshkey EC2 key name for SSH access to the Unravel EC2 instance. This name can include uppercase letters, lowercase letters, numbers, dashes, and underscores only and must be 1-64 characters long. TrustedSshIPBlock 10.10.0.0\/16 Trusted IP block for SSH, port 3000, and port 4043 access to the Unravel EC2 instance, in CIDR format (x.x.x.x\/x). UnravelInstanceCount 1 Leave this as 1. UnravelVPCBlock 10.10.0.0\/16 Internal subnet (VPC) IP block for the Unravel EC2 instance, in CIDR format (x.x.x.x\/x). Zone us-east-1a Availability zone for the Unravel EC2 instance. The zone must Create an EC2 Instance from Our Amazon Machine Image (AMI) Navigate to the EC2 console ( https:\/\/console.aws.amazon.com\/ec2\/ From the menu on the left, select IMAGES AMIs In search box pull-down menu, select Public images Search for ami-08d8b2a645bdc7482 Select this AMI and click Launch On the Choose an Instance Type R4.2xlarge Next: Configure Instance Details (Optional) Modify the configuration of the EC2 instance: On the Configure Instance Details only Network Subnet IAM role Click Next: Add Storage (Optional) Increase the storage capacity of the EC2 instance to a maximum of 500GiB, depending on the number of clusters, the number of jobs running on those clusters, and whether you plan to enable debug logging. Click Next: Add Tags (Optional) Add tags. Click Next: Configure Security Group On the Configure Security Group Rule Type Protocol Port(s) Source Inbound: SSH TCP 22 Your trusted CIDR for SSH access Inbound: Custom TCP TCP 3000 0.0.0.0\/0 or EMR security group ID or EMR subnet IP block Inbound: Custom TCP TCP 4043 EMR security group ID or EMR subnet IP block Outbound: All traffic All All 0.0.0.0\/0 Security policy required for all nodes in the EMR cluster (master, core, and task nodes) All All Unravel EC2 node security group ID or Unravel EC2 node private IP address Security reminder: Don't make Unravel UI accessible on the public Internet. Click Review and Launch Review your settings and click Launch Enter the name of your key pair file and click Launch Instances Verify that you see the following notice: Click the instance. Find the public IP address of the instance in its Description Log into Unravel UI Using a web browser, navigate to http:\/\/ EC2_Public_IP admin unraveldata For the free trial version, use the Chrome browser. Congratulations! Unravel Server is up and running. Proceed to connect to your existing or new EMR clusters " }, 
{ "title" : "Option 3: Provision and Configure an Unravel EC2 Instance Manually", 
"url" : "install/install-emr/install-emr-part1/install-emr-part1-option3.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 1: Provision and Configure an Unravel EC2 Instance \/ Option 3: Provision and Configure an Unravel EC2 Instance Manually", 
"snippet" : "This topic explains how to manually deploy Unravel Server 4.5.X on an AWS EC2 instance. Requirements Checklist Platform Compatibility Base OS for EC2 EC2 Instance Type and Size Ports AWS EMR 5.17, 5.18, 5.19 Base OS Redhat\/CentOS 6.4 - 7.4 Recommended Centos 7.4 AMI. For example, ami-02e98f78 Instan...", 
"body" : "This topic explains how to manually deploy Unravel Server 4.5.X on an AWS EC2 instance. Requirements Checklist Platform Compatibility Base OS for EC2 EC2 Instance Type and Size Ports AWS EMR 5.17, 5.18, 5.19 Base OS Redhat\/CentOS 6.4 - 7.4 Recommended Centos 7.4 AMI. For example, ami-02e98f78 Instance type: Minimum: R4.2xlarge (60 GB Ram) Virtualization type: HVM Virtualization type: HVM Root device type: EBS Root disk space: Minimum: 100GB. Virtualization type: HVM Virtualization type: HVM In a PoC or evaluation, the minimal root disk size 100GB should be sufficient. When monitoring more EMR clusters or lots of jobs, we recommend to set minimal root disk from 300 - 500GB \"Provisioned IOPS\" EBS volume with 3000 IOPS. For production unravel use case, 200GB root disk Provisioned IOPS EBS and RDS are recommended. Allowed ports for inbound access to unravel EC2 node: Port 3000 Port 4043 Unravel EC2 node can access EMR cluster all ports Unravel EC2 node has Read permission on the S3 bucket by EMR clusters. Create an EC2 Instance Base OS: See the table above. EC2 instance's type and size: See the table above. EC2 instance's security group \/ IAM role: See the table above. For instance, create an IAM role that contains the policy that only reads Creating IAM Role Create Instance Profile CLI Base OS: See the table above. Ports: See the table above. Networking: The EC2 instance must Security reminder: Don't make Unravel UI accessible on the public Internet. Security group or policy required for the Unravel EC2 instance: Create an S3 ReadAccess only IAM role and assign it to Unravel EC2 node to read the archive logs on the S3 bucket configured for the EMR cluster. Unravel EC2 node works with multiple EMR clusters, including existing and newly created clusters. A TCP & UDP connection is needed from the EMR master node to Unravel EC2 node. You must Create a security group that allows port 3000 and port 4043 from EMR cluster nodes IP address Put the member of security group used on EMR cluster in this rule. A sample security group used for Unravel EC2 node. Inbound Rule Type Protocol Port Range Source All traffic All All SG ID of this group or Subnet IP block (e.g. 10.10.0.0\/16) SSH TCP 22 0.0.0.0\/0 or trusted public IP for ssh access Custom TCP Rule TCP 3000 SG ID used on EMR cluster or Subnet IP block (if IP block belong to different VPC) is required for VPC peering connection Custom TCP Rule TCP 4043 SG ID used on EMR cluster or Subnet IP block (if IP block belong to different VPC) is required for VPC peering connection Outbound Rule Type Protocol Port Range Source All traffic All All 0.0.0.0\/0 The Unravel EC2 node should have all TCP access to the EMR cluster (master or slave) nodes. You can grant access by inserting a security policy into both SG (security group) of EMR master and slave with (All TCP, All port range) and the source is the SG ID of the unravel VM. (see screen capture below) If it's not possible to Unravel EC2 access All traffic to EMR cluster, you must minimally allow the Unravel EC2 node to access cluster nodes' TCP port 8020, 50010 and 50020. Configure the EC2 Instance at First Login Disable selinux # sudo setenforce Permissive Edit \/etc\/selinux\/config SELINUX=permissive # vi \/etc\/selinux\/config Install libaio.x86_64 lzop.x86_64 # sudo yum install -y libaio.x86_64\n# sudo yum install -y lzop.x86_64 Start ntpd # sudo service ntpd start\n# sudo ntpq -p Create a new user named hadoop # sudo useradd hadoop Install the Unravel RPM on the EC2 Instance Get the Unravel Server RPM; see Download Unravel Software Install the Unravel Server RPM. The precise filename can vary, depending on how it was fetched or copied. # sudo rpm -U unravel-4.5.0.*-EMR-latest.rpm Run the await_fixups.sh In a routine upgrade, it is okay to start all Unravel daemons, but do not stop or restart them until the await_fixups.sh Done # \/usr\/local\/unravel\/install_bin\/await_fixups.sh\n# \/usr\/local\/unravel\/install_bin\/switch_to_user.sh hadoop hadoop Append the following line to \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.onprem=false For monitoring EMR Spark service, add the following properties to unravel.properties com.unraveldata.spark.live.pipeline.enabled=true\ncom.unraveldata.spark.hadoopFsMulti.useFilteredFiles=true\ncom.unraveldata.spark.events.enableCaching=true The installation creates the following items: Virtualization type: HVM User unravel Initial internal database and other durable states in \/srv\/unravel\/ Virtualization type: HVM scripts for controlling services, and \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh Log into Unravel UI Start Unravel daemons. # sudo \/etc\/init.d\/unravel_all.sh start Create an ssh # ssh -i ssh_key.pem centos@ UNRAVEL_HOST_IP Using a web browser, navigate to http:\/\/127.0.0.1:3000 Log into the Unravel UI with username admin unraveldata Congratulations! Unravel Server is up and running. Proceed to connect to your existing or new EMR clusters " }, 
{ "title" : "Step 2: Connect the Unravel EC2 Instance to a New\/Existing EMR Cluster", 
"url" : "install/install-emr/install-emr-part2.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 2: Connect the Unravel EC2 Instance to a New\/Existing EMR Cluster", 
"snippet" : "Table of Contents Introduction Connect the Unravel EC2 Instance to a New EMR Cluster Connect the Unravel EC2 Instance to an Existing EMR Cluster Sanity Check Troubleshooting Introduction This topic explains how to set up and configure your EMR cluster so the Unravel EC2 instance can begin monitoring...", 
"body" : "\n Table of Contents \n Introduction \n Connect the Unravel EC2 Instance to a New EMR Cluster \n Connect the Unravel EC2 Instance to an Existing EMR Cluster \n Sanity Check \n Troubleshooting Introduction This topic explains how to set up and configure your EMR cluster so the Unravel EC2 instance can begin monitoring your jobs running on the cluster. Assumptions The Unravel EC2 instance is created. The Unravel daemon is running. The security group on the Unravel EC2 instance allows traffic via TCP ports 3000 for EMR cluster nodes. The Unravel EC2 instance and EMR cluster(s) allow all outbound traffic. EMR cluster nodes allow all traffic from the Unravel node . Both EMR cluster and Unravel node are created in same VPC, same subnet; and the security group allows all traffic from the same subnet. For existing EMR cluster connection located on a different VPC, you must configure VPC peering, route table creation, and security policy update. For details, see Step 4: Set Up VPC Peering for Unravel EC2 Node (Optional) Network ACL on VPC allows all traffic. Connect the Unravel EC2 Instance to a New EMR Cluster To connect the Unravel EC2 instance to a new EMR cluster, follow the steps below to run the Unravel EMR bootstrap script on all nodes in the cluster. The bootstrap script does the following: On the master node: On Hive clusters, it updates \/etc\/hive\/conf\/hive-site.xml On Spark clusters, it updates \/etc\/spark\/conf\/spark-defaults.conf Updates \/etc\/hadoop\/conf\/mapred-site.xml Updates \/etc\/hadoop\/conf\/yarn-site.xml If TEZ is installed, it updates \/etc\/tez\/conf\/tez-site.xml Installs and starts the unravel_es daemon \/usr\/local\/unravel_es Installs the Spark and MapReduce sensors in \/usr\/local\/unravel-agent Installs the Hive hook sensor in \/usr\/lib\/hive\/lib\/ On all other nodes: Installs the Spark and MapReduce sensors in \/usr\/local\/unravel-agent Installs Hive sensors in \/usr\/lib\/hive\/lib Download the Unravel EMR bootstrap script from https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel_emr_bootstrap.py # curl https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel_emr_bootstrap.py -o \/tmp\/unravel_emr_bootstrap.py Upload the Unravel EMR bootstrap script unravel_emr_bootstrap.py s3:\/\/aws-logs-account_number-region\/elasticmapreduce # aws s3 cp unravel_emr_bootstrap.py s3:\/\/aws-logs-account_number-region\/elasticmapreduce In the AWS console, select the EMR service and click Create cluster In the Create Cluster - Quick Options Go to advanced options . Select Step 1: Software and Steps emr-5.14.0 Release Next. Select Step 2: Hardware Set Network EC2 Subnet If you created the Unravel EC2 node from our CloudFormation template, then a new VPC was generated, named Unravel_VPC. This VPC comes with one configured subnet, and by default has a CIDR \/ network address block of 10.10.0.0\/16 (but you might have changed this during stack creation). If you created the Unravel EC2 node from our Amazon Machine Image (AMI), you must create the EMR cluster on the same VPC and same subnet as the Unravel EC2 node. Modify the instance type and enter the desired instance count for core (slave) node(s). Click \n Next. Select Step 3: General Cluster Settings Cluster name S3 folder Add bootstrap action Custom action Configure and add For details on how to set up your EMR cluster, see https:\/\/docs.aws.amazon.com\/emr\/latest\/ManagementGuide\/emr-gs-launch-sample-cluster.html In the Add Bootstrap Action Script location step 2 Optional arguments Add \n \n \n Sample script location \n \n s3:\/\/aws-logs-account_number-region\/elasticmapreduce \n \n Optional arguments (mandatory here) \n \n --unravel-server UNRAVEL_EC2_IP --bootstrap In the Bootstrap Actions Nex Advanced Options Select Step 4: Security Choose the EC2 key pair Select the EC2 security groups In this example, the security group picked for both Maste Core & Task You must choose the security group that includes the Unravel EC2 instance, otherwise bootstrap will fail. Click Create cluster If everything was entered correctly, your new EMR cluster should finish the bootstrap process and be in the Waiting Once your new EMR cluster is up and running, you can run some jobs and log into the Unravel EC2 node's web UI to see the metrics collected by the Unravel node. Connect the Unravel EC2 Instance to an Existing EMR Cluster To connect the Unravel EC2 instance to an existing EMR cluster, follow the steps below to run the Unravel EMR Ansible playbook either or Substitute your local values for text in {red brackets} Whenever you upgrade Unravel Server, repeat the steps below to upgrade Unravel Sensors as well. Prerequisites Save the private key to access all the EMR nodes somewhere in the master node and change the key's permissions to read-only ( chmod 400 <key> Option 1: Run Our Ansible Playbook on the EMR Master Node Download unravel-emr-ansible.zip $ curl https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel-emr-ansible.zip --output unravel-emr-ansible.zip\n % Total % Received % Xferd Average Speed Time Time Time Current\n Dload Upload Total Spent Left Speed\n100 11708 100 11708 0 0 66541 0 --:--:-- --:--:-- --:--:-- 66902 Unzip unravel-emr-ansible.zip $ unzip unravel-emr-ansible.zip \nArchive: unravel-emr-ansible.zip\n inflating: unravel-emr-ansible\/README.md \n inflating: unravel-emr-ansible\/emr_ansible_inventory \n inflating: unravel-emr-ansible\/emr_ansible_playbook.yaml \n inflating: unravel-emr-ansible\/prepare_inventory.py \n inflating: unravel-emr-ansible\/unravel_emr_bootstrap.py Run prepare_inventory.py Enter the following values either interactively at the prompts or through their command line options: \n --ssh-key path \n --unravel-host hostname \n --cluster-name displayname $ python prepare_inventory.py \nPlease Enter Unravel host IP: 172.31.62.27\nPlease Enter ssh key path: \/home\/hadoop\/id_rsa\n\nAnsible Inventory updated Install Ansible on the EMR master node: $ sudo pip install ansible (Optional) Determine what directory Ansible was installed in, and add that directory to the $PATH variable in ~\/.bashrc $ which ansible\n\/usr\/local\/bin\/ansible In ~\/.bashrc export PATH=\/usr\/local\/bin\/:$PATH Run the Unravel Ansible playbook: $ cd unravel-emr-ansible\n$ ANSIBLE_HOST_KEY_CHECKING=false \n$ ansible-playbook -i emr_ansible_inventory emr_ansible_playbook.yaml\n\nPLAY [nodes] *******************************************************************\n\nTASK [Gathering Facts] *********************************************************\nok: [172.31.109.7]\nok: [172.31.109.251]\nok: [172.31.97.203]\n\nTASK [Run emr bootstrap script] ************************************************\nchanged: [172.31.109.7]\nchanged: [172.31.109.251]\nchanged: [172.31.97.203]\n\nTASK [Check Unravel sensor version] ********************************************\nchanged: [172.31.109.7]\nchanged: [172.31.109.251]\nchanged: [172.31.97.203]\n\nTASK [Print sensor version] ****************************************************\nok: [172.31.109.7] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [172.31.109.251] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [172.31.97.203] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\n\nPLAY RECAP *********************************************************************\n172.31.109.251 : ok=4 changed=2 unreachable=0 failed=0 \n172.31.109.7 : ok=4 changed=2 unreachable=0 failed=0 \n172.31.97.203 : ok=4 changed=2 unreachable=0 failed=0 Option 2: Run Our Ansible Playbook on Your Personal Workstation (Mac or Linux Only) Set up AWS CLI. For instructions, see https:\/\/aws.amazon.com\/cli\/ Make sure AWS CLI has permission to list EMR clusters: $ aws emr list-instances --cluster-id {cluster id} Download unravel-emr-ansible.zip $ wget https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel-emr-ansible.zip Unzip unravel-emr-ansible.zip $ unzip unravel-emr-ansible.zip \nArchive: unravel-emr-ansible.zip\n inflating: unravel-emr-ansible\/README.md \n inflating: unravel-emr-ansible\/emr_ansible_inventory \n inflating: unravel-emr-ansible\/emr_ansible_playbook.yaml \n inflating: unravel-emr-ansible\/prepare_inventory.py \n inflating: unravel-emr-ansible\/unravel_emr_bootstrap.py Run prepare_inventory.py Enter the following values either interactively at the prompts or through their command line options: \n --cluster-id string \n --region string \n --inventory path \n --ssh-key path \n --ssh-user string hadoop \n --unravel-host hostname \n --cluster-name displayname \n --use-public $ python prepare_inventory.py \nPlease Enter Unravel host IP: 172.31.62.27\nPlease Enter ssh key path: \/home\/hadoop\/id_rsa\n\nAnsible Inventory updated Install Ansible: $ sudo pip install ansible Run the Unravel Ansible playbook: $ cd unravel-emr-ansible\n$ ANSIBLE_HOST_KEY_CHECKING=false \n$ ansible-playbook -i emr_ansible_inventory emr_ansible_playbook.yaml\n\nPLAY [nodes] *******************************************************************\n\nTASK [Gathering Facts] *********************************************************\nok: [172.31.109.7]\nok: [172.31.109.251]\nok: [172.31.97.203]\n\nTASK [Run emr bootstrap script] ************************************************\nchanged: [172.31.109.7]\nchanged: [172.31.109.251]\nchanged: [172.31.97.203]\n\nTASK [Check Unravel sensor version] ********************************************\nchanged: [172.31.109.7]\nchanged: [172.31.109.251]\nchanged: [172.31.97.203]\n\nTASK [Print sensor version] ****************************************************\nok: [172.31.109.7] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [172.31.109.251] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [172.31.97.203] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\n\nPLAY RECAP *********************************************************************\n172.31.109.251 : ok=4 changed=2 unreachable=0 failed=0 \n172.31.109.7 : ok=4 changed=2 unreachable=0 failed=0 \n172.31.97.203 : ok=4 changed=2 unreachable=0 failed=0 Sanity Check After you connect the Unravel EC2 instance to your EMR cluster, run some jobs on the EMR cluster and monitor the information displayed in Unravel UI ( http:\/\/unravel_ec2_node_public_IP:3000) Troubleshooting Check Ansible playbook logs in \/tmp\/unravel\/unravel_sensor_ansible.log If the EMR cluster is created in a different VPC, see Step 4 for configuring VPC peering connection for an Unravel node. " }, 
{ "title" : "Step 3: Set Up AWS RDS for Unravel DB (Optional)", 
"url" : "install/install-emr/install-emr-part3.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 3: Set Up AWS RDS for Unravel DB (Optional)", 
"snippet" : "Unravel's default installation uses a bundled database for part of it's storage. For performance and ease of management, using an RDS (MySQL) instead of Unravel's bundled DB is recommended. Configuration Requirements for RDS RDS Security Group: create on VPC of Unravel Server and allow access from U...", 
"body" : "Unravel's default installation uses a bundled database for part of it's storage. For performance and ease of management, using an RDS (MySQL) instead of Unravel's bundled DB is recommended. Configuration Requirements for RDS RDS Security Group: create on VPC of Unravel Server and allow access from Unravel Server security group. Create new DB subnet group Create new DB parameter group Set Up MySQL RDS in AWS Create Unravel RDS instance In AWS portal ? RDS, click Create database Choose MySQL Next Choose Production - MySQL Change the following properties, leave all others as the default. License model DB engine version DB instance class Multi-AZ deployment Storage type Allocated storage Provisioned IOPS Create a new DB instance and Master user and password. Click Next DB Instance identifier Master username Master password In the Advanced Settings . Network & Security Settings Virtual Private Cloud Subnet group Public accessibility Availability zone VPC security group Create a new DB subnet group in advance. It is required for Multi-AZ deployment. The VPC should at least contains two subnets in at least two Availability zones in a given region. For further information please check AWS documentation. Screenshot for DB subnet group. Create a new DB parameter group in advance, and this group is based on mysql 5.5. Alter the parameters base on the custom db parameters. key_buffer_size = 268435456 max_allowed_packet = 33554432 table_open_cache = 256 read_buffer_size = 262144 read_rnd_buffer_size = 4194304 max_connect_errors=2000000000 net-read-timeout = 300 net-write-timeout = 600 open_files_limit=9000 innodb_open_files=9000 character_set_server=utf8 collation_server = utf8_unicode_ci innodb_autoextend_increment=100 innodb_additional_mem_pool_size = 20971520 innodb_log_file_size = 134217728 innodb_log_buffer_size = 33554432 innodb_flush_log_at_trx_commit = 2 innodb_lock_wait_timeout = 50 Database Options Settings Database name Port DB parameter group For other RDS options such as Encryption Backup Monitoring Maintenance Create database Connecting Unravel Node to the Unravel RDS Instance By default, the security group created for the unravel RDS has no network access granted on port 3306 on the subnet connected. You must modify the security group applied on Unravel RDS. Locate the MySQL database endpoint in the RDS dashboard. Look for the security group used for unravel RDS instance from RDS dashboard. Edit the inbound rule of the security group. Add a new rule to allow connections from either Unravel node's Security Group Subnet IP block which unravel node located The SG or IP block works provided the RDS instance is located on the same region as the VPC. Verify the MySQL connection from the Unravel Node. # \/usr\/local\/unravel\/mysql\/bin\/mysql -h unravelmysqlprod.csfw1hkmlpgh.us-east-1.rds.amazonaws.com -u unravel -p Click here to see a sample screenshot. Verify that the database unravel_mysql_prod # CREATE DATABASE IF NOT EXISTS unravel_mysql_prod; Create Unravel db Schema in RDS Unravel Database Stop Unravel Server. # sudo \/etc\/init.d\/unravel_all.sh stop Configure the following properties in unravel.properties # vi \/usr\/local\/unravel\/etc\/unravel.properties Locate and modify the properties below so that they reflect your particular values. If the property isn't found, add it. Use the actual values you set in the above steps, here here Encrypting Passwords in Unravel Properties and Settings unravel.jdbc.username=unravel\nunravel.jdbc.password={unraveldata}\nunravel.jdbc.url=jdbc:mysql:\/\/unravelmysqlprod.csfw1hkmlpgh.us-east-1.rds.amazonaws.com:3306\/unravel_mysql_prod\n Ensure the schema is up to date using the schema upgrade utility provided by Unravel server. The script step connects to the database and applies schema deltas in-order until the schema is up to date. The success or failure of the update is noted. # sudo \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh If table creation privilege is not granted because an internal DBA support group provides the external database, request that they apply the schemas in \/usr\/local\/unravel\/sql\/mysql\/ Create the default user admin # \/usr\/local\/unravel\/install_bin\/db_initial_inserts.sh | \/usr\/local\/unravel\/install_bin\/db_access.sh Start Unravel Daemon Disable the bundled db on Unravel Server. Only one of these commands is needed, depending on your exact version of 4.3.x Unravel. The unnecessary command produces an error that can be ignored. # sudo chkconfig unravel_db off\n# sudo chkconfig unravel_pg off Start Unravel Server. # sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Step 4: Set Up VPC Peering for Unravel EC2 Node (Optional)", 
"url" : "install/install-emr/install-emr-part4.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Step 4: Set Up VPC Peering for Unravel EC2 Node (Optional)", 
"snippet" : "Follow these steps only This topic explains how to resolve connectivity issues when Unravel EC2 node is created in a VPC different than where the EMR cluster located. This documentation also applies for Unravel EC2 node connecting to RDS created on different VPC of the same region. Assumptions The V...", 
"body" : " Follow these steps only This topic explains how to resolve connectivity issues when Unravel EC2 node is created in a VPC different than where the EMR cluster located. This documentation also applies for Unravel EC2 node connecting to RDS created on different VPC of the same region. Assumptions The VPC where Unravel EC2 located is in the same region where the EMR cluster located (e.g., us-east-1). The subnet used by Unravel EC2 does not overlap the IP block range of the subnet used in EMR cluster. Network ACL on both VPC for Unravel EC2 and EMR cluster are the default and allow all traffic. The security group is the only security enforcement on network access. In the following steps, we have both Unravel EC2 node and EMR cluster located in us-east-1 region but configured with different VPC and subnet. There is no network access allowed between Unravel EC2 and EMR cluster by default. Resources Internal IP Address Subnet ID Subnet IP Block VPC ID (Name) IP block in VPC Security Group ID (Name) Unravel EC2 node 10.10.0.7 subnet-03b82c56b2c26dbd1 10.10.0.0\/24 vpc-0b0e17b01c4a3b54a (Unravel_VPC) 10.10.0.0\/16 sg-0e0a03084398287c9 (Unravel-EC2_SG) EMR Cluster Master node 10.11.0.53 subnet-0294cc17a42a9acfd 10.11.0.0\/24 vpc-c3d079a4 (VPC_for_VPC Peering) 10.11.0.0\/16 sg-0a73c3aea9340ae49 (EMR_VPC_SG) EMR Cluster Core nodes 10.11.0.76 10.11.0.130 subnet-0294cc17a42a9acfd 10.11.0.0\/24 vpc-c3d079a4 (VPC_for_VPC Peering) 10.11.0.0\/16 sg-0a73c3aea9340ae49 (EMR_VPC_SG) Create VPC Peering in VPC Dashboard From the AWS console VPC services Peering Connections Create Peering Connection VPC (Requester) VPC (Accepter) Create Peering Connection. A success message should appear in the screen. Click OK Accept the VPC Peering Request In the VPC Dashboard Pending Acceptance Action Accept Request Click Yes Accept Close Create Routes Between Peered VPC Go to VPC Dashboard Route Tables Edit Add another route Find the Unravel_VPC route table. Enter the IP block of EMR VPC, e.g., 10.11.0.0\/16 In the Destination Target Save Find the Test_EMR_VPC route table. Set the Destination Target Save. pcx-0a57a978ef9a525e2 Target Save Update Security Groups Go to VPC Dashboard Security Group After locating each security group: Click Add another rule Set Type ALL traffic Protocol ALL Locate the security group used on Unravel EC2 node. Enter the EMR VPC IP block, e.g., 10.11.0.0\/16 in the Source Save Locate the security group used on EMR cluster node and enter the Unravel VPC IP block. For example, 10.10.0.0\/16 Click Save Verify Connection Between Unravel EC2 Node and EMR Master Node Open SSH sessions to both Unravel EC2 nodeand EMR master node. Since the above example allowed ALL Traffic On Unravel EC2 node, telnet On the EMR master node, telnet If telnet port tests are positive, the VPC peering connection is setup correctly. If not, troubleshoot the configuration on network ACL, security groups, and route tables used on both VPCs. See unsupported VPC peering configurations on AWS documentation. " }, 
{ "title" : "Testing and Troubleshooting", 
"url" : "install/install-emr/install-emr-test-troubleshoot.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Testing and Troubleshooting", 
"snippet" : "This is an excerpt of the User Guide Testing the Deployment Troubleshooting Common Issues Sending Diagnostics to Unravel Support Reconnecting to your EMR Instance Deleting the Unravel EC2 Instance Diagnosing and Using Oozie with Unravel Not seeing Sufficient Historical Data in Unravel Testing the De...", 
"body" : " This is an excerpt of the User Guide \n Testing the Deployment \n Troubleshooting Common Issues \n Sending Diagnostics to Unravel Support \n Reconnecting to your EMR Instance \n Deleting the Unravel EC2 Instance \n Diagnosing and Using Oozie with Unravel \n Not seeing Sufficient Historical Data in Unravel Testing the Deployment Connect to the Unravel UI via an SSH tunnel. Create sn SSH tunnel to port 3000 on the Unravel EC2 node. For example: \n \n \n \n ssh -i ssh_key.pem centos@ Unravel_node_public_IP -L 3000:127.0.0.1:3000 Start your browser from your workstation and navigate to http:\/\/127.0.0.1:3000 Log in with username admin unraveldata The OPERATIONS Trial versions include a message in the top menu bar about the trial license and the number of days remaining until the trial expires. Please contact us Run some jobs from the EMR master node. The EMR master node has some sample MapReduce and Spark jobs on it. Run these jobs to verify that the Unravel EC2 node is collecting data from the EMR cluster. Your usage may vary depending on what applications you installed on your cluster. \n Sample MapReduce (MR) Job Connect to the EMR master node via SSH: \n ssh -i ssh_key.pem ec2-user@EMR_master_public_IP Run this MapReduce (MR) \"Pi\" job: sudo -u hdfs hadoop jar \/usr\/lib\/hadoop-mapreduce\/hadoop-mapreduce-examples.jar pi 100 100 When the MR job finishes, check Unravel UI: You should see one successful application labeled MR To see details about the MR job, click the APPLICATIONS Click the orange bar that notifies you that there Unravel has recommendation(s) for tuning this job. Explore other metrics about this job by clicking the tabs within the job details screen. \n Sample Spark Job Connect to the EMR master node via SSH: \n ssh -i ssh_key.pem ec2-user@EMR_master_public_IP Run this Spark \"Pi\" job: sudo -u hdfs spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --num-executors 1 --driver-memory 512m --executor-memory 512m --executor-cores 1 \/usr\/lib\/spark\/examples\/jars\/spark-examples.jar 1000 When the Spark job finishes, check Unravel UI: You should see one successful application labeled SPARK To see details about the Spark job, click the APPLICATIONS Click the orange bar that notifies you that there Unravel has recommendation(s) for tuning this job. Explore other metrics about this job by clicking the tabs within the job details screen. \n Sample Tez Job a. Run \/usr\/local\/unravel\/install_bin\/hive_test_simple.sh hive.execution.engine=tez b. Check Unravel Web UI for Tez data. For instructions, see Tez Application Manager Troubleshooting Common Issues Sending Diagnostics to Unravel Support In the upper right corner of Unravel Web UI, click the pull-down menu, and select Manage Wait for the page to fully load Select the Diagnostics Click Send Diagnostics to Unravel Support This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com.unraveldata.login.admins If you don't have access to push the bundle through the Web UI. 1. On the Unravel Host bundle the diagnostic information. # \/usr\/local\/unravel\/install_bin\/diag_dump.sh 2. Email the bundle to the Unravel support team Reconnecting to your EMR Instance If you used our CloudFormation template to create your Unravel EC2 instance, it's protected by ASG, which sets the target\/maximum number of instances at 1. In the rare scenario of this instance failing, ASG will recreate it with the same configuration, and restore its prior history from a backup saved in the S3 bucket. In this case, your existing EMR clusters just need to be reconnected to the newly created Unravel EC2 node as described in Step 2: Connect the Unravel EC2 Instance to a New\/Existing EMR Cluster Deleting the Unravel EC2 Instance If you're done with the Unravel EC2 instance, you can delete it as follows. From your EC2 console ( https:\/\/console.aws.amazon.com\/cloudformation\/ In the Actions Delete Stack Click Yes, Delete Monitor the Status Diagnosing and Using Oozie with Unravel You may see this common error: org.apache.oozie.action.ActionExecutorException: JA010: Property [fs.default.name] not allowed in action [job-xml] configuration The reason behind can be older version of configuration where some properties being deprecated. The straight-forward solution is to comment out the <job-xml> element in the workflow.xml file. Not seeing Sufficient Historical Data in Unravel Settings are needed to adjust the time horizon. In this example, it is set to 2 years with recent data showing the max amount minus 2 weeks: In Manage page, Core section, under Retention heading, under TIME SERIES RETENTION DAYS, adjust the number of days to retain data. This corresponds to com.unraveldata.retention.max.days property. In \/usr\/local\/unravel\/etc\/unravel.properties set com.unraveldata.history.maxSize.weeks=104 After changing the settings,restart the servers \n sudo \/etc\/init.d\/unravel_all.sh restart " }, 
{ "title" : "Operational Guidance", 
"url" : "install/install-emr/install-emr-ops-guidance.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Elastic MapReduce (EMR) \/ Operational Guidance", 
"snippet" : "Health Check For monitoring the Unravel server, CloudWatch alerts can be set up to monitor the specific EC2 instance that has the Unravel Server deployed. Unravel also has a monitoring service - a lightweight daemon which allows you to monitor various Unravel components. Details here For Storage Cap...", 
"body" : " Health Check For monitoring the Unravel server, CloudWatch alerts can be set up to monitor the specific EC2 instance that has the Unravel Server deployed. Unravel also has a monitoring service - a lightweight daemon which allows you to monitor various Unravel components. Details here For Storage Capacity monitoring for RDS (if thatâ€™s the user chosen configuration for the Unravel DB), here (It is required for the Unravel EC2 instance to be in the same region with the target EMR clusters which Unravel EC2 node will be monitoring. So the region disruption does not apply for Unravel. ) Backup and Recovery The best course of action to take in situations like instance failures is to have followed the process of backing up and disaster recovery as described here It is required for the Unravel EC2 instance to be in the same region with the target EMR clusters which Unravel EC2 node will be monitoring. So the region recovery does not apply for Unravel. Unravel is designed to maintain business continuity and does not support complete\/true High Availability (HA). Routine Maintenance There are multiple means by which Unravel announces and documents details of availability of new versions: On the Unravel Product documentation main page here Unravel Solution Engineers and Account Management Team members engage with customers directly to tell them about the availability of upgrades and patches Blogs Through the Unravel Newsletter (sign up on the Unravel website Emergency Maintenance In the event of fault conditions, such as a transient failure of an AWS Service such that the availability of EC2 in a particular availability zone (AZ) is degraded, or a more permanent failure of an AWS service such that EC2 instance has faulted, or an EC2 Scheduled Maintenance Event is received, the best course of action to take in such situations is to have followed the process of backing-up and disaster recovery as described here Support To contact Unravel Support, visit Support Costs Currently, there are no additional costs for obtaining Unravel Support. " }, 
{ "title" : "Amazon Athena", 
"url" : "install/install-athena.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Athena", 
"snippet" : "Amazon Athena is a serverless query service that lets you interact with data directly in place on AWS S3 using ANSI standard SQL. You pay only for the queries you run, based on how much data the queries scan. Failed queries cost $0. For more information on Athena pricing, see https:\/\/aws.amazon.com\/...", 
"body" : "Amazon Athena is a serverless query service that lets you interact with data directly in place on AWS S3 using ANSI standard SQL. You pay only for the queries you run, based on how much data the queries scan. Failed queries cost $0. For more information on Athena pricing, see https:\/\/aws.amazon.com\/athena\/pricing\/?nc=sn&loc=3 You send Unravel information about your Athena queries through an AWS Lambda function which monitors your AWS CloudTrail trail for Athena events. Use Cases Amazon Athena is well suited to structured data such as logs. " }, 
{ "title" : "Integrating Athena with Unravel", 
"url" : "install/install-athena.html#UUID-b2a95ebb-f7d8-2193-ce63-5cedd69b45d7_id_AmazonAthena-IntegratingAthenawithUnravel", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Athena \/ Integrating Athena with Unravel", 
"snippet" : "Follow these steps to connect your Athena queries to Unravel through an AWS Lambda function. These steps assume you already have Athena queries set up. For help with Athena, see https:\/\/aws.amazon.com\/athena\/ Summary Create a trail in AWS CloudTrail for management read\/write events. Create a new AWS...", 
"body" : "Follow these steps to connect your Athena queries to Unravel through an AWS Lambda function. These steps assume you already have Athena queries set up. For help with Athena, see https:\/\/aws.amazon.com\/athena\/ Summary Create a trail in AWS CloudTrail for management read\/write events. Create a new AWS role to allow AWS Lambda functions to call AWS services on your behalf. Create an AWS Lambda function that sends data to Unravel whenever your trail has a new entry. In Unravel UI, look at Apps Athena " }, 
{ "title" : "Create a Trail in AWS CloudTrail", 
"url" : "install/install-athena.html#UUID-b2a95ebb-f7d8-2193-ce63-5cedd69b45d7_id_AmazonAthena-CreateaTrailinAWSCloudTrail", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Athena \/ Create a Trail in AWS CloudTrail", 
"snippet" : "You can capture Athena activity by creating a specific CloudTrail trail for management read\/write events, and specifying a new or existing S3 bucket to store the trail. Your AWS account must have the following permissions for these steps: AWSCloudTrailReadOnlyAccess CloudtrailFullAccess Log into you...", 
"body" : "You can capture Athena activity by creating a specific CloudTrail trail for management read\/write events, and specifying a new or existing S3 bucket to store the trail. Your AWS account must have the following permissions for these steps: AWSCloudTrailReadOnlyAccess CloudtrailFullAccess Log into your AWS console at https:\/\/console.aws.amazon.com In the AWS console, select CloudTrail On the CloudTrail Trails Create trail In the Trail name In the Apply trail to all regions Yes In the Management events section, next to Read\/Write events All In the Data events In the Storage location You can create a new S3 bucket or use an existing S3 bucket. If you create a new bucket: Set the S3 bucket name to unravel-cloudtrail Expand the Advanced Leave the Log file prefix For Encrypt log files with SSE-KMS No For Enable log file validation Yes For Send SNS notification for every log file delivery No Click Create Configure CloudWatch permissions on unravel-cloudtrail Click your newly created trail, unravel-cloudtrail CloudWatch Logs Click Configure In the New or existing log group CloudTrail\/UnravelLogGroup Click Continue On the next page, expand View Details IAM Role Create a new IAM Role Role Name unravel-cloudtrail-role Click Allow " }, 
{ "title" : "Create a Role for Unravel's AWS Lambda Function", 
"url" : "install/install-athena.html#UUID-b2a95ebb-f7d8-2193-ce63-5cedd69b45d7_id_AmazonAthena-CreateaRoleforUnravelsAWSLambdaFunction", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Athena \/ Create a Role for Unravel's AWS Lambda Function", 
"snippet" : "Unravel provides an AWS Lambda function to forward your CloudTrail trail to Unravel. To connect Unravel’s AWS Lambda function with your trail, you first need to create an AWS role for Unravel’s Lambda function to use, if you don’t have one already. For more information on AWS Lambda, see Using AWS L...", 
"body" : "Unravel provides an AWS Lambda function to forward your CloudTrail trail to Unravel. To connect Unravel’s AWS Lambda function with your trail, you first need to create an AWS role for Unravel’s Lambda function to use, if you don’t have one already. For more information on AWS Lambda, see Using AWS Lambda with AWS CloudTrail Log into your AWS console at https:\/\/console.aws.amazon.com In the AWS console, select IAM On the IAM Roles Click Create role In the Select type of trusted entity AWS service In the Choose the service that will use this role Lambda Click Next: Permissions On the Attach permissions policies AmazonS3ReadOnlyAccess AWSLambdaVPCAccessExecutionRole Click Next: Tags (Optional) If you want to add tags to this role, add them here. Click Next: Review On the Review page, set Role name unravel-athena-lambda-role Click Create role The AWS console displays a message indicating that it created the role. Select the role in the list of roles. On the role summary page, select the Trust relationships " }, 
{ "title" : "Create Unravel's AWS Lambda Function", 
"url" : "install/install-athena.html#UUID-b2a95ebb-f7d8-2193-ce63-5cedd69b45d7_id_AmazonAthena-CreateUnravelsAWSLambdaFunction", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Athena \/ Create Unravel's AWS Lambda Function", 
"snippet" : "This section explains how to create an AWS Lambda function that sends data to Unravel whenever your trail has a new entry. Your AWS account must have the following permission for these steps: AWSLambdaFullAccess Define Basic Settings for the Lambda Function Log into your AWS console at https:\/\/conso...", 
"body" : "This section explains how to create an AWS Lambda function that sends data to Unravel whenever your trail has a new entry. Your AWS account must have the following permission for these steps: AWSLambdaFullAccess Define Basic Settings for the Lambda Function Log into your AWS console at https:\/\/console.aws.amazon.com In the AWS console, select Lambda On the Lambda Create function On the Create function Function name UnravelAthenaLambda Runtime Python 3.7 Execution role Use an existing role Existing role unravel-athena-lambda-role Click Create function AWS displays a banner indicating success, and displays your new Lambda function’s page. Add a Trigger to the Lambda Function On your new Lambda function’s page, select Amazon S3 From the list of triggers on the right, select S3 In the Configure triggers Bucket unravel-cloudtrail Event type All object create events Select the Enable trigger At the top of the page, click Test Click Add AWS shows the new S3 trigger at the bottom of the page. At the top of the page, click Save Add Code to Unravel’s AWS Lambda Function Select the new Lambda function: AWS displays configurable settings for this function. In the Function code Code entry type Upload a file Amazon S3 Amazon S3 link URL: s3:\/\/unraveldatarepo\/share\/lambda\/UnravelAthenaLambda.zip Runtime Python 3.7 In the Environment variables Key: unravel_lr_url Value: <Private IP of Unravel Node>:<Port>\/logs\/athena\/j-default\/athena\/athena <Private IP of Unravel Node> <Port> In the Execution role Select Use an existing role Existing role: unravel-athena-lambda-role In the Network Don’t select No VPC Select your VPC. Select at least two subnets from the pull-down list (hold CTRL to select multiple subnets). Select your private security group (SG). Review the inbound and outbound rules. At the top of the page, click Test At the top of the page, click Save AWS displays a banner indicating success. " }, 
{ "title" : "View Athena Queries in Unravel UI", 
"url" : "install/install-athena.html#UUID-b2a95ebb-f7d8-2193-ce63-5cedd69b45d7_id_AmazonAthena-ViewAthenaQueriesinUnravelUI", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Athena \/ View Athena Queries in Unravel UI", 
"snippet" : "In Unravel UI, look at Apps Athena User Guide...", 
"body" : "In Unravel UI, look at Apps Athena User Guide " }, 
{ "title" : "Resources", 
"url" : "install/install-athena.html#UUID-b2a95ebb-f7d8-2193-ce63-5cedd69b45d7_id_AmazonAthena-Resources", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Athena \/ Resources", 
"snippet" : "Create a Lambda Function with the Console AWS Lambda Permissions Examine Athena requests using CloudTrail Logs...", 
"body" : " Create a Lambda Function with the Console AWS Lambda Permissions Examine Athena requests using CloudTrail Logs " }, 
{ "title" : "Empty or missing topic", 
"url" : "install/install-athena/empty-or-missing-topic.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Amazon Athena \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "MapR", 
"url" : "install/install-mapr.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ MapR", 
"snippet" : "This section explains how to deploy Unravel Server and Sensors on MapR. The following reports require that you install the OnDemand service: Sessions Cluster Optimization Capacity Forecasting Small Files For installation instructions, see OnDemand...", 
"body" : "This section explains how to deploy Unravel Server and Sensors on MapR. The following reports require that you install the OnDemand service: Sessions Cluster Optimization Capacity Forecasting Small Files For installation instructions, see OnDemand " }, 
{ "title" : "MapR Pre-Installation Check", 
"url" : "install/install-mapr/install-mapr-pre.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ MapR \/ MapR Pre-Installation Check", 
"snippet" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility MapR: 6.1.0, 6.0.0, 5.2.0 Hadoop 1.x - 2.x Kafka 0.9, 0.10, 0.11, and 1.0 (Apache version equivalent) Kerberos\/MapR Tickets Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architectur...", 
"body" : "The following installation requirements must be met for a successful installation of Unravel. Platform Compatibility MapR: 6.1.0, 6.0.0, 5.2.0 Hadoop 1.x - 2.x Kafka 0.9, 0.10, 0.11, and 1.0 (Apache version equivalent) Kerberos\/MapR Tickets Hive 0.9.x - 1.2.x Spark 1.3.x - 2.0.x Hardware Architecture: x86_64 Cores: 8 RAM: 64GB minimum, 128GB for medium volume (>25,000 jobs per day), 256GB for high volume (>50,000 jobs per day) Disk: \/usr\/local\/unravel Disk: \/srv\/unravel For 60,000+ MR jobs per day, two or more gateway\/edge nodes are recommended (\"multi-host Unravel\") Software Operating System: RedHat\/Centos 6.4 - 7.4 If you're running Red Hat Enterprise Linux (RHEL) 6.x, you must set the following property in your elasticsearch.yml boostrap.system_call_filter: false Reference: https:\/\/github.com\/elastic\/elasticsearch\/issues\/22899 installed libaio.x86_64 (or disabled) should be set in SELINUX=permissive \/etc\/selinux\/config HDFS+Hive+YARN+Spark client\/gateway, Hadoop and Hive commands in PATH LDAP (AD or Open LDAP) compatible for Unravel Web UI user authentication (Open signup by default) Zookeeper is not MySQL: 5.5, 5.6, 5.7 (recommended) MySQL driver (connector): 5.1.47 Access Permissions If Kerberos is in use, a keytab for principal hdfs (or read-only equivalent) is required for access to: YARN's \"done\" directory in HDFS YARN's log aggregation directory in HDFS Spark event log directory in HDFS File sizes under Hive warehouse directory Access to YARN Resource Manager REST API principal needs right to find out which RM is active JDBC access to the Hive Metastore (read-only user is sufficient) Network The following ports must be open on Unravel's host(s): Port(s) Direction Description 3000 or 4020 Both Non- HTTPS traffic to and from Unravel UI 3003, 3005 Both HTTPS traffic to and from Unravel UI. Open these ports if localhost 3316 Both Database traffic 4020 Both Unravel APIs 4021 Both Host monitoring of JMX on localhost 4031 Both Database traffic 4043 In UDP and TCP ingest traffic from the entire cluster to Unravel Server(s) 4044-4049 In UDP and TCP ingest spares for unravel_lr* 4091-4099 Both Kafka brokers 4171-4174, 4176-4179 Both ElasticSearch; localhost communication between Unravel daemons or Unravel Servers (if multi-host Unravel deployment) 4181-4189 Both Zookeeper daemons 4210 Both Cluster access service HDFS ports In Traffic from the cluster to Unravel Server(s) Hive Metadata DB port Out For YARN only. Traffic from Hive to Unravel Server(s) for partition reporting 7180 (or 7183 for HTTPS) Out Traffic from Unravel Server(s) to Cloudera Manager 8032 Out Traffic from Unravel Server(s) to the Resource Manager (RM) server(s) 8188 Out Traffic from Unravel Server(s) to the ATS server(s) 11000 Out For Oozie only. Traffic from Unravel Server(s) to the Oozie server " }, 
{ "title" : "Step 1: Install Unravel Server on MapR", 
"url" : "install/install-mapr/install-mapr-part1.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ MapR \/ Step 1: Install Unravel Server on MapR", 
"snippet" : "This topic explains how to deploy Unravel Server on the MapR converged data platform. must be a fully qualified domain name or IP address. UNRAVEL_HOST_IP Configure the Host Machine Allocate a cluster gateway\/edge\/client host with HDFS access. If you do not already have a gateway\/edge\/client host pr...", 
"body" : "This topic explains how to deploy Unravel Server on the MapR converged data platform. must be a fully qualified domain name or IP address. UNRAVEL_HOST_IP Configure the Host Machine Allocate a cluster gateway\/edge\/client host with HDFS access. If you do not already have a gateway\/edge\/client host provisioned for Unravel server, follow these steps which are needed to enable the hadoop fs For more information about the MapR client configuration, see https:\/\/maprdocs.mapr.com\/52\/ReferenceGuide\/configure.sh.html Run the following commands on Unravel Server as root NAME CLDB_LIST HISTORY_SERVER # sudo yum install mapr-client.x86_64\n# sudo \/opt\/mapr\/server\/configure.sh -N NAME CLDB_LIST HISTORY_SERVER Configure the host before installing the RPM: Run the following commands on Unravel Server as root # sudo useradd -g mapr unravel\n# hadoop fs -mkdir \/user\/unravel\n# hadoop fs -chown unravel:mapr \/user\/unravel If MapR tickets are enabled, check mapr ticket for users unravel mapr MAPR_TICKETFILE_LOCATION \/srv\/unravel\/unravel_ctl Check available RAM to ensure availability: # free -g Adjust RAM if needed. Only change this setting on the Unravel gateway\/client machine. For instructions on adjusting RAM allocated to MapR-FS (mfs), see https:\/\/community.mapr.com\/docs\/DOC-1209 For example, edit \/opt\/mapr\/conf\/warden.conf service.command.mfs.heapsize.maxpercent=10 Restart MapR-FS (mfs). Install the Unravel Server RPM on the Host Machine Get the Unravel Server RPM. See Download Unravel Software Make symlinks if required. If you want the two disk areas used by Unravel to be on different volumes, you can make symlinks to specific areas before installing (or do a mv and symlink after installing). Do it before the first install if there is insufficient space on the target paths \/usr\/local\/unravel \/srv\/unravel Install the Unravel Server RPM. # sudo rpm -U unravel-4.*.x86_64.rpm*\n# \/usr\/local\/unravel\/install_bin\/await_fixups.sh The precise filename can vary, depending on how it was fetched or copied. The rpm .rpm -U Run the specified await_fizup.sh await_fizup.sh DONE The installation creates the following items: which contains the executables, scripts, the master configuration file ( \/usr\/local\/unravel\/ \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/logs\/ are scripts for controlling services. You can use \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh User unravel Initial internal database and other durable states in \/srv\/unravel\/ The initial installation includes a bundled database; you can switch to AWS RDS for production. For details, see externally managed MySQL During initial install, a bundled database is used. This can be switched to use an for production. Do host-specific post-installation actions. Run the following commands on Unravel Server: # sudo \/etc\/init.d\/unravel_all.sh stop\n# sudo \/usr\/local\/unravel\/install_bin\/switch_to_mapr.sh Configure Unravel Server (Basic\/Core Options) Enable Optional Daemons Depending on your workload volume or kind of activity, you can enable additional daemons at this point, see Creating Multiple Workers for High Volume Data Modify unravel.properties Edit \/usr\/local\/unravel\/etc\/unravel.properties # Property Description Example Values com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. http:\/\/ LAN_DNS com.unraveldata.customer.organization Identifies your installation for reporting purposes. Company_and_org com.unraveldata.tmpdir Optional. Location where Unravel's temp file will reside \/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Sets retention for search data. 26 com.unraveldata.hive.hook.topic.num.threads Optional. Defines the number of threads. Default is 1. Depending on job volume, increase this property to N N ThousandJobsPerDay 1 com.unraveldata.job.collector.done.log.base HDFS path to \"done\" directory of MR logs \/var\/mapr\/cluster\/yarn\/rm\/staging\/history\/done com.unraveldata.job.collector.log.aggregation.base An HDFS path that helps locate MR job logs to process \/tmp\/logs\/*\/logs\/ com.unraveldata.login.admins Defines the usernames that can access Unravel Web UI's admin pages. Default is admin admin com.unraveldata.spark.eventlog.location Where to find Spark event logs maprfs:\/\/\/apps\/spark yarn.resourcemanager.webapp.address Resource Manager web app address http:\/\/example.localdomain:8088 yarn.resourcemanager.webapp.username Resource Manager username to login yarn.resourcemanager.webapp.password Resource Manager password to login https.protocols Enable https access to Resource Manager TLSv1.2 javax.jdo.option.ConnectionURL A JDBC connection URL jdbc: mysql:\/\/example.localdomain:3306\/hive postgresql:\/\/example.localdomain:7432\/hive_zzzzzz javax.jdo.option.ConnectionDriverName JDBC driver or com.mysql.jdbc.Driver org.postgresql.Driver javax.jdo.option.ConnectionUserName Hive metastore user name hiveuser javax.jdo.option.ConnectionPassword Hive metastore password com.unraveldata.metastore.databasePattern Optional s*|t*|d* oozie.server.url Oozie URL http:\/\/example.localdomain:11000\/oozie If Kerberos is Enabled, Add Authentication for HDFS: Create or identify a principal and keytab for Unravel daemons to access HDFS and REST when Kerberos is enabled. Add properties for Kerberos in \/usr\/local\/unravel\/etc\/unravel.properties Substitute the correct filename and principal: com.unraveldata.kerberos.principal=unravel\/myhost.mydomain@MYREALM\ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab Find and verify the principal the keytab by running this command: # klist -kt KEYTAB_FILE Set the Linux file permissions of the keytab file to 500 ( chmod 500 unravel Run Unravel Daemons with Custom User If Sentry is Enabled, Add These Permissions: Define your own alt principal with narrow privileges. The alt principal can be admin unravel rpm X Resource Principal Access Purpose hdfs:\/\/user\/spark\/applicationHistory mapr or alt read Spark event log hdfs:\/\/usr\/history\/done mapr or alt read MapReduce logs hdfs:\/\/tmp\/logs mapr or alt read YARN aggregation folder hdfs:\/\/user\/hive\/warehouse mapr or alt read Obtain table partition sizes Hive Metastore access hive read Hive table information Switch User Depending on your cluster security configuration, you will need to run the switch_to_user # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh x y where x y switch_to_user Do Host-Specific Configuration Steps For MapR, there are no host-specific configuration steps. Restart Unravel Server After edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties # sudo \/etc\/init.d\/unravel_all.sh start\n# sleep 60 Run the echo # echo \"http:\/\/( UNRAVEL_HOST_IP This completes the basic\/core configuration. Log into Unravel Web UI Using a web browser, navigate to http:\/\/ UNRAVEL_HOST_IP admin unraveldata For the free trial version, use the Chrome browser. Congratulations! Unravel Server is up and running. Unravel UI displays collected data. For instructions on using Unravel UI, see the User Guide Enable Additional Data Collection\/Instrumentation Install the Unravel Sensor Parcel on gateway\/edge\/client nodes that are used to submit Hive queries to push additional information to Unravel Server. For details, see Step 2: Enable Additional Data Collection \/ Instrumentation for MapR " }, 
{ "title" : "Step 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"url" : "install/install-mapr/install-mapr-part2.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ MapR \/ Step 2: Enable Additional Data Collection \/ Instrumentation for MapR", 
"snippet" : "Table of Contents Introduction 1. Enable Additional Instrumentation on Unravel Server's Host 2. For Oozie, Copy Unravel Hive Hook and BTrace JARs to the HDFS Shared Library Path 3. Confirm that Unravel Web UI Shows Additional Data 4. Confirm and Adjust the Settings in yarn-site.xml 5. Enable Additio...", 
"body" : "\n Table of Contents \n Introduction \n 1. Enable Additional Instrumentation on Unravel Server's Host \n 2. For Oozie, Copy Unravel Hive Hook and BTrace JARs to the HDFS Shared Library Path \n 3. Confirm that Unravel Web UI Shows Additional Data \n 4. Confirm and Adjust the Settings in yarn-site.xml \n 5. Enable Additional Instrumentation on Other Hosts in the Cluster \n 6. Enable Instrumentation Manually Introduction This topic explains how to enable additional data collection or instrumentation on the MapR converged data platform. These instructions apply to Unravel Server v4.0. For older versions of Unravel Server, contact Unravel Support. \n HIGHLIGHTED UNRAVEL_HOST_IP \n SPARK_VERSION_X.Y.Z HIVE_VERSION 1. Enable Additional Instrumentation on Unravel Server's Host # sudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server {UNRAVEL_HOST_IP} --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} # sudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server {UNRAVEL_HOST_IP} --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} --sensor-only \n For Sensor Upgrade only # sudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server {UNRAVEL_HOST_IP} --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} --sensor-only \n Dry-Run (test\/check instrumentation, this does not change any configuration file): # sudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server {UNRAVEL_HOST_IP} --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} --dry-run Hive hook jar is installed under: \n \/usr\/local\/unravel_client\/ Resource metrics sensor jars are installed under: \n \/usr\/local\/unravel-agent\/ Configuration changes (for MapR 5.2\/MapR 6.0) are made to the following files, (< SPARK_VERSION X.Y.Z> \n \/opt\/mapr\/spark\/spark-<SPARK VERSION X.Y.Z>\/conf\/spark-defaults.conf \n \/opt\/mapr\/hive\/hive-<HIVE VERSION X.Y>\/conf\/hive-site.xml \n \/opt\/mapr\/hive\/hive-<HIVE VERSION X.Y>\/conf\/hive-env.sh \n \/opt\/mapr\/hadoop\/hadoop-<HADOOP VERSION X.Y.Z>\/etc\/hadoop\/yarn-site.xml \n \/opt\/mapr\/hadoop\/hadoop-<HADOOP VERSION X.Y.Z>\/etc\/hadoop\/mapred-site.xml \n \/usr\/local\/unravel\/etc\/unravel.properties Copy of original configuration is saved in same directory named *.preunravel \/opt\/mapr\/hive\/hive-1.2\/conf\/hive-site.xml.preunravel Once the files are present on edge host where Unravel rpm is installed, you can tar tar 2. For Oozie, Copy Unravel Hive Hook and BTrace JARs to the HDFS Shared Library Path Copy the Hive Hook JAR in \/usr\/local\/unravel_client\/ \/usr\/local\/unravel-agent\/ oozie.libpath 3. Confirm that Unravel Web UI Shows Additional Data Run a Hive job using a test script provided by Unravel Server. This is where you can see the effects of the instrumentation setup. Best practice is to run this test script on Unravel Server rather than on a gateway\/edge\/client node. That way you can verify that instrumentation is working first, and then enable instrumentation on other gateway\/edge\/client nodes. \n someUser must This script creates a uniquely named table in the default database, adds some data, runs a Hive query on it, and then deletes the table. It runs the query twice using different workflow tags so you can clearly see the two different runs of the same workflow in Unravel Web UI. # sudo -u {someUser} \/usr\/local\/unravel\/install_bin\/hive_test_simple.sh 4. Confirm and Adjust the Settings in yarn-site.xml Check specific properties in \/opt\/mapr\/hadoop\/hadoop-2.7.0\/etc\/hadoop\/yarn-site.xml \n yarn.resourcemanager.webapp.address \n \n \n <property> <name>yarn.resourcemanager.webapp.address<\/name> <value>10.0.0.110:8088<\/value> <source>yarn-site.xml<\/source> <\/property> \n yarn.log-aggregation-enable \n \n \n <property> <name>yarn.log-aggregation-enable<\/name> <value>true<\/value> <description>For log aggregations<\/description> <\/property>> 5. Enable Additional Instrumentation on Other Hosts in the Cluster Run the shell script unravel_mapr_setup.sh Copy the newly edited yarn-site.xml Do a rolling-restart of HiveServer2 To instrument more servers, you can use the setup script we provide or see the effect it has and replicate it using your own provisioning automation system. If you already have a way to customize and deploy hive-site.xml yarn-site.xml 6. Enable Instrumentation Manually Enable instrumentation manually by updating the following files: \n hive-site.xml \n hive-env.sh \n spark-defaults.conf \n hadoop-env.sh \n mapred-site.xml Once the files are updated on edge host where Unravel rpm is installed, you can scp Update hive-site.xml Copy the content in \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip \/opt\/mapr\/hive\/hive-HIVE_VERSION_X.Y.Z\/conf\/hive-site.xml \n \n \n <property> <name>com.unraveldata.host<\/name> <value>{UNRAVEL_HOST_IP}<\/value> <description>Unravel hive-hook processing host<\/description> <\/property> <property> <name>com.unraveldata.hive.hook.tcp<\/name> <value>true<\/value> <\/property> <property> <name>com.unraveldata.hive.hdfs.dir<\/name> <value>\/user\/unravel\/HOOK_RESULT_DIR<\/value> <description>destination for hive-hook, Unravel log processing<\/description> <\/property> <property> <name>hive.exec.driver.run.hooks<\/name> <value>com.unraveldata.dataflow.hive.hook.UnravelHiveHook<\/value> <description>for Unravel, from unraveldata.com <\/property> <property> <name>hive.exec.pre.hooks<\/name> <value>com.unraveldata.dataflow.hive.hook.UnravelHiveHook<\/value> <description>for Unravel, from unraveldata.com <\/property> <property> <name>hive.exec.post.hooks<\/name> <value>com.unraveldata.dataflow.hive.hook.UnravelHiveHook<\/value> <description>for Unravel, from unraveldata.com <\/property> <property> <name>hive.exec.failure.hooks<\/name> <value>com.unraveldata.dataflow.hive.hook.UnravelHiveHook<\/value> <description>for Unravel, from unraveldata.com <\/property> <\/configuration> Update hive-env.sh In \/opt\/mapr\/hive\/hive-HIVE_VERSION_X.Y.Z\/conf\/hive-env.sh \n \n \n export AUX_CLASSPATH=${AUX_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-HIVE_VERSION _X.Y.Z} Update spark-defaults.conf In \/opt\/mapr\/spark\/spark-SPARK_VERSION_X.Y.Z\/conf\/spark-defaults.conf \n \n \n \n spark.unravel.server.hostport UNRAVEL_HOST_IP:4043 spark.eventLog.dir maprfs:\/\/\/apps\/spark \/\/ the following is one line spark.history.fs.logDirectory maprfs:\/\/\/apps\/spark -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=spark-{SPARK_VERSION_X.Y.Z},config=driver \/\/ the following is one line spark.executor.extraJavaOptions -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=spark-SPARK_VERSION_X.Y.Z,config=executort Update hadoop-env.sh In \/opt\/mapr\/hadoop\/hadoop-HADOOP_VERSION_X.Y.Z\/etc\/hadoop\/hadoop-env.sh \n \n \n export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-HIVE_VERSION_X.Y.Z.0-hook.jar Update mapred-site.xml In \/opt\/mapr\/hadoop\/hadoop-HADOOP_VERSION_X.Y.Z\/etc\/hadoop\/mapred-site.xml \n \n \n <property> <name>mapreduce.task.profile<\/name> <value>true<\/value> <\/property> <property> <name>mapreduce.task.profile.maps<\/name> <value>0-5<\/value> <\/property> <property> <name>mapreduce.task.profile.reduces<\/name> <value>0-5<\/value> <\/property> <property> <name>mapreduce.task.profile.params<\/name> <value> -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr UNRAVEL_HOST_IP <\/property> <property> <name> yarn.app.mapreduce.am <value> -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr <\/property> Make sure the original value of \n yarn.app.mapreduce.am " }, 
{ "title" : "Azure HDInsight", 
"url" : "install/install-hdi.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Azure HDInsight", 
"snippet" : "You can deploy Unravel on Azure HDInsight either as a separate Azure virtual machine (VM) or as an Azure Marketplace app. In either deployment, the Unravel app resides on an edge node to allow you to spin up\/down ephemeral Hadoop clusters....", 
"body" : "You can deploy Unravel on Azure HDInsight either as a separate Azure virtual machine (VM) or as an Azure Marketplace app. In either deployment, the Unravel app resides on an edge node to allow you to spin up\/down ephemeral Hadoop clusters. " }, 
{ "title" : "Unravel VM", 
"url" : "install/install-hdi.html#UUID-c86a5db3-5dda-d4e6-a53e-a41f884f44d8_id_AzureHDInsight-UnravelVM", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Unravel VM", 
"snippet" : "Installing Unravel as a separate Azure VM...", 
"body" : " Installing Unravel as a separate Azure VM " }, 
{ "title" : "Unravel App", 
"url" : "install/install-hdi.html#UUID-c86a5db3-5dda-d4e6-a53e-a41f884f44d8_id_AzureHDInsight-UnravelApp", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Unravel App", 
"snippet" : "Installing Unravel as an Azure Marketplace app The following reports require that you install the OnDemand service: Sessions Cluster Optimization Capacity Forecasting Small Files For installation instructions, see OnDemand...", 
"body" : " Installing Unravel as an Azure Marketplace app The following reports require that you install the OnDemand service: Sessions Cluster Optimization Capacity Forecasting Small Files For installation instructions, see OnDemand " }, 
{ "title" : "Option 1: Install Unravel on a Separate Azure VM", 
"url" : "install/install-hdi/install-hdi-part1-option1.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Option 1: Install Unravel on a Separate Azure VM", 
"snippet" : "This option involves the following steps: Create Azure Storage Step 1: Install Unravel Server for Azure HDInsight Cluster Step 2: Use Script Action to Configure HDInsight Cluster for Unravel...", 
"body" : "This option involves the following steps: Create Azure Storage Step 1: Install Unravel Server for Azure HDInsight Cluster Step 2: Use Script Action to Configure HDInsight Cluster for Unravel " }, 
{ "title" : "Create Azure Storage", 
"url" : "install/install-hdi/install-hdi-part1-option1/install-hdi-azure-storage.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Option 1: Install Unravel on a Separate Azure VM \/ Create Azure Storage", 
"snippet" : "Introduction This topic explains how to create Azure storage appropriate for your Hadoop cluster, which could be Kafka or Spark. First, you need to determine which storage type is appropriate for your cluster and supported by Unravel. You have the following options: Windows Azure Storage Blob By def...", 
"body" : " Introduction This topic explains how to create Azure storage appropriate for your Hadoop cluster, which could be Kafka or Spark. First, you need to determine which storage type is appropriate for your cluster and supported by Unravel. You have the following options: Windows Azure Storage Blob By default HDInsight 3.6 uses Blob storage, which is a general-purpose storage type for Big Data. Blob storage is a key-value store with a flat namespace. It has full support for: Analytics workloads; batch, interactive, streaming analytics Machine learning data such as log files, IoT data, click streams, large datasets Low-cost, tiered storage High availability\/disaster recovery Unravel doesn't support encryption (SSL) with Blob storage (WASB). Azure Data Lake Storage generation 1 The other major option for Hadoop clusters is ADLS v1. ADLS is a hierarchical file system. It has full support for Analytics workloads; batch, interactive, streaming analytics Machine learning data such as log files, IoT data, click streams, large datasets File system semantics File-level security Scalability Azure Data Lake Storage generation 2 Preview mode ADLS generation 2 combines the features of Blob storage andADLS generation 1. Unravel has not been tested with ADLS v2 since it is still in preview mode. For an in-depth comparison, see https:\/\/docs.microsoft.com\/en-us\/azure\/data-lake-store\/data-lake-store-comparison-with-blob-storage The rest of this document refers to these storage types as Blob ADLS Prerequisites You must already have an Azure account and able to log into https:\/\/portal.azure.com You must already have a resource group assigned to a region in order to group your policies, VMs, and storage blobs\/lakes\/drives. A resource group is a container that holds related resources for an Azure solution. In Azure, you logically group related resources such as storage accounts, virtual networks, and virtual machines (VMs) to deploy, manage, and maintain them as a single entity. You must already have a virtual network for your resource group. This virtual network will be shared by your Hadoop cluster and the Unravel VM. Steps Log into https:\/\/portal.azure.com Click Storage accounts + Add On the Basics Subscription Resource Group Storage Account Name Location Performance Standard Premium Standard Premium Account kind Blob Storage OR StorageV2 StorageV2 (ADLS v2) is still in preview mode and is not currently supported by Unravel. Replication Locally redundant storage (LRS) Zone-redundant storage (ZRS) Geo-redundant storage (GRS) Read-access geo-redundant storage (RA-GRS) FIXLINK: Handles failures in the data-center, zone, region, and allows read-access in another region. Durability guarantee is 16 9's. Access Tier Blob ADLS v2 hot Click the Advanced Set Secure transfer required Disabled Enabled Unravel doesn't support encryption (SSL) with Blob storage (WASB). For Virtual Networks, Click Review + create If your settings are correct, click Create Previous Related Resources Finding Unravel Properties' Values in Microsoft Azure Azure - creating a storage account Difference between Replication types " }, 
{ "title" : "Step 1: Install Unravel Server for Azure HDInsight Cluster", 
"url" : "install/install-hdi/install-hdi-part1-option1/install-hdi-part1.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Option 1: Install Unravel on a Separate Azure VM \/ Step 1: Install Unravel Server for Azure HDInsight Cluster", 
"snippet" : "Introduction This topic explains how to create a separate Azure VM, install the Unravel RPM, and configure it. Prerequisites You must already have an Azure account and able to log into https:\/\/portal.azure.com You must already have a resource group assigned to a region in order to group your policie...", 
"body" : "Introduction This topic explains how to create a separate Azure VM, install the Unravel RPM, and configure it. Prerequisites You must already have an Azure account and able to log into https:\/\/portal.azure.com You must already have a resource group assigned to a region in order to group your policies, VMs, and storage blobs\/lakes\/drives. A resource group is a container that holds related resources for an Azure solution. In Azure, you logically group related resources such as storage accounts, virtual networks, and virtual machines (VMs) to deploy, manage, and maintain them as a single entity. You must already have a virtual network and network security group set up for your resource group. Your virtual network and subnet(s) must be big enough to be shared by the Unravel VM and the target HDInsight cluster(s). You must have root privilege You must already have created a storage system. For instructions, see Create Azure Storage You must have an SSH key pair. Your VM host must meet the requirements below. Support Chart and VM Requirements: Azure HDI cluster compatibility HDInsight 3.6 Storage type: Blob (WASB) or ADLS v1 Limitations Unravel currently only works with Blob (WASB) or ADLS v1. It does not support multiple Azure Data Lake Storage accounts or ADLS v2 (preview). HDP 2.6.5 Spark 1.6.3, 2.1.0, 2.2.0, 2.3.0 Limitations Spark relies on the yarn-site configuration property yarn.log-aggregation.file-formats <property>\n <name>yarn.log-aggregation.file-formats<\/name>\n <value>TFile<\/value>\n<\/property> Hive 2.1 Kafka 0.10.0, Kafka 1.0, Kafka 1.1 (preview) Image (underlying operating system for the VM) RHEL 7 or CentOS 7.2 - 7.6 Note that the actual HDInsight Kafka\/Spark cluster can run another OS. CPU and RAM minimum requirements Minimum VM type suggested: Medium memory optimized such as Standard_E8s_v3 Cores: 8 min RAM: 64 GB min Disk requirement min 100GB for \/srv \/srv Network requirement Unravel VM should be located in the same VNET and VSNET as the HDInsight cluster Port 3000 (or 4020) for Unravel Web UI access UDP and TCP ports 4041-4043 open from Hadoop cluster to Unravel Server HDFS ports open from Hadoop cluster to Unravel Server Hive MetaStore DB port open to Unravel Server for partition reporting For Oozie, port 11000 open to Unravel Server Security requirement Allows inbound ssh to the unravel VM Allows outbound Internet access and all traffic within the subnet (VSNET). Allows TCP port 3000 and 4043 to Unravel VM from HDInsight cluster HIGHLIGHTED UNRAVEL_HOST_IP Provision an Azure VM for Unravel Server Log into https:\/\/portal.azure.com Select Virtual machines + Add On the Basics Subscription Resource Group Virtual machine name Region Availability options Image Size standard, memory optimized E8s_v3 Select your VM's Authentication type Best practice is to authenticate using an SSH public key, which you can generate using ssh-keygen Set Inbound Port Rules: If you plan on allowing external access to Unravel UI, then select Allow selected ports HTTPS SSH Click Next: Disks On the Disks OS disk type Premium SSD Standard SSD Advanced managed disks Data disks If you don't have a disk ready, click Create and attach new disk Otherwise, click Attach an existing disk Click Next: Networking On the Networking It is imperative that the VM, the Azure storage, and the cluster(s) you plan to monitor are all on the same virtual network and subnet(s). Virtual network Subnet https:\/\/www.aelius.com\/njh\/subnet_sheet.html NIC network security group: Basic Unravel Server works with multiple HDInsight clusters, including existing clusters and new clusters. A TCP and UDP connection is needed from the \" head node Add an inbound security policy to allow SSH access and 443 access to the Unravel node. The default security policy should allow all access within the VNET. Default rules start with a priority of 65000. Click Review + create Click Create It takes about 2 minutes to create your VM. When Azure completes the creation of your VM,click Go to resource Copy the VM's public IP address. Open an SSH session to your VM's public IP address. # ssh -i {ssh_key} {user}@{IP}\nVerify your IP as expected, example:\n Verify that eth0 on the new VM is bound to the private IP address shown in the Azure portal. # ifconfig\neth0 Link encap:Ethernet HWaddr 00:0d:3a:1b:c2:48\n inet addr:10.10.1.96 Configure the VM at First Login Install ntpd ntpd https:\/\/wiki.archlinux.org\/index.php\/Network_Time_Protocol_daemon # sudo su -\n# yum install ntp\n# ntpd -u ntp:ntp Disable Security Enhanced Linux (SELinux) permanently. This is important because HDFS maintains replication in different nodes\/racks, so setting firewall rules in SELinux will lead to performance degradation. # sudo setenforce Permissive Edit the file to make sure the setting persists after reboot, be sure to set, SELINUX=permissive # vi \/etc\/selinux\/config\nSELINUX=permissive\n\n:wq Install libaio.x86_64. # sudo yum -y install libaio.x86_64 Install lzop.x86_64 # sudo yum install lzop.x86_64 Disable the firewall and check your iptable rules. # sudo systemctl disable firewalld\n# sudo systemctl stop firewalld\n# sudo iptables -F\n# sudo iptables -L Prepare the second disk (for example, \/dev\/sdc fdisk -l # sudo su -\n\n\nList all disks and partitions\nYou should see one called \"sdc\" if you attached a 500-1000 GB disk.\n# fdisk -l\n# fdisk \/dev\/sdc\np (list current partitions)\nn (new partition)\np (primary)\nKeep accepting rest of default configs.\nw (save)\n\nFormat the disk\n# \/usr\/sbin\/mkfs -t ext4 \/dev\/sdc\n\n\n# mkdir -p \/srv\n\n# DISKUUID=`\/usr\/sbin\/blkid |grep ext4 |grep sdc | awk '{ print $2}' |sed -e 's\/\"\/\/g'`\n# echo $DISKUUID\n\nMount the disk on \/srv\n# echo \"${DISKUUID} \/srv ext4 defaults 0 0\" >> \/etc\/fstab\n# mount \/dev\/sdc1 \/srv\n\nVerify the disk space\n# df -hT \/srv\n\nFilesystem Type Size Used Avail Use% Mounted on\n\/dev\/sdc1 ext4 197G 61M 187G 1% \/srv\n\n\nSet permissions for Unravel and symlink Unravel's directories to the \/srv mount\n# mkdir -p \/srv\/local\/unravel\n# chmod -R 755 \/srv\/local\n# ln -s \/srv\/local\/unravel \/usr\/local\/unravel\n# chmod 755 \/usr\/local\/unravel Create the hdfs hadoop # sudo useradd hdfs\n# sudo groupadd hadoop\n# sudo usermod -a -G hadoop hdfs Install the Unravel Server RPM on the VM Get the Unravel Server RPM. Download the RPM from the Unravel distribution server to the Unravel VM. For instructions, see Download Unravel Software # cd \/tmp\n# Note that the same RPM is used for both EMR and HDInsight.\n# curl -u {USERNAME}:{PASSWORD} -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.4.3\/unravel-4.4.3.0-EMR-latest.rpm -o unravel-4.4.3.0-EMR-latest.rpm Install the Unravel Server RPM. The precise filename can vary, depending on how it was fetched or copied. The rpm rpm U # sudo rpm -U unravel-4.4.3.0-EMR-latest.rpm Run the specified await_fixups.sh If you're doing a routine upgrade, you can start all Unravel daemons, but don't stop or restart them until await_fixups.sh DONE # \/usr\/local\/unravel\/install_bin\/await_fixups.sh\nDONE Useful Information Dirs \/usr\/local\/unravel\/ unravel.properties \/etc\/init.d\/unravel_* \/etc\/init.d\/unravel_all.sh Subsequent RPM upgrades don't change unravel.properties User: unravel DB \/srv\/unravel\/ Config \/usr\/local\/unravel\/etc\/unravel.properties Logs \/usr\/local\/unravel\/logs\/ Grant access to Unravel Server Security Reminder Do not make Unravel Server UI TCP port 3000 accessible on the public Internet because doing so would violate your licensing terms. By default, a Public IP should be assigned to the Unravel VM . Create a security policy that allows ssh It is recommended that you use an SSH key to access the Unravel node. Modify Properties and Start Unravel Daemons Open an SSH session to the Unravel VM. # ssh -i {ssh_private_key} {ssh_user}@{UNRAVEL_HOST_IP} Set correct permissions on the Unravel configuration directory. # cd \/usr\/local\/unravel\/etc\n# sudo chown unravel:unravel *.properties\n# sudo chmod 644 *.properties Update unravel.ext.sh Check # Find the version of HDP that is installed by checking the HDP symlink. Take the first 2 digits, such as 2.6\n# You can also check https:\/\/docs.microsoft.com\/en-us\/azure\/hdinsight\/hdinsight-component-versioning#supported-hdinsight-versions \n# hdp-select status | grep hadoop\nhadoop-client - 2.6.5.3005-27\n\n# Append this classpath based on the version you found\n# echo \"export CDH_CPATH=\/usr\/local\/unravel\/dlib\/hdp2.6.x\/*\" >> \/usr\/local\/unravel\/etc\/unravel.ext.sh Run the \"switch user\" script. # \/usr\/local\/unravel\/install_bin\/switch_to_user.sh hdfs hadoop In \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.onprem This is optional at this time but is required later. echo \"com.unraveldata.onprem=false\" >> \/usr\/local\/unravel\/etc\/unravel.properties Modify other values in unravel.properties Property Description Required Example Values com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. Yes http:\/\/{LAN_DNS}:3000 com.unraveldata.customer.organization Identifies your installation for reporting purposes. Yes Company_and_org com.unraveldata.tmpdir Location where Unravel's temp file will reside Yes; this is set by installation \/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Sets retention for search data. optional 26 com.unraveldata.login.admins Unravel UI admin Yes; this is set by installation admin com.unraveldata.hdinsight.storage-account-name-1 Optional for Spark when HDInsight uses a blob storage storage account name for the HDInsight cluster Yes for Blob storage fs.azure.account.key.{STORAGE_NAME} com.unraveldat Primary storage account key Yes for Blob storage ABCDABCD ABCD ABCD ABCD ABCD ABCD ABCD ABCD ABCD ABCD ABCD com.unraveldat a.hdinsight Optional for Spark when HDInsight using blob storage Storage account name for the HDInsight cluster (same as account-name-1 Yes for Blob storage fs.azure.account.key. com.unraveldat a.hdinsight Secondary storage account key Yes for Blob storage ABCDEF\/ ABCDEFGHIJKLM \/ABCD ABCD ABCD ABCD ABCD ABCD AB com.unraveldat a.adl.accountFQDN The data lake fully qualified domain name. Yes for ADLS datalake0001.azuredatalakestore.net com.unraveldat a.adl.clientld An application ID. An application registration has to be created in the Azure Active Directory Yes for ADLS 12345678-1234-1234-1234-123456789ABC com.unraveldat a.adl.clientKey An application access key which can be created after registering an application Yes for ADLS ABCD ABCD ABCD ABCD ABCD ABCD ABCD ABCD ABCD ABC= com.unraveldat a.adl.accessTokenEndpoint The OAUTH 2.0 Access Token Endpoint. It is obtained from the application registration tab on Azure portal Yes for ADLS https:\/\/login.microsoftonline.com\/ABCDABCD-ABCD-ABCD-ABCD-ABCDABCDABCD\/oauth2\/token com.unraveldat a.adl.clientRootPath It is the path in the Data lake store where the target cluster has been given access. Yes for ADLS \/clusters\/{CLUSTERNAME} com.unraveldat a.ext.kafka.clusters Name of Kafka cluster. The display name show on the Unravel UI to define Kafka cluster. Other Unravel Kafka properties depends on this name {CLUSTERNAME} mandatory for HDI Kafka udkafka com.unraveldat a.ext.kafka. Kafka cluster bootstrap server and port (usually are two worker nodes) mandatory for HDI Kafka wn0-UDKAFK:9092,wn1-UDKAFK:9092 com.unraveldat a.ext.kafka. Define Kafka cluster broker servers names mandatory for HDI Kafka broker-1,broker-2,broker-3 com.unraveldat a.ext.kafka. Define jmx broker-1 host mandatory for HDI Kafka wn0-UDKAFK com.unraveldat a.ext.kafka. Define jmx broker-1 port mandatory for HDI Kafka 9999 com.unraveldat a.ext.kafka. Define jmx broker-2 host mandatory for HDI Kafka wn1-UDKAFK com.unraveldat a.ext.kafka. Define jmx broker-2 port mandatory for HDI Kafka 9999 com.unraveldat a.ext.kafka. Define jmx broker-3 host mandatory for HDI Kafka wn2-UDKAFK com.unraveldat a.ext.kafka. Define jmx broker-3 port mandatory for HDI Kafka 9999 Update the following properties for an HDInsight cluster, depending on whether you're using Blob storage or ADLS Set these properties with values you obtain from Azure. For help in locating the right values, see Finding Unravel Properties' Values in Microsoft Azure For Blob storage, update: com.unraveldata.hdinsight.storage-account-name-1\ncom.unraveldata.hdinsight.primary-access-key\ncom.unraveldata.hdinsight.storage-account-name-2\ncom.unraveldata.hdinsight.secondary-access-key For ADLS, update: com.unraveldata.adl.accountFQDN\ncom.unraveldata.adl.clientld\ncom.unraveldata.adl.clientKey\ncom.unraveldata.adl.accessTokenEndpoint\ncom.unraveldata.adl.clientRootPath Restart Unravel Server Whenever making edits to com.unraveldata.login.admins \/usr\/local\/unravel\/etc\/unravel.properties The echo If you are using an SSH tunnel or HTTP proxy, you might need to make adjustments to the host\/IP of the URL: # sudo \/etc\/init.d\/unravel_all.sh restart\n# sleep 60\n# echo \"http:\/\/({UNRAVEL_HOST_IP} -f):3000\/\" Log into Unravel UI Create an SSH # ssh -i {ssh_private_key} {ssh_user}@${UNRAVEL_HOST_IP} -L 3000:127.0.0.1:3000 Using a web browser, navigate to http:\/\/127.0.0.1:3000 admin unraveldata For the free trial version, use the Chrome browser. Congratulations! Unravel Server is up and running. Proceed to Step 2: Use Script Action to Configure HDInsight Cluster for Unravel " }, 
{ "title" : "Step 2: Use Script Action to Configure HDInsight Cluster for Unravel", 
"url" : "install/install-hdi/install-hdi-part1-option1/install-hdi-part2.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Option 1: Install Unravel on a Separate Azure VM \/ Step 2: Use Script Action to Configure HDInsight Cluster for Unravel", 
"snippet" : "Table of Contents Introduction Prerequisites Option A: Deploy a New Cluster Option B: Configure an Existing Hadoop, Hive, or Spark Cluster Option C: Configure an Existing Kafka Cluster Troubleshooting Tips Introduction This guide explains how to spin up a Hadoop, Hive, Spark, or Kafka cluster and co...", 
"body" : "\n Table of Contents \n Introduction \n Prerequisites \n Option A: Deploy a New Cluster \n Option B: Configure an Existing Hadoop, Hive, or Spark Cluster \n Option C: Configure an Existing Kafka Cluster \n Troubleshooting Tips Introduction This guide explains how to spin up a Hadoop, Hive, Spark, or Kafka cluster and connect it to Unravel Server. Before Unravel can analyze any job running on your HDInsight cluster, Unravel agent and sensors must be deployed on the cluster nodes through the Azure \" Script action There are two kinds of Unravel \" script actions Note: For HDInsight cluster without Internet access, you can download these scripts and store them in your Azure blob storage and use the blob storage URI on the script action's \" Bash script URI \n \n \n Cluster Type \n Download path \n Supported HDI cluster(s) \n Apply to cluster node type(s) \n \n Hadoop, Hive, or Spark \n \n unravel_hdi_spark_bootstrap_3.0.sh \n Hadoop 2.7.3 Hive 2.1 Spark 2.0, 2.1, 2.3 \n Head Node, Worker Node, Edge node \n \n Kafka \n \n unravel_hdi_kafka_bootstrap.sh \n Kafka 0.10.0, Kafka 1.0.0, Kafka 1.1.0 \n Head Node Checks before running script action Read the latest documentation on the ports required by HDInsight: https:\/\/docs.microsoft.com\/en-us\/azure\/hdinsight\/hdinsight-hadoop-port-settings-for-services Ensure Unravel service is running on Unravel VM and ports 3000 and 4043 are reachable from the Azure HDInsight cluster master node before running the the Unravel \"script action\" script. E.g., # ssh -i {ssh_key} {ssh_user}@{UNRAVEL_HOST_IP}\n# sudo su -\n# netstat -anp | grep 3000\ntcp 0 0 0.0.0.0:3000 0.0.0.0:* LISTEN 65072\/node\n# hostname\n\nOn one of the cluster's head nodes.\n# ping {UNRAVEL_HOST_IP} Depending on the type of cluster you are deploying, follow one of these options: \n Option A: Deploy a new cluster \n Option B: Configure an existing Hadoop, Hive, or Spark cluster \n Option C: Configure an existing Kafka cluster Prerequisites You must already have an Unravel VM on Azure HDInsight running and the Unravel UI available on port 3000. For instructions, see Step 1: Install Unravel Server for Azure HDInsight Cluster If you plan to create a cluster, you must have the following information ready: Virtual Network and subnet of the Unravel VM Your Azure Storage details. For storage setup, see Create Azure Storage Option A: Deploy a New Cluster Login into the Azure portal ( https:\/\/portal.azure.com HDInsight clusters Add In the Security + networking In the Storage In the \" Cluster size \n Optional Script action In the Summary - Confirm configurations Option B: Configure an Existing Hadoop, Hive, or Spark Cluster Login into the Azure portal ( https:\/\/portal.azure.com HDInsight cluster, Click on Script actions Submit new Create \n \n \n Script type \n Custom \n \n Name \n unravel-script-01 (or any name to identify this script action run) \n \n Bash script URI \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh \n \n Node type(s) \n Head, Worker, Edge (only if you have deployed edge node) \n \n Parameters \n \n --unravel-server unravel_server_private_ip spark_version e.g. --unravel-server 10.10.1.10:3000 --spark-version 2.3.0\n\nTo UNDO the changes use --uninstall parameter\ne.g. --unravel-server 10.10.1.10:3000 --spark-version 2.3.0 --uninstall \n \n Persist this script action \n Checked. Note that persistence only applies on new Head and Worker nodes Option C: Configure an Existing Kafka Cluster Login into the Azure portal ( https:\/\/portal.azure.com HDInsight cluster, If the Kafka cluster has no Internet access; then download the HDInsightUtilities-v01.sh script and scp\/download it to the Kafka \"head node\" \/tmp folder . wget -O \/tmp\/HDInsightUtilities-v01.sh -q https:\/\/hdiconfigactions.blob.core.windows.net\/linuxconfigactionmodulev01\/HDInsightUtilities-v01.sh Click Script actions Submit new \n \n \n Script type \n Custom \n \n Name \n unravel-script-01 (or any name to identify this script action run) \n \n Bash script URI \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-kafka-script-action\/unravel_hdi_kafka_bootstrap.sh \n \n Node type(s) \n Head \n \n Parameters \n \n --unravel-server unravel_server_private_ip e.g. --unravel-server 10.10.1.10:3000 \n \n Persist this script action \n Checked. Note that persistence only applies on new Head nodes Click Create After the Kafka script action script completed successfully, ssh to the Kafka cluster's \"head node\" and append the content of \/tmp\/unravel\/unravel.ext.properties nravel.ext.properties # Adding Kafka properties\ncom.unraveldata.ext.kafka.clusters=<cluster_name>\ncom.unraveldata.ext.kafka.<cluster_name>.bootstrap_servers=wn0-<cluster_name>:9092,wn1-<cluster_name>:9092\ncom.unraveldata.ext.kafka.<cluster_name>.jmx_servers=broker1,broker2\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker1.host=wn0-<cluster_name>\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker1.port=9999\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker2.host=wn1-<cluster_name>\ncom.unraveldata.ext.kafka.<cluster_name>.jmx.broker2.port=9999 Unravel VM must have access to the Kafka worker nodes' broker port 9092 and Kafka JMX port 9999 After updating the Kafka properties, you will need to restart the Unravel server. sudo \/etc\/init.d\/unravel_all.sh restart Troubleshooting Tips From the Azure portal, you can check if a script action finished successfully by checking the SCRIPT ACTION HISTORY . If script action process fails, you can check the error messages from the HDInsight cluster's Ambari dashboard, which has a balloon next to the cluster name on the top menu bar with the recent operations. Click on the \"ops\" button and search for the most recent \"run_customscriptaction\" command and inspect the log messages. You may see multiple entries of \"run_customscriptaction\" which were created by previous runs. The Unravel script action cannot be rerun. If you need to redeploy the Unravel script action, you must submit a new \"script action\" script with a different name. " }, 
{ "title" : "Option 2: Install Unravel's Azure Marketplace App", 
"url" : "install/install-hdi/install-hdi-part1-option2.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Option 2: Install Unravel's Azure Marketplace App", 
"snippet" : "This topic explains how to to install Unravel's Azure Marketplace app on a fresh Spark 2.1 cluster. The Unravel HDInsight app currently only supports Spark 2.1 and 2.2 HDInsight clusters running on either blob (WASB) or ADL (Azure Data Lake) storage. Launch a Spark 2.1 Cluster Log into the Azure por...", 
"body" : "This topic explains how to to install Unravel's Azure Marketplace app on a fresh Spark 2.1 cluster. The Unravel HDInsight app currently only supports Spark 2.1 and 2.2 HDInsight clusters running on either blob (WASB) or ADL (Azure Data Lake) storage. Launch a Spark 2.1 Cluster Log into the Azure portal. Select the HDInsight service. Create a new cluster: For cluster type, select Spark For version 2.1 Enter the access credentials for the Spark cluster. Click Next. Set Up a Storage Account for the Cluster Create a new storage account or use existing one. Fill in the storage account information for the Spark cluster. Find the Unravel App on Azure Marketplace Enter UNRAVEL Available applications Click OK Click Create Accept the terms of use and privacy policy. Click Next Launch the Cluster and Unravel App Review the summary. Change the worker node size or number on step 4. You can change the edge node size for Unravel app if you wish. Click Create Find the URL of the Unravel App After the Unravel app and the Spark2 cluster have launched successfully, go to the Azure portal and select the HDInsight service. Select the Spark2 cluster. Click Applications Click unravel-edgenode The Unravel HDInsight app's webpage URL is displayed. In most cases, the URL looks like https:\/\/clusterName-unr.apps.azurehdinsight.net\/ Log into Unravel UI In your web browser, navigate to the Unravel app's URL, https:\/\/clusterName-apps.azurehdinsight.net. The Unravel login screen appears. Log in with username admin The dashboard appears. (Optional) Start\/Stop Unravel Daemons Use ssh \/usr\/local\/unravel\/init_scripts\/unravel_all.sh status To restart Unravel daemons enter: \/usr\/local\/unravel\/init_scripts\/unravel_all.sh restart To stop Unravel daemons enter: \/usr\/local\/unravel\/init_scripts\/unravel_all.sh stop Enter Your License By default Unravel app doesn't contains any license keys, and runs without any issue during the initial 30 days trial period. To continue using Unravel app and technical support, contact our sales team. Support contact: azuresupport@unraveldata.com License contact: sales@unraveldata.com Unraveldata Main number: (650) 741-3442 Get Started with Unravel UI See the Unravel User Guide Get Started with Unravel API Unravel provides REST API for some operations. To try the API, click the API An API page with available command options are displayed and explained. You can try the API by clicking \"Try it out\" ? Execute buttons as shown below. From the Unravel user interface, trying out the API results in an error \"TypeError: Failed to fetch\" because the generated curl Copy the generated curl Modify it to include default user credentials. For example: ## From original \ncurl -X GET \"http:\/\/CLUSTERNAME-unr.apps.azurehdinsight.net\/api\/v1\/clusters\/nodes\" -H \"accept: application\/json\"\n\n## Change to \ncurl -u admin:unraveldata -X GET \"https:\/\/CLUSTERNAME-unr.apps.azurehdinsight.net\/api\/v1\/clusters\/nodes\" -H \"accept: application\/json\"\n Re-send it using the HTTPS protocol. The response body is in JSON format. The date field is in epoch time. {\n \"date\":[1525294800000,1525298400000],\n \"total\":{\"1525294800000\":3,\"1525298400000\":3},\n \"active\":{\"1525294800000\":3,\"1525298400000\":3},\n \"lost\":{\"1525294800000\":0,\"1525298400000\":0},\n \"unhealthy\":{\"1525294800000\":0,\"1525298400000\":0},\n \"decommissioned\":{\"1525294800000\":0,\"1525298400000\":0},\n \"rebooted\":{\"1525294800000\":0,\"1525298400000\":0}\n } " }, 
{ "title" : "INTERNAL: Update the Azure Resource Manager if Needed", 
"url" : "install/install-hdi/internal-install-hdi-update-azure-rm.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ INTERNAL: Update the Azure Resource Manager if Needed", 
"snippet" : "For the dev team, you may need to update the ARM (Azure Resource Manager) Template for Kafka or Spark clusters: INTERNAL: Step 3: Create Unravel VM Using ARM template INTERNAL: Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions INTERNAL: Step 5: ARM Template for Kafka Clus...", 
"body" : "For the dev team, you may need to update the ARM (Azure Resource Manager) Template for Kafka or Spark clusters: INTERNAL: Step 3: Create Unravel VM Using ARM template INTERNAL: Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions INTERNAL: Step 5: ARM Template for Kafka Cluster with Unravel Script Actions " }, 
{ "title" : "INTERNAL: Step 3: Create Unravel VM Using ARM template", 
"url" : "install/install-hdi/internal-install-hdi-update-azure-rm/internal--step-3--create-unravel-vm-using-arm-template.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ INTERNAL: Update the Azure Resource Manager if Needed \/ INTERNAL: Step 3: Create Unravel VM Using ARM template", 
"snippet" : "Unravel VM can be deployed using Azure Resource Manager (ARM) template. This can automate the Unravel VM node setup; however the ARM template needs to be updated to reflect your specific environment. The following ARM templates and parameter JSON file are for reference only. You need to update or cr...", 
"body" : "Unravel VM can be deployed using Azure Resource Manager (ARM) template. This can automate the Unravel VM node setup; however the ARM template needs to be updated to reflect your specific environment. The following ARM templates and parameter JSON file are for reference only. You need to update or create your own template files. This example template creates an Azure \"standard E8s V3\" VM in the existing VNET and subnet, and it adds a data disk on the VM for \"\/ srv\" You will need to update VNET, subnetRef, vmName, adminPassword before running the ARM template to create VM Optionally change the size of data disk; it is currently set to 500G. If you change the disk size, update unravel-setup.sh For Centos 7.3 Unravel VM ARM Template https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-centos73x\/azuredeploy.json P https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-centos73x\/azuredeploy.parameters.json For Redhat 7.4 Unravel VM ARM Template https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-redhat74x\/azuredeploy.json P https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-redhat74x\/azuredeploy.parameters.json Both ARM template and parameter files have to be modified to fit your Azure environment This ARM template embedded with Azure Extension script to download and install Unravel RPM. The Azure Extension script for Unravel RPM installation Extension Script for CentOS 7.3 https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-centos73x\/unravel-setup.sh Extension Script for CentOS 7.4 https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/unravel-VM-redhat74x\/unravel-setup.sh The custom extension script will fix most of the basic unravel configuration; however you will have to manually edit \/usr\/local\/unravel\/etc\/unravel.properties file for blob storage account or data lake store access information. Please see Step 1 Below is the content of this extension script # Download unravel rpm\n\/usr\/bin\/wget http:\/\/preview.unraveldata.com\/unravel\/RPM\/4.2.7\/Azure\/unravel-4.2.7-Azure-latest.rpm\n\nBLOBSTORACCT=${1}\nBLOBPRIACKEY=${2}\nBLOBSECACKEY=${3}\n\nDLKSTOREACCT=${4}\nDLKCLIENTAID=${5}\nDLKCLIENTKEY=${6}\nDLKCLITOKEPT=${7}\nDLKCLIROPATH=${8}\n\n\n# Prepare the VM for unravel rpm install\n\/usr\/bin\/yum install -y ntp\n\/usr\/bin\/yum install -y libaio\n\/usr\/bin\/yum install -y lzop\n\/usr\/bin\/systemctl enable ntpd\n\/usr\/bin\/systemctl start ntpd\n\/usr\/bin\/systemctl disable firewalld\n\/usr\/bin\/systemctl stop firewalld\n\n\/usr\/sbin\/iptables -F\n\n\/usr\/sbin\/setenforce 0\n\/usr\/bin\/sed -i 's\/enforcing\/disabled\/g' \/etc\/selinux\/config \/etc\/selinux\/config\n\nsleep 30\n\n\n# Prepare disk for unravel\nmkdir -p \/srv\n\nDATADISK=`\/usr\/bin\/lsblk |grep 500G | awk '{print $1}'`\necho $DATADISK > \/tmp\/datadisk\necho \"\/dev\/${DATADISK}1\" > \/tmp\/dataprap\n\necho \"Partitioning Disk ${DATADISK}\"\necho -e \"o\\nn\\np\\n1\\n\\n\\nw\" | fdisk \/dev\/${DATADISK}\n\nDATAPRAP=`cat \/tmp\/dataprap`\nDDISK=`cat \/tmp\/datadisk`\n\/usr\/sbin\/mkfs -t ext4 ${DATAPRAP}\n\nDISKUUID=`\/usr\/sbin\/blkid |grep ext4 |grep $DDISK | awk '{ print $2}' |sed -e 's\/\"\/\/g'`\necho \"${DISKUUID} \/srv ext4 defaults 0 0\" >> \/etc\/fstab\n\n\/usr\/bin\/mount -a\n\n# install unravel rpm\n\/usr\/bin\/rpm -U unravel-4.2.7-Azure-latest.rpm\n\n\/usr\/bin\/sleep 5\n\n\n# Update Unravel Lic Key into the unravel.properties file\n# Obtain a valid unravel Lic Key file ; the following is just non working one\necho \"com.unraveldata.lic=1p6ed4s492012j5rb242rq3x3w702z1l455g501z2z4o2o4lo675555u3h\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"export CDH_CPATH=\"\/usr\/local\/unravel\/dlib\/hdp2.6.x\/*\"\" >> \/usr\/local\/unravel\/etc\/unravel.ext.sh\n\n# Update Azure blob storage account credential in unravel.properties file\n# Update and uncomment the following lines to reflect your Azure blob storage account name and keys\n\nif [ $BLOBSTORACCT != \"NONE\" ] && [ $BLOBPRIACKEY != \"NONE\" ] && [ $BLOBSECACKEY != \"NONE\" ]; then\n\n echo \"blob storage account name is ${BLOBSTORACCT}\"\n echo \"blob primary access key is ${BLOBPRIACKEY}\"\n echo \"blob secondary access key is ${BLOBSECACKEY}\"\n echo \"# Adding Blob Storage Account information, Update and uncomment following lines\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.storage-account-name-1=fs.azure.account.key.${BLOBSTORACCT}.blob.core.windows.net\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.primary-access-key=${BLOBPRIACKEY}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.storage-account-name-2=fs.azure.account.key.${BLOBSTORACCT}.blob.core.windows.net\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.hdinsight.secondary-access-key=${BLOBSECACKEY}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n\nelse\n echo \"One or more of your blob storage account parameter is invalid, please check your parameter file\"\nfi\n\nsleep 3\n\nif [ $DLKSTOREACCT != \"NONE\" ] && [ $DLKCLIENTAID != \"NONE\" ] && [ $DLKCLIENTKEY != \"NONE\" ] && [ $DLKCLITOKEPT != \"NONE\" ] && [ $DLKCLIROPATH != \"NONE\" ]; then\n\n echo \"Data Lake store name is ${DLKSTOREACCT}\"\n echo \"Data Lake Client ID is ${DLKCLIENTAID}\"\n echo \"Data Lake Client Key is ${DLKCLIENTKEY}\"\n echo \"Data Lake Access Token is ${DLKCLITOKEPT}\"\n echo \"Data Lake Client Root Path is ${DLKCLIROPATH}\"\n echo \"# Adding Data Lake Account information, Update and uncomment following lines\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.accountFQDN=${DLKSTOREACCT}.azuredatalakestore.net\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.clientId=${DLKCLIENTAID}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.clientKey=${DLKCLIENTKEY}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.accessTokenEndpoint=${DLKCLITOKEPT}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n echo \"com.unraveldata.adl.clientRootPath=${DLKCLIROPATH}\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n\nelse\n echo \"One or more of your data lake storge parameter is invalid, please check your parameter file\"\nfi\n\n# Adding unravel properties for Azure Cloud\n\necho \"com.unraveldata.onprem=false\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"com.unraveldata.spark.live.pipeline.enabled=true\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"com.unraveldata.spark.appLoading.maxAttempts=10\" >> \/usr\/local\/unravel\/etc\/unravel.properties\necho \"com.unraveldata.spark.appLoading.delayForRetry=4000\" >> \/usr\/local\/unravel\/etc\/unravel.properties\n\n# Starting Unravel daemons\n# uncomment below will start unravel daemon automatically but within unravel_all.sh start will have exit status=1.\n# Thus we recommend login to unravel VM and run unravel_all.sh manually\n# \/etc\/init.d\/unravel_all.sh start 1. Download the ARM template and parameter JSON files onto your configured Azure CLI workstation. 2. Use the Azure CLI to deploy Unravel VM using this template and parameters JSON file. To Validate template before deployment. # az group deployment validate --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json # az group deployment create --name deploymentname --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json 2. Once unravel VM creation completed; ssh to the VM using your defined ssh user then manually start unravel daemons. # \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "INTERNAL: Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions", 
"url" : "install/install-hdi/internal-install-hdi-update-azure-rm/internal--step-4--arm-template-for-spark2-hdinsight-cluster-with-unravel-script-actions.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ INTERNAL: Update the Azure Resource Manager if Needed \/ INTERNAL: Step 4: ARM template for Spark2 HDinsight cluster with Unravel Script Actions", 
"snippet" : "Install Spark 2.1 Cluster with Unravel script action script This ARM template allows you to create a Linux-based Spark2 HDInsight cluster and a Spark edge node. This template also runs the Unravel's Script Actions script to setup Unravel Sensors and configuration on header, worker, and edge nodes. T...", 
"body" : "Install Spark 2.1 Cluster with Unravel script action script This ARM template allows you to create a Linux-based Spark2 HDInsight cluster and a Spark edge node. This template also runs the Unravel's Script Actions script to setup Unravel Sensors and configuration on header, worker, and edge nodes. This ARM template uses the existing VNET, Subnet, and Storage Account on the same resource group. You need to update those values in the parameter variables to reflect your Azure environment. A Spark edge node is a Linux virtual machine with the same client tools installed and configured as in the headnodes. You can use Spark edge node for accessing the cluster and testing\/hosting your client applications. Substitute your local values for text in red, i.e., UNRAVEL_IP. You will need to deploy unravel VM and update the script action parameters --unravel-server UNRAVEL_IP Optionally you can change the VM size of header, worker and edge nodes; and currently they are all using VM size of \"Standard_D3_v2\" \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-spark2.1\/azuredeploy.json \n \n Parameter file \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-spark2.1\/azuredeploy.parameters.json \n \n Unravel Script Action URI \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh After modifying this template please validate it before applying. HDInsight cluster creation takes about 15 - 25 minutes Install using CLI. 1. Download the ARM template and parameter JSON files into your configured Azure CLI workstation. 2. Validate template before deployment. # az group deployment validate --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json 3. Use the Azure CLI to deploy Spark 2.1 cluster using this template and parameters JSON file. # az group deployment create --name deploymentname --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json Optionally, Install manually on an existing Spark2 cluster. 1. From Azure portal, click the resource of the target Spark2 cluster under your resource group and click \"Script actions\". In Script Actions dialog box: Click Submit new Select script type \"- Custom\" Enter a Name for this script, e.g., \" unravel-spark-setup\" Enter the script path from above e.g., https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh Use input parameters: --unravel-server UNRAVEL_VM_IP_Address:30000 --spark-version 2.1.0 Check the box Persist this script action to rerun when .. Click Create The checkbox for Persist this script action to rerun when ... does not Script Action You can upload the unravel_hdi_bootstrap.sh 2. Script Action validates the script and then processes it. Monitor the Azure portal until script actions has completed. 3. After completion, login to Ambari and check the Ambari task status. Install additional edge node on existing HDinsight cluster. The ARM template for install edge node with Unravel Script Action only \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node\/azuredeploy.json \n \n Parameter file \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node\/azuredeploy.parameters.json Use the ARM template to install the edge node with your custom Install Script Action script and Unravel Script Action (two scripts are run in this example). In this example, an edge node will be created first. Next, it runs emptynode-setup.sh unravel_hdi_bootstrap.sh \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node-ms\/azuredeploy.json \n \n Parameter file \n \n https:\/\/github.com\/unravel-data\/public\/blob\/master\/hdi\/ARM-templates\/HDinsight-edge-node-ms\/azuredeploy.parameters.json Use of the above ARM template for edge node requires change in scriptActionUri path and application name in variables and also parameters for cluster name. Adjust the ARM templates for your setup and validate it before using. " }, 
{ "title" : "INTERNAL: Step 5: ARM Template for Kafka Cluster with Unravel Script Actions", 
"url" : "install/install-hdi/internal-install-hdi-update-azure-rm/internal--step-5--arm-template-for-kafka-cluster-with-unravel-script-actions.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ INTERNAL: Update the Azure Resource Manager if Needed \/ INTERNAL: Step 5: ARM Template for Kafka Cluster with Unravel Script Actions", 
"snippet" : "This ARM template allows you to create a Linux-based Kafka cluster with Unravel setup on predefined Unravel VM. The Unravel Script Actions This template runs Unravel's Script Actions unravel.properties This ARM template uses an existing VNET, Subnet and Storage Account on the same resource group. Th...", 
"body" : "This ARM template allows you to create a Linux-based Kafka cluster with Unravel setup on predefined Unravel VM. The Unravel Script Actions This template runs Unravel's Script Actions unravel.properties This ARM template uses an existing VNET, Subnet and Storage Account on the same resource group. The worker nodes in this Kafka cluster use two data disks per node. You need to deploy unravel VM and update the Script Actions UNRAVEL__IP \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-kafka\/azuredeploy.json \n \n Parameter file \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-kafka\/azuredeploy.parameters.json \n \n Unravel Script Action URI \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-kafka-script-action\/unravel_hdi_kafka_bootstrap.sh Validate the modified ARM template before applying. The HDInsight cluster creation takes about 15 - 25 minutes Apply Script Actions via CLI. 1. Download the ARM template and parameter JSON files into your configured Azure CLI workstation. 2. Modify parameter file. Change the parameter values to reflect your Azure environment, i.e., VNET, Subnet, StorageAccount, Cluster name, etc. You can change the VM size of header, worker nodes. By default they use \"Standard_D3_v2\" as the VM size. 3. Validate the template before deployment. # az group deployment validate --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json 4. Use Azure CLI to deploy the Kafka cluster using the template and parameters JSON file. # az group deployment create --name deploymentname --resource-group RESOURCEGROUPNAME --template-file azuredeploy.json --parameters azuredeploy.parameters.json 5. After the Kafka cluster is successfully created, the unravel Script Actions \/usr\/local\/unravel\/etc\/unravel.properties The following is the example of the lines appended to unravel.properties com.unraveldata.ext.kafka.clusters=seuguiko98003\ncom.unraveldata.ext.kafka.seuguiko98003.bootstrap_servers=wn0-seugui:9092,wn1-seugui:9092\ncom.unraveldata.ext.kafka.seuguiko98003.jmx_servers=broker1,broker2\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker1.host=wn0-seugui\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker1.port=9999\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker2.host=wn1-seugui\ncom.unraveldata.ext.kafka.seuguiko98003.jmx.broker2.port=9999 6. Once unravel.properties unravel_km # \/etc\/init.d\/unravel_km restart Alternatively, apply Unravel Kafka Script Actions scripts manually on an existing Kafka cluster. If you already have an existing Kafka cluster, you can apply Unravel's Kafka script via Azure portal. 1. From Azure portal, click the resource of the target Kafka cluster under your resource group and click Script Actions 2. In the Script Actions dialog box: Click Submit New Select script type, e.g., \"- Custom\" Enter a Name for this script, e.g. \" unravel-kafka-setup\" Enter the script path from above, e.g. https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-kafka-script-action\/unravel_hdi_kafka_bootstrap.sh Input parameters: UNRAVEL_VM_IP_Address Check the box \" Persist this script action to rerun You can upload the unravel_hdi_kafka_bootstrop.sh 3. Script Action validates the script and then processes it. Monitor the Azure portal until script actions has completed 4. After completion, login to Ambari of the cluster and check the Ambari task status. 4. Login to unravel VM and restart the unravel Kafka monitor daemon, unravel_km. # \/etc\/init.d\/unravel_km restart " }, 
{ "title" : "Upgrading the Unravel VM or App", 
"url" : "install/install-hdi/upgrading-the-unravel-vm-or-app.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Upgrading the Unravel VM or App", 
"snippet" : "From time to time, Unravel releases new VMs\/apps with new features and improvements. To check for updates, go to Download Unravel Software Versions Upgrading the Unravel VM or app should not affect the connected HDInsight cluster operation and can be done at any time. However, in some cases you'll a...", 
"body" : "From time to time, Unravel releases new VMs\/apps with new features and improvements. To check for updates, go to Download Unravel Software Versions Upgrading the Unravel VM or app should not affect the connected HDInsight cluster operation and can be done at any time. However, in some cases you'll also need to upgrade your Unravel Sensor(s) as well, and this requires you to re-submit the Unravel action scripts to head, worker, and edge nodes. " }, 
{ "title" : "Empty or missing topic", 
"url" : "install/install-hdi/empty-or-missing-topic.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Using Azure HDInsight APIs", 
"url" : "install/install-hdi/using-azure-hdinsight-apis.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ Azure HDInsight \/ Using Azure HDInsight APIs", 
"snippet" : "Submit a Script Action: Pre-requirements install Azure CLI shell Tip: The quickest way to install Azure CLI shell is on a docker container. 1. Login to the Azure CLI shell $ az login To sign in, use a web browser to open the page https:\/\/microsoft.com\/devicelogin 2. Run the script action. First, ref...", 
"body" : "Submit a Script Action: \n Pre-requirements install Azure CLI shell Tip: The quickest way to install Azure CLI shell is on a docker container. 1. Login to the Azure CLI shell $ az login To sign in, use a web browser to open the page https:\/\/microsoft.com\/devicelogin 2. Run the script action. First, refer to the script you want to run and ensure you have its proper parameters. You may need to remove any \"--\" from the parameters. $ azure hdinsight script-action create $CLUSTER -g $RESOURCEGROUP -n $SCRIPTNAME -u $SHELLSCRIPT -p 'unravel-server $PRIVATEIP:3000 spark-version $MAJOR.$MINOR.$PATCH' -t \"headnode;workernode;edgenode\" -g = Resource Group name -n = Name of this script action task -u = script path -p = paramaters -t = node types \nE.g., $ azure hdinsight script-action create DEVCLUSTER -g UNRAVEL01 -n unravel-script-action -u https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh Create an Edge Node: An Edge Node 1. Determine which ARM template and parameter file to download to the workstation that contains Azure CLI. A. Edge node that also runs the Unravel script ( recommended \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node-ms\/azuredeploy.json \n \n Parameter file \n \n https:\/\/github.com\/unravel-data\/public\/blob\/master\/hdi\/ARM-templates\/HDinsight-edge-node-ms\/azuredeploy.parameters.json OR B. Edge node that only simple a emptynode-setup.sh script \n \n \n ARM Template \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node\/azuredeploy.json \n \n Parameter file \n \n https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/ARM-templates\/HDinsight-edge-node\/azuredeploy.parameters.json 2. Download the ARM template and parameter JSON files into your configured Azure CLI workstation. curl <file> -o <name.json> 3. Modify the VM type, parameters, Kafka\/Spark version, etc. For example, In the ARM Template file, edit these fields as appropriate. \"vmSize\": \"Standard_D3_v2\" \"parameters\": \"unravel-server $PRIVATEIP:3000 spark-version $MAJOR.$MINOR.$PATCH\" \"applicationName1\": \"$NEW_EDGE_NODE_HOSTNAME\" In the Parameter file, modify the cluster name. E.g., \"clusterName\": {\n \"value\": \"$MY_CLUSTER_NAME\"\n} 4. Validate template before deployment. \n \n \n \n $ az group deployment validate --resource-group \"$RESOURCEGROUP\" --template- file azuredeploy.json --parameters azuredeploy.parameters.json { \"error\": null, ... \"provisioningState\": \"Succeeded\", ... } 5. Create the edge node. This should take 10-15 minutes to run since it has to provision a VM and install the Hadoop binaries. \n \n \n \n $ az group deployment create --name deploymentname --resource-group \"$RESOURCEGROUP\" --template- file azuredeploy.json --parameters azuredeploy.parameters.json 6. Verify it was added to Ambari. Auto-Scaling: HDInsight allows you to resize your cluster up\/down to meet your current demands. 1. From the Azure portal, navigate to HDInsight Clusters <Your Cluster> Cluster Size 2. Enter your desired number of workers and validate that you have enough resources for your resource group and region (based on any quotas). Click the Save button, and HDInsight will take the appropriate action. Down-size: Will run the \" Decommission Up-size: Will provision new VM, install the Hadoop bits, and add the worker components (DataNode, NodeManager, and potentially HBase RegionServer). Notice that if the Unravel script action was also \"persisted\" to run on \"worker nodes\", then new VMs will automatically run a custom command for the Unravel bootstrap script. " }, 
{ "title" : "MySQL", 
"url" : "install/install-mysql.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ MySQL", 
"snippet" : "Install and Configure MySQL for Unravel Move MySQL to a Custom Location MySql Partitioning and Data Migration...", 
"body" : " Install and Configure MySQL for Unravel Move MySQL to a Custom Location MySql Partitioning and Data Migration " }, 
{ "title" : "Install and Configure MySQL for Unravel", 
"url" : "install/install-mysql/install-mysql-details.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ MySQL \/ Install and Configure MySQL for Unravel", 
"snippet" : "Pre-install Steps Install MySQL Server 5.7 Configure and Start MySQL Server Install MySQL JDBC Driver Post-Install Steps Copy MySQL JDBC JAR Configure Unravel to Connect MySQL Server Pre-install Steps Do the following steps, before installing Unravel RPM. Install MySQL Server 5.7 Install MySQL datab...", 
"body" : " Pre-install Steps Install MySQL Server 5.7 Configure and Start MySQL Server Install MySQL JDBC Driver Post-Install Steps Copy MySQL JDBC JAR Configure Unravel to Connect MySQL Server Pre-install Steps Do the following steps, before installing Unravel RPM. Install MySQL Server 5.7 Install MySQL database. CentOS 6 # wget https:\/\/dev.mysql.com\/get\/mysql80-community-release-el6-1.noarch.rpm\n# sudo yum install yum-utils\n# sudo rpm -ivh mysql80-community-release-el6-1.noarch.rpm\n# sudo yum-config-manager --disable mysql80-community\n# sudo yum-config-manager --enable mysql57-community\n# sudo yum install mysql-community-server CentOS 7 # wget https:\/\/dev.mysql.com\/get\/mysql80-community-release-el7-1.noarch.rpm\n# sudo rpm -ivh mysql80-community-release-el7-1.noarch.rpm\n# sudo yum-config-manager --disable mysql80-community\n# sudo yum-config-manager --enable mysql57-community\n# sudo yum install mysql-community-server If you are installing MySQL on a Unravel SELinux host and are not using the default datadir here Configure and Start MySQL Server Stop MySQL server if it is running. # sudo service mysqld stop Backup old InnoDB log files to a directory of your choosing, { Backup_Path \/var\/lib\/mysql\/ib_logfile # mv \/var\/lib\/mysql\/ib_logfile* {Backup_Path}\n\n\/\/ OR\n\n# rm -rf \/var\/lib\/mysql\/ib_logfile* Append the following properties at the end of [mysqld] \/etc\/my.cnf datadir \/srv\/unravel\/db_data key_buffer_size = 256M max_allowed_packet = 32M sort_buffer_size = 32M query_cache_size = 64M max_connections = 500 max_connect_errors = 2000000000 open_files_limit = 10000 port-open-timeout = 121 expire-logs-days = 1 character_set_server = utf8 collation_server = utf8_unicode_ci innodb_open_files = 2000 innodb_file_per_table = 1 innodb_data_file_path = ibdata1:100M:autoextend # The innodb_buffer_pool_size depends on load and cluster size. # On a dedicated machine, it can be 50% of the RAM size. # Using 1G is the absolute minimum. For a large cluster, we use 48G. innodb_buffer_pool_size = 4G innodb_flush_method = O_DIRECT innodb_log_file_size = 256M innodb_log_buffer_size = 64M innodb_flush_log_at_trx_commit = 2 innodb_lock_wait_timeout = 50 innodb_thread_concurrency = 20 innodb_read_io_threads = 16 innodb_write_io_threads = 4 binlog_format = mixed # if SSD disk is used uncomment the line below #innodb_io_capacity = 4000 Ensure MySQL server starts at boot. CentOS 6 # sudo chkconfig mysqld on CentOS 7 # sudo systemctl enable mysqld Start MySQL server. CentOS 6 # sudo service mysqld start CentOS 7 # sudo systemctl start mysqld Check disk space used by MySQL's datadir from the MySql configuration file (eg., ( \/etc\/my.cnf # sudo grep \"^datadir=\" \/etc\/my.cnf | awk -F\"[=]\" '{print $2}' | sudo xargs du -sh Check available file system disk space for MySQL's datadir from the MySql configuration file (eg., \/etc\/my.cnf) # sudo grep \"^datadir=\" \/etc\/my.cnf | awk -F\"[=]\" '{print $2}' | sudo xargs df -h Install MySQL JDBC Driver Download MySQL JDBC driver to \/tmp # wget https:\/\/dev.mysql.com\/get\/Downloads\/Connector-J\/mysql-connector-java-5.1.47.tar.gz -O \/tmp\/mysql-connector-java-5.1.47.tar.gz cd \/tmp # cd \/tmp\n# tar xvzf \/tmp\/mysql-connector-java-5.1.47.tar.gz Post-Install Steps After installing the Unravel RPM, complete the following steps. Copy MySQL JDBC JAR Copy the MySQL JDBC JAR to \/usr\/local\/unravel\/share\/java\/ # mkdir -p \/usr\/local\/unravel\/share\/java\n# sudo cp \/tmp\/mysql-connector-java-5.1.47\/mysql-connector-java-5.1.47.jar \/usr\/local\/unravel\/share\/java\n# sudo cp \/tmp\/mysql-connector-java-5.1.47\/mysql-connector-java-5.1.47.jar \/usr\/local\/unravel\/dlib\/unravel Configure Unravel to Connect MySQL Server Run mysql and create a Database and user for Unravel. # mysql\nmysql> CREATE DATABASE unravel_mysql_prod;\nmysql> CREATE USER 'unravel'@'localhost' IDENTIFIED BY '<password>';\nmysql> GRANT ALL PRIVILEGES ON unravel_mysql_prod.* TO 'unravel'@'localhost'; Update the following properties in \/usr\/local\/unravel\/etc\/unravel.properties unravel.jdbc.username=unravel unravel.jdbc.password={password} # If MySQL JDBC driver is installed replace jdbc:mariadb with jdbc:mysql unravel.jdbc.url=jdbc: mariadb:\/\/127.0.0.1:3306\/unravel_mysql_prod Create schema for Unravel tables. # sudo \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh Create default admin for Unravel UI. # \/usr\/local\/unravel\/install_bin\/db_initial_inserts.sh | \/usr\/local\/unravel\/install_bin\/db_access.sh Once you have successfully configured Unravel to use the MySQL database, ensure that the unravel_pg service unravel_pg stop; chkconfig unravel_pg off " }, 
{ "title" : "Move MySQL to a Custom Location", 
"url" : "install/install-mysql/install-mysql-move.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ MySQL \/ Move MySQL to a Custom Location", 
"snippet" : "Depending on your deployment, follow the steps in one of these sections below If you're using Unravel bundled MySQL see Move a Bundled MySQL If you're using an external MySQL see Move an External MySQL Whichever move you perform, you must do a slow shutdown ( If you need to move MySQL to another hos...", 
"body" : "Depending on your deployment, follow the steps in one of these sections below If you're using Unravel bundled MySQL see Move a Bundled MySQL If you're using an external MySQL see Move an External MySQL Whichever move you perform, you must do a slow shutdown ( If you need to move MySQL to another host please see here Move a Bundled MySQL Daemon user must Perform a slow shutdown ( Get DB root password from \/root\/unravel.install.include \/root\/unravel.install.include.prev The example below uses *.include # grep 'DB_ROOT_PASSWORD' { \/root\/unravel.install.include | \/root\/unravel.install.include.prev } Run the following commands to set MySQL clean shutdown. # \/usr\/local\/unravel\/mysql\/bin\/mysql -uroot --port=3316 --host=127.0.0.1 -p mysql> SET GLOBAL innodb_fast_shutdown=0; Stop unravel_db # \/etc\/init.d\/unravel_db stop Back up MySQL database folder \/srv\/unravel\/db_data # cd \/srv\/unravel # tar cvf unravel_db_data.tar db_data\/ Restore MySQL datadir to the custom path. Replace NEW_DB_LOCATION # cp unravel_db_data.tar NEW_DB_LOCATION NEW_DB_LOCATION Update unravel.install.include .cnf In \/root\/unravel.install.include NEW_DB_LOCATION In \/usr\/local\/unravel\/mysql\/unravel_mysql.cnf innodb_data_home_dir innodb_log_group_home_dir NEW_DB_LOCATION Move an External MySQL MySQL user must Perform a slow shutdown ( Run the following commands to set MySQL clean shutdown. # mysql -uroot -p mysql> SET GLOBAL innodb_fast_shutdown=0; Stop MySQL daemon # service mysqld stop Backup MySQL database folder \/var\/lib\/mysql # cd \/var\/lib # tar cvf unravel_db_data.tar mysql\/ Restore MySQL datadir to the custom path. Replace NEW_DB_LOCATION # cp unravel_db_data.tar NEW_DB_LOCATION Update MySQL cnf file In \/etc\/my.cnf NEW_DB_LOCATION " }, 
{ "title" : "MySql Partitioning and Data Migration", 
"url" : "install/install-mysql/install-mysql-partition.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ MySQL \/ MySql Partitioning and Data Migration", 
"snippet" : "MySql is not bundled with Unravel and you must manually migrate Unravel's tables for its use. The time for the data migration varies by machine and MySql configuration. Unravel uses the following partitioned tables: BLACKBOARDS EVENT_INSTANCES HIVE_QUERIES IMPALA_QUERIES JOBS OOZIE_WORKFLOW_JOBS Upg...", 
"body" : "MySql is not bundled with Unravel and you must manually migrate Unravel's tables for its use. The time for the data migration varies by machine and MySql configuration. Unravel uses the following partitioned tables: BLACKBOARDS EVENT_INSTANCES HIVE_QUERIES IMPALA_QUERIES JOBS OOZIE_WORKFLOW_JOBS Upgrade Unravel Server. Stop all daemons. # \/etc\/init.d\/unravel_all.sh stop Make sure MySQL is running. Perform the data migration. For external MySQL servers you might need to change the chunk size to 100 or 1000. # \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh migration:migration_partitioning During migration process the status of the tables is written to stdout. The output is continually updated until the migration for the table is complete. Start migration: migration_partitioning\nStart table rows calculation...\nTable blackboards_old has 0 rows total\nTable impala_queries_old has 0 rows total\nTable jobs_old has 205437 rows total\nTable event_instances_old has 0 rows total\nTable oozie_workflow_jobs_old has 0 rows total\nTable hive_queries_old has 0 rows total\nTable rows calculation finished.\nMigrating records for table: blackboards, chunk size: 10000, chunk count: 10...\nMigrated records for table: blackboards, rows total: 0, chunk size: 10000, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: impala_queries, chunk size: 1500, chunk count: 10...\nMigrated records for table: impala_queries, rows total: 0, chunk size: 1500, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: jobs, chunk size: 500, chunk count: 10...\nMigrated records for table: jobs, rows total: 5000, chunk size: 500, chunk count: 10, finished: false, time: 32 seconds, progress 2,43 %\nMigrating records for table: event_instances, chunk size: 50000, chunk count: 10...\nMigrated records for table: event_instances, rows total: 0, chunk size: 50000, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: oozie_workflow_jobs, chunk size: 1500, chunk count: 10...\nMigrated records for table: oozie_workflow_jobs, rows total: 0, chunk size: 1500, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: hive_queries, chunk size: 1500, chunk count: 10...\nMigrated records for table: hive_queries, rows total: 0, chunk size: 1500, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: jobs, chunk size: 500, chunk count: 10...\nMigrated records for table: jobs, rows total: 10000, chunk size: 500, chunk count: 10, finished: false, time: 33 seconds, progress 4,87 %\nMigrating records for table: jobs, chunk size: 500, chunk count: 10... When all the tables have been successfully migrated you see: Migration: migration_partitioning is finished During the process a temporary migration_partitioning keeps track the tables' status. You can view the table using The chunks field increases until the migration for the table is complete. Upon completion the table's finished column is set to 1. # \/usr\/local\/unravel\/install_bin\/db_access.sh\n\nmysql> select * from migration_partitioning;\n+---------------------+--------------------+--------+----------+\n| migr_table_name | lowest_id_migrated | chunks | finished |\n+---------------------+--------------------+--------+----------+\n| blackboards | 11018258 | 119 | 0 |\n| event_instances | 0 | 1 | 1 |\n| hive_queries | 0 | 1 | 1 |\n| impala_queries | 0 | 1 | 1 |\n| jobs | 240884 | 100 | 0 |\n| oozie_workflow_jobs | 0 | 1 | 1 |\n+---------------------+--------------------+--------+----------+\n\n To see the oldest migrated records per each table you can use the following MySql query. # \/usr\/local\/unravel\/install_bin\/db_access.sh\n\nmysql>\nselect \"blackboards\" as \"table_name\", min(created_at) as \"oldest_migrated_record\" from blackboards\nunion select \"event_instances\", min(created_at) from event_instances\nunion select \"hive_queries\", min(created_at) from hive_queries\nunion select \"impala_queries\", min(created_at) from impala_queries\nunion select \"jobs\", min(created_at) from jobs\nunion select \"oozie_workflow_jobs\", min(created_at) from oozie_workflow_jobs;\n+---------------------+------------------------+\n| table_name | oldest_migrated_record |\n+---------------------+------------------------+\n| blackboards | 2018-08-30 00:57:45 |\n| event_instances | 2018-08-30 00:57:55 |\n| hive_queries | 2018-08-30 00:59:10 |\n| impala_queries | NULL |\n| jobs | 2018-08-30 00:57:50 |\n| oozie_workflow_jobs | 2018-09-07 10:25:22 |\n+---------------------+------------------------+ If the migration process is interrupted or killed, you can run shell script again. However, if the process has failed you must truncate and reset ID's During migration the original files were renamed to _old TableName migration_partitioning # \/usr\/local\/unravel\/install_bin\/db_access.sh\n\nDROP TABLE blackboards_old;\nDROP TABLE event_instances_old;\nDROP TABLE hive_queries_old;\nDROP TABLE impala_queries_old;\nDROP TABLE jobs_old;\nDROP TABLE oozie_workflow_jobs_old;\nDROP TABLE migration_partitioning;\nquit Restart daemons. # \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Restarting a Failed Migration", 
"url" : "install/install-mysql/install-mysql-partition.html#UUID-aa105656-ef76-f548-7147-bcde7f4abe0e_RestartHowtorestartafailedmigration", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ MySQL \/ MySql Partitioning and Data Migration \/ Restarting a Failed Migration", 
"snippet" : "Truncate partitioned tables, reset autoincrement ID values and truncate migration_partitioning # \/etc\/init.d\/unravel_all.sh stop # \/usr\/local\/unravel\/install_bin\/db_access.sh truncate blackboards; truncate event_instances; truncate hive_queries; truncate impala_queries; truncate jobs; truncate oozie...", 
"body" : " Truncate partitioned tables, reset autoincrement ID values and truncate migration_partitioning # \/etc\/init.d\/unravel_all.sh stop\n\n# \/usr\/local\/unravel\/install_bin\/db_access.sh\n\ntruncate blackboards;\ntruncate event_instances;\ntruncate hive_queries;\ntruncate impala_queries;\ntruncate jobs;\ntruncate oozie_workflow_jobs;\n\ntruncate migration_partitioning;\n\ncall resetAutoIncrementId('blackboards_old', 'blackboards');\ncall resetAutoIncrementId('event_instances_old', 'event_instances');\ncall resetAutoIncrementId('hive_queries_old', 'hive_queries');\ncall resetAutoIncrementId('impala_queries_old', 'impala_queries');\ncall resetAutoIncrementId('jobs_old', 'jobs');\ncall resetAutoIncrementId('oozie_workflow_jobs_old', 'oozie_workflow_jobs'); Return to Step 3 and complete the remaining steps. " }, 
{ "title" : "OnDemand", 
"url" : "install/install-ondemand.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ OnDemand", 
"snippet" : "If you want Unravel to generate any of these reports, you must install the OnDemand service on the Unravel Server host machine: Operational Insights | Queue Analysis Operational Insights | Cluster Optimization Cloud Reports Data Insights | Forecasting Data Insights | Small Files Data Insights | File...", 
"body" : "If you want Unravel to generate any of these reports, you must install the OnDemand service on the Unravel Server host machine: Operational Insights | Queue Analysis Operational Insights | Cluster Optimization Cloud Reports Data Insights | Forecasting Data Insights | Small Files Data Insights | File Reports Data Insights | Top X Installation or Upgrade of OnDemand Library Versions and Licensing for OnDemand " }, 
{ "title" : "Installation or Upgrade of OnDemand", 
"url" : "install/install-ondemand/install-ondemand-details.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ OnDemand \/ Installation or Upgrade of OnDemand", 
"snippet" : "The OnDemand service works with both MySQL and Postgres. If you want to use your own MySQL, start your installation at Install External MySQL on the Unravel Host Install the OnDemand Service on the Unravel Host OnDemand support is currently a Beta feature. Please contact Unravel Support before insta...", 
"body" : "The OnDemand service works with both MySQL and Postgres. If you want to use your own MySQL, start your installation at Install External MySQL on the Unravel Host Install the OnDemand Service on the Unravel Host OnDemand support is currently a Beta feature. Please contact Unravel Support before installing OnDemand. (Optional) 1. Install External MySQL on the Unravel Host You can skip these steps if you plan to use Postgres with OnDemand. Install MySQL. See MySQL Verify MySql is installed correctly by running netstats # netstat -tunlp | grep :3306\ntcp6 0 0 :::3306 :::* LISTEN 28006\/mysqld For fresh installations report_instances # cat \/usr\/local\/unravel\/sql\/mysql\/20180517031500.sql | sudo \/usr\/local\/unravel\/install_bin\/db_access.sh Run db_access.sh ondemand_tasks ondemand_sessions report_instances # sudo \/usr\/local\/unravel\/install_bin\/db_access.sh\nmysql> show tables; Install the OnDemand Service on the Unravel Host Download the OnDemand package from https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/OnDemand\/ \/tmp Contact Unravel Support for {USERNAME} {PASSWORD} For example, if your host operating system is Red Hat Enterprise Linux 7 (RHEL 7): # cd \/tmp\n# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/OnDemand\/Ondemand-4.5.0.0-GA-rhel7.tar.gz \\\n-o Ondemand-4.5.0.0-GA-rhel7.tar.gz -u {USERNAME}:{PASSWORD} For example, if your host operating system is Red Hat Enterprise Linux 6 (RHEL 6): # cd \/tmp\n# curl -v https:\/\/preview.unraveldata.com\/unravel\/RPM\/4.5.0\/OnDemand\/Ondemand-4.5.0.0-GA-rhel6.tar.gz \\\n-o Ondemand-4.5.0.0-GA-rhel6.tar.gz -u {USERNAME}:{PASSWORD} Navigate to the the \/tmp ondemand # cd \/tmp\n# sudo rm -rf \/usr\/local\/unravel\/ondemand (Optional) If you want Unravel to generate Small Files reports and you've customized hive_properties.hive hive_properties.hive ondemand # cp \/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/scripts\/hive_queries_reporting\/hive_properties.hive \\\nondemand\/unravel-python-1.0.0\/scripts\/hive_queries_reporting\/hive_properties.hive Extract the contents of the tarball. # tar xvf Ondemand-4.5.0.0-GA-rhel<6|7>.tar.gz Run the installation script. # sudo mv ondemand\/ \/usr\/local\/unravel\/\n# cd \/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\n# chmod +rwx install\/ondemand_quick_install.sh\n# sudo .\/install\/ondemand_quick_install.sh\n# sudo \/etc\/init.d\/unravel_all.sh restart In unravel.properties For details, see the General OnDemand Configurations for Reports Restart Unravel Server. # sudo \/etc\/init.d\/unravel_all.sh restart If your host operating system is SELinux, you might get alerts like these after restarting Unravel Server, depending on your environment. You can ignore them: # sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. Howver, if you encounter problems, contact Unravel Support. Start the OnDemand Daemon Execute the following four commands in the order shown, replacing { run_as_user run_as_group switch_to_user For RHEL 7.x: # sudo systemctl stop unravel_ondemand.service\n# sudo chown -RL {run_as_user}:{run_as_group} \/usr\/local\/unravel\/ondemand\/\n# sudo service unravel_all.sh restart\n# sudo systemctl start unravel_ondemand.service For RHEL 6.x: # sudo service unravel_ondemand stop\n# sudo chown -RL {run_as_user}:{run_as_group} \/usr\/local\/unravel\/ondemand\/\n# sudo service unravel_all.sh restart\n# sudo service unravel_ondemand start Confirm that the OnDemand service is running: You should see output similar to the example below. Process IDs will vary dynamically based on the number of processors in Unravel Server, number of current tasks, and so on. For RHEL 7.x: # sudo systemctl status unravel_ondemand.service For RHEL 6.x: # sudo service unravel_ondemand status You can also use ps # ps -ef | grep ondemand | grep -v grep root 11159 1 0 Sep21 ? 00:00:00 su - -c bash -c cd \/usr\/local\/unravel\/ondemand\/; nohup unravel-python-*\/bin\/unravel_on_demand_start.sh root 11163 11159 0 Sep21 ? 00:00:00 -bash -c cd \/usr\/local\/unravel\/ondemand\/; nohup unravel-python-*\/bin\/unravel_on_demand_start.sh root 11450 11176 0 Sep21 ? 00:00:42 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/flask run hdfs 11452 11176 0 Sep21 ? 00:27:19 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3 hdfs 11670 11452 0 Sep21 ? 00:00:00 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3 hdfs 11671 11452 0 Sep21 ? 00:00:00 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3 hdfs 11672 11452 0 Sep21 ? 00:00:00 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3 Enable Various OnDemand-based Reports or Features Cluster Optimization Reports - See Enabling or Disabling Cluster Optimization Reports. Cloud Reports Enabling or Disabling Cloud Reports and Forecasting Reports Queue Analysis - is enabled by default. See Enabling or Disabling Queue Analysis Reports Small Files Reports and File Reports - are enabled by default. See Enabling or Disabling Small Files Reports and Files Reports Top X Report- is always enabled. There are no Top X specific properties. Sessions Enabling or Disabling Sessions " }, 
{ "title" : "Library Versions and Licensing for OnDemand", 
"url" : "install/install-ondemand/install-ondemand-libraries-licensing.html", 
"breadcrumbs" : "Unravel 4.5 \/ Installation Guides \/ OnDemand \/ Library Versions and Licensing for OnDemand", 
"snippet" : "Library Licensing MySQL-python>=1.2.5 GPL Celery>=4.1.0 BSD pyhive>=0.5.1 Apache 2.0 sasl>=0.2.1 Apache License, Version 2.0 Thrift>=0.11.0 Apache License, Version 2.0 pandas>=0.20.3 BSD Flask>=0.12.2 BSD cm_api>=16.0.0 Apache 2.0 six>=1.10.0 MIT matplotlib>=2.1.0 BSD pystan>=2.16 GPLv3 (dependency ...", 
"body" : " Library Licensing MySQL-python>=1.2.5 GPL Celery>=4.1.0 BSD pyhive>=0.5.1 Apache 2.0 sasl>=0.2.1 Apache License, Version 2.0 Thrift>=0.11.0 Apache License, Version 2.0 pandas>=0.20.3 BSD Flask>=0.12.2 BSD cm_api>=16.0.0 Apache 2.0 six>=1.10.0 MIT matplotlib>=2.1.0 BSD pystan>=2.16 GPLv3 (dependency of FBprophet) fbprophet>=0.1.1 BSD scipy>=0.19.1 BSD seasonal>=0.3.1 MIT statsmodels>=0.8.0 BSD gatspy>=0.3 BSD 3-Clause numpy>=1.13.1 BSD PyAstronomy>=0.12.0 MIT python_dateutil>=2.6.1 Simplified BSD fastdtw>=0.3.2 MIT requests==2.20.1 Apache 2.0 seaborn>=0.8.1 BSD 3-Clause sqlalchemy==1.2.7 MIT License pymysql>=0.8.0 MIT psycopg2==2.7.6.1 LGPL, Version 3.0 cython 0.27.3 Apache License, Version 2.0 kombu 4.1.0 BSD 3-Clause \"New\" Thrift-sasl 0.3.0 Apache License, Version 2.0 " }, 
{ "title" : "Post Installation Steps", 
"url" : "post-installation-steps.html", 
"breadcrumbs" : "Unravel 4.5 \/ Post Installation Steps", 
"snippet" : "Before using Unravel, configure the following properties in \/usr\/local\/unravel\/etc\/unravel.properties Login mode If mode=ldap see enable LDAP authentication ldap properties If mode=saml see enable SAML authentication saml properties Configure the email properties Set com.unraveldata.customer.organiz...", 
"body" : "Before using Unravel, configure the following properties in \/usr\/local\/unravel\/etc\/unravel.properties Login mode If mode=ldap see enable LDAP authentication ldap properties If mode=saml see enable SAML authentication saml properties Configure the email properties Set com.unraveldata.customer.organization Configure Hive Metastore Permissions Set up HBASE Configuration Configure Kafka Monitoring Configure Oozie Configure the following Tez yarn.ats.webapp.username yarn.ats.webapp.password yarn.timeline-service.webapp.address yarn.timeline-service.port " }, 
{ "title" : "Other Configuration Options", 
"url" : "post-installation-steps.html#UUID-6d82f0d9-86d3-5fa2-de6c-9064960c0b6d_id_PostInstallationSteps-OtherConfigurationOptions", 
"breadcrumbs" : "Unravel 4.5 \/ Post Installation Steps \/ Other Configuration Options", 
"snippet" : "Adding More Admins to Unravel Web UI. Add read-only admins. See here See here Creating Multiple Workers for High Volume Data...", 
"body" : " Adding More Admins to Unravel Web UI. Add read-only admins. See here See here Creating Multiple Workers for High Volume Data " }, 
{ "title" : "Further Configuration Options", 
"url" : "post-installation-steps.html#UUID-6d82f0d9-86d3-5fa2-de6c-9064960c0b6d_id_PostInstallationSteps-FurtherConfigurationOptions", 
"breadcrumbs" : "Unravel 4.5 \/ Post Installation Steps \/ Further Configuration Options", 
"snippet" : "Custom Configurations Security Configurations...", 
"body" : " Custom Configurations Security Configurations " }, 
{ "title" : "User Guide", 
"url" : "uguide.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide", 
"snippet" : "Unravel Web UI helps you to analyze, optimize, and troubleshoot big data applications and operations. Getting Started Common UI Features The Operations Page The Applications Page The Reports Page The Application Managers Events & Insights Auto Actions Use Cases Detecting Resource Contention in the C...", 
"body" : "Unravel Web UI helps you to analyze, optimize, and troubleshoot big data applications and operations. Getting Started Common UI Features The Operations Page The Applications Page The Reports Page The Application Managers Events & Insights Auto Actions Use Cases Detecting Resource Contention in the Cluster Identifying Rogue Applications Optimizing the Performance of Spark Applications Kafka Insights " }, 
{ "title" : "Getting Started", 
"url" : "uguide/uguide-getting-started.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Getting Started", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Use Case Videos", 
"url" : "uguide/uguide-getting-started.html#UUID-b3fb06e1-2e21-f098-7dd2-071d84f1cae6_id_GettingStarted-UseCaseVideos", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Getting Started \/ Use Case Videos", 
"snippet" : "The Use Case videos below use Unravel 4.2 Case #1: How to Search for Applications and Optimize\/Tune a Hive Application Case #2: How to \"Root Cause\" Issues with a Workflow that Missed Its SLA Case #3: How to Debug Failed Applications Case #4: How to Review Spark Applications and Identify Areas for Pe...", 
"body" : " The Use Case videos below use Unravel 4.2 Case #1: How to Search for Applications and Optimize\/Tune a Hive Application Case #2: How to \"Root Cause\" Issues with a Workflow that Missed Its SLA Case #3: How to Debug Failed Applications Case #4: How to Review Spark Applications and Identify Areas for Performance Improvements " }, 
{ "title" : "Common UI Features", 
"url" : "uguide/uguide-common-ui-features.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Common UI Features", 
"snippet" : "Every page has the Unravel Title Bar. No matter what your permissions the pages available to you are listed on the left with the one you are viewing underlined and noted below in the black bar. To the right there is a search box, Docs Supported Roles Role Based Access Control If you are end-user res...", 
"body" : " Every page has the Unravel Title Bar. No matter what your permissions the pages available to you are listed on the left with the one you are viewing underlined and noted below in the black bar. To the right there is a search box, Docs Supported Roles Role Based Access Control If you are end-user restricted by Role Based Access Control Applications About Logout If you are unrestricted end-user or an admin, you have all the pages available with possible read\/write restrictions. The pull-down menu has Manage About Logout If your admin has disabled Support If you can configure the date range time period cluster(s) When there are multiple tabs, click on the tab to display its contents. When detailed or further information is available open section ( To expand section to the width of the entire tile click on the double arrows displayed ( Clicking on the application name\/id\/workflow usually bring ups information on the appplication, fragment etc, i.e., the Spark Application Manager, table information, etc. Hovering over an auto action alert ( Lists\/Tables Can be sorted by a column, i.e., start time, in ascending or descending order. The sort column highlights the arrow indicating the sort order ( Clicking on a column being used reverses the sort order. If you can chose which columns to display a plus ( When applicable, the application status is color coded: Clicking on the app name\/id\/workflow usually bring ups the information on the app, i.e., the Spark Application Manager, table information, etc. When applicable, the Notifications When relevant there is an Auto Actions\/Events column ( When an application has a parent a link to it will appear in the GoTo A block glyph ( Graphs (see Operations | Usage Details | Infrastructure Hovering over a line in a graph causes the information to be displayed in a text box ( When \" Applications running at mm\/dd\/yy hh:mm:ss\" Clicking Show More ( If graph can be displayed based upon Group By Tags Metric Click If you can zoom in\/out of a diagram\/execution graph the magnifiers ( " }, 
{ "title" : "The Operations Page", 
"url" : "uguide/uguide-operations-page.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Operations Page", 
"snippet" : "The Operations Page provides synopsis of your cluster(s) and its activities. It has two tabs: and Dashboard Usage Detail Operations Dashboard Note Click here...", 
"body" : "The Operations Page provides synopsis of your cluster(s) and its activities. It has two tabs: and Dashboard Usage Detail Operations Dashboard Note Click here " }, 
{ "title" : "Dashboard", 
"url" : "uguide/uguide-operations-page.html#UUID-784a48e0-ac6d-e0c8-3358-ba18449faa11_id_TheOperationsPage-Dashboard", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Operations Page \/ Dashboard", 
"snippet" : "To view the Dashboard, click Operations Dashboard The Dashboard provides an overview of cluster activities with links to drill down into YARN applications, resource usage, application inefficiencies, and events\/alerts. By default it is configured to display all hourly 24 hours Finished YARN applicat...", 
"body" : "To view the Dashboard, click Operations Dashboard The Dashboard provides an overview of cluster activities with links to drill down into YARN applications, resource usage, application inefficiencies, and events\/alerts. By default it is configured to display all hourly 24 hours Finished YARN applications Tile The line graphs display the successful, failed, and killed jobs for the time period time incremen cluster Clicking on Open Section Applications Applications Finding Applications Running YARN Application Tile The line graphs display the running and pending jobs for the current time. It textually displays the total number at the current time period. Clicking on Open Section Operations Usage Details Jobs here Inefficient Applications Tile Its three sub-tabs, HIVE MapReduce Spark; Event Name The inefficiencies application is equal to the Applications Applications Finding Applications Recent Events and Alerts Sidebar The sidebar lists all events and alerts that have occurred organized by date and time. A separate entry appears for each time a particular Auto Action was triggered. In the image below, the same auto action triggered at 23:56 and 2:58. Clicking an event\/alert brings up a Cluster Resource view ( >Operations Usage Detail Infrastructure Add a New Auto Action Clear " }, 
{ "title" : "Resources Tile", 
"url" : "uguide/uguide-operations-page.html#UUID-784a48e0-ac6d-e0c8-3358-ba18449faa11_id_TheOperationsPage-ResourcesTile", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Operations Page \/ Dashboard \/ Resources Tile", 
"snippet" : "Displays the available and allocated VCore and Memory for the entire cluster. Clicking on Open Section Operations Usage Details Infrastructure below...", 
"body" : "Displays the available and allocated VCore and Memory for the entire cluster. Clicking on Open Section Operations Usage Details Infrastructure below " }, 
{ "title" : "Usage Details", 
"url" : "uguide/uguide-operations-page.html#UUID-784a48e0-ac6d-e0c8-3358-ba18449faa11_id_TheOperationsPage-ChartsUsageDetails", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Operations Page \/ Usage Details", 
"snippet" : "Usage Detail has four tabs: Infrastructure Jobs Nodes Impala Usage Kafka The Operations Page#HBase One of the most difficult challenges in managing multi-tenant Hadoop clusters is understanding how resources are being used by the applications running in the clusters. Through Usage Details For exampl...", 
"body" : " Usage Detail has four tabs: Infrastructure Jobs Nodes Impala Usage Kafka The Operations Page#HBase One of the most difficult challenges in managing multi-tenant Hadoop clusters is understanding how resources are being used by the applications running in the clusters. Through Usage Details For example, Unravel can pinpoint the applications causing a sudden a spike in the total VCores or memory MB usage. This allows you to easily you drill down into these applications to understand their behavior. Whenever possible, Unravel provides recommendations and insights All the charts and tables are automatically refreshed; however refreshing is disabled when you interact within a page to alter its display, e.g., change the date range, click on a point within in a graph. When disabled a Refresh Refresh By default the Usage Details tab opens showing the Infrastructure tab. For all charts, click on the menu bars ( Show more Reset Graph Infrastructure Infrastructure This tab contains four (4) graphs. The upper two list available and allocated Vcores and memory for the entire Cluster, and The bottom show the Vcores and memory used by specific view, i.e., Application Type User Queue Clicking within a chart (1) displays the applications running for that point in time. You can chose how to display the bottom two graphs by clicking on the View By Showing View Showing x Infrastructure Application Type show more To View by use the Business Tags Showing Jobs Graphs the running and accepted jobs as applicable. You can Group by Impala Usage Graphs memory MB consumption and Query Number. The # Queries Tags Group By Kafka Lists all the configured Kafka clusters. See Kafka Application Manager for more information. See Kafka Use Case for information on drilling down into a Cluster to locate lagging and stalled Topics\/Partitions. Clicking the cluster name brings detailed information about the Kafka Cluster HBase Please see HBase Configuration Clusters View Clusters page lists all the available HBase clusters Click on a cluster name to bring up the cluster's information the HBase Cluster view. Cluster View This view is divided into four (4) sections. When an component's health is noted, hovering over it's health glyph brings up details, Cluster Information A bar at shows what cluster you are displaying with a pull-down which allows you to switch between clusters. Listed immediately below are the cluster metrics. You can choose to tab between clusters by choosing all cluster Region Servers Lists the the cluster regional services, with their KPI's and health. You can search on the region server by name. Click on the server's name to bring up its details. Region Servers KPI Graphs the regional server metrics, the graphs are linked with the table list below them. Click within a graph to see up the tables associated servers that point in time. Hover over a point to bring up a popup displaying the information for that point in time. Click Show More Tables List all the tables associated with the cluster, their KPIs and the table's health. Click on the table name to bring up its information. You can search for a table by name; any table with a name matching or containing the string is displayed. Region Server View Server, Operational, and OS Metrics are displayed. Hover over the metric for its description. For more information on the metrics see here Table View Table has two tabs, Table Region Table Table Regions Lists all the regions with their KPIs and health. " }, 
{ "title" : "Nodes", 
"url" : "uguide/uguide-operations-page.html#UUID-784a48e0-ac6d-e0c8-3358-ba18449faa11_id_OperationPage--Nodes", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Operations Page \/ Usage Details \/ Nodes", 
"snippet" : "This chart graphs the Total = Total Active Unhealthy Where: currently running and healthy nodes, and Active: currently running and unhealthy nodes. Unhealthy: You can toggle the display of an item by clicking on its name....", 
"body" : "This chart graphs the Total = Total Active Unhealthy Where: currently running and healthy nodes, and Active: currently running and unhealthy nodes. Unhealthy: You can toggle the display of an item by clicking on its name. " }, 
{ "title" : "The Applications Page", 
"url" : "uguide/uguide-applications-page.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Applications Page", 
"snippet" : "The Applications Ad-hoc applications are Hive queries, MapReduce jobs, Spark applications, Impala queries, and so on; these are generated by end user tools (such as BI tools like Tableau, Microstrategy, etc.) or submitted via CLI. Repeatedly running workflows or data pipelines are created using cron...", 
"body" : "The Applications Ad-hoc applications are Hive queries, MapReduce jobs, Spark applications, Impala queries, and so on; these are generated by end user tools (such as BI tools like Tableau, Microstrategy, etc.) or submitted via CLI. Repeatedly running workflows or data pipelines are created using cron Unravel currently supports the the following application frameworks: Cascading\/Pig Hive (on Map-Reduce) Hive (on Tez) Impala Kafka Map-Reduce Tez Spark Native Spark Streaming SparkSQL Athena (preview) Your application's performance and reliability depends on several factors such as quality of the code, types of joins used, configuration settings, data size, scheduler settings, contention with other applications, etc. It takes significant expertise and effort to get to the root cause(s) of an application's problems. Unravel's Intelligence Engine provides insights into your application's run to help resolve it's problems\/inefficiencies. These insights are called events Events & Insights The Applications Page has three tabs: Applications, Workflows, and Sessions Note Click here " }, 
{ "title" : "Applications Tab", 
"url" : "uguide/uguide-applications-page.html#UUID-78d669b5-9b31-e22b-d447-3ae9e41a832c_id_TheApplicationsPage-ApplicationsTab", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Applications Page \/ Applications Tab", 
"snippet" : "By default this tab lists all applications for the past week. However, this view initially search results are ordered by the most recent start time. To reorder the results by another property, click the appropriate header in the results table. Finding Applications You can search for your application...", 
"body" : "By default this tab lists all applications for the past week. However, this view initially search results are ordered by the most recent start time. To reorder the results by another property, click the appropriate header in the results table. Finding Applications You can search for your application(s) in a variety of ways: The left sidebar allows you to filter you App Name App type Status Tags Queue User Cluster Duration Number of Events. By time period, If the job is part of a Hive query, Pig script, or a Workflow, a link to it is noted in the job's Go To Athena jobs are serverless and retrieved upon completion. Therefore, its Write Cluster ID Queue Go To " }, 
{ "title" : "Workflow Tab", 
"url" : "uguide/uguide-applications-page.html#UUID-78d669b5-9b31-e22b-d447-3ae9e41a832c_N1552897837398", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Applications Page \/ Workflow Tab", 
"snippet" : "The layout of this window mirrors the Applications...", 
"body" : "The layout of this window mirrors the Applications " }, 
{ "title" : "Sessions Tab", 
"url" : "uguide/uguide-applications-page.html#UUID-78d669b5-9b31-e22b-d447-3ae9e41a832c_id_TheApplicationsPage-SessionsTab", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Applications Page \/ Sessions Tab", 
"snippet" : "This Report does not work with Postgres. You must be using MySQL and have the OnDemand Sessions allows you to run your application expressly to tune its performance for: efficiency: decrease the application's time (end-to end duration) and resources (shortening duration is first priority), or reliab...", 
"body" : " This Report does not work with Postgres. You must be using MySQL and have the OnDemand Sessions allows you to run your application expressly to tune its performance for: efficiency: decrease the application's time (end-to end duration) and resources (shortening duration is first priority), or reliability: in attempting to reduce resources Unravelprioritizes memory allocation to ensure the application doesn't fail due to \"out of memory\" exceptions. Why use sessions when Unravel already offers insights and recommendations on an application's run? You direct the tuning goal. You can provide multiple runs of an application providing a larger data pool for Unravel to analyze. You can have Unravel apply the recommendations for you and run the newly configured application. You can see the effects, both positive and negative, the tuning has on an applications run. You can compare runs configurations. You can repeatedly tune the application until Unravel has no more recommendations. Your session is saved and can be run again, e.g., new runs added, cluster configuration changed. You can tune: Spark Hive on MapReduce Sessions can serve simply as a tool to compare two runs of the same application. The Sessions tabs opens displaying all current sessions sorted on Sessions Name Start Time Number of Apps The four (4) KPI's Duration IO vCore Seconds Memory Seconds Duration Cluster ID You can search for a session by name. Enter the string in the search box; any session name matching or containing the string will be displayed. " }, 
{ "title" : "Creating a Session", 
"url" : "uguide/uguide-applications-page.html#UUID-78d669b5-9b31-e22b-d447-3ae9e41a832c_id_TheApplicationsPage-CreatingaSession", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Applications Page \/ Creating a Session", 
"snippet" : "You can uses sessions - where you actively control the analysis and application of recommendations, or manually - where sessions performs the iterations without you intervention until it reaches the maximum allowed runs or finds no more recommendations automatically Manual Session Click Create Sessi...", 
"body" : "You can uses sessions - where you actively control the analysis and application of recommendations, or manually - where sessions performs the iterations without you intervention until it reaches the maximum allowed runs or finds no more recommendations automatically Manual Session Click Create Session. Application Type Tuning Goal App IDs +Add another App ID Add If you are tuning a Spark App you must supply the JAR path and Class Name. If you do not intend to use the \"Apply\" feature for sessions, you can enter \"none\" (without quotes) for JAR path and Class Name. Auto Tune Session You have the additional option to specify the maximum number of runs. If not specified, iterates continues until no recommendations are available. When specified, the iteration stops at the maximum number or lack of recommendations, whichever comes first. Via the Events Panel If an applications has events " }, 
{ "title" : "Session", 
"url" : "uguide/uguide-applications-page.html#UUID-78d669b5-9b31-e22b-d447-3ae9e41a832c_id_TheApplicationsPage-Session", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Applications Page \/ Session", 
"snippet" : "The Sessions APM layout is similar to all APMs. Instead of KPIs reflecting the Application, Sessions KPI's are trends which graph the various runs resource usages measured when tuning, duration, IO, and resources. The example below is a session view immediately after creation. The left tab, Applicat...", 
"body" : "The Sessions APM layout is similar to all APMs. Instead of KPIs reflecting the Application, Sessions KPI's are trends which graph the various runs resource usages measured when tuning, duration, IO, and resources. The example below is a session view immediately after creation. The left tab, Applications Right Tabs - Keeps a log of all the activity. See example above. Progress Tab - Expanded graphs of Duration, IO, Resources Trends - Allows you to compare two of the runs. Compare " }, 
{ "title" : "The Reports Page", 
"url" : "uguide/ug-rep-the-reports-page.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Reports Page", 
"snippet" : "Unravel provides a variety of reports to help you manage your clusters. The page has four tabs. Operational Insights Data Insights Cloud Reports Report Archives Scheduled Reports Scheduling Reports The Reports page opens displaying Operational Insights...", 
"body" : " Unravel provides a variety of reports to help you manage your clusters. The page has four tabs. \n Operational Insights \n Data Insights \n Cloud Reports \n Report Archives \n Scheduled Reports \n Scheduling Reports The Reports page opens displaying \n Operational Insights " }, 
{ "title" : "Operational Insights", 
"url" : "uguide/ug-rep-the-reports-page/ug-rep-operational-insights.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Reports Page \/ Operational Insights", 
"snippet" : "- generates chargeback Yarn jobs. Chargeback Yarn - generates chargeback reports for Impala jobs. Chargeback Impala - generates summary reports for cluster usages. Cluster Summary - generates reports comparing cluster activity between two time periods. Cluster Compare - analyzes the cluster performa...", 
"body" : " - generates chargeback Yarn jobs. Chargeback Yarn - generates chargeback reports for Impala jobs. Chargeback Impala - generates summary reports for cluster usages. Cluster Summary - generates reports comparing cluster activity between two time periods. Cluster Compare - analyzes the cluster performance and provide fine tuning insights\/recommendations. Cluster Optimization - Generates a report of active queues for time frame. The report analyzes queue activity by apps, vcores and memory. Queue Analysis - shows the aggregated workload for all clusters. Cluster Workload When you specify a date range, a pull down menu appears on the right hand side of the Operational Insights title bar. By default Operational Insights Chargeback Application Type , Click here Chargeback The Chargeback Yarn and Impala tabs are identical expect that the reports are limited to Yarn and Impala jobs respectively. You can generate ChargeBack Group By Application Type, User, Queue Tags. Application Type Donut graphs showing the top results for the Group by Charge back report showing costs, sorted by the Group By choice(s), and List of Yarn applications running. Generate Charge Back Report You can set the date range and clusters to use for the report in the Operational Insights Group By Group By User dept User dept r VCore\/Hou Memory MB\/Hour t Update Repor CSV A new charge back report is generated each time you change the Group By must Update Report Charge Back Yarn Cluster Summary The Cluster Summary Applications User Queue User Applications You can sort applications on vCore or memory seconds. User Queue Cluster Compare This tab opens displays the cluster group by User Time Range Compare with Range Last 7 Days Use y Group B User Queue e Time Rang Compare With Range Any deviation in metrics across the time ranges is highlighted (3). A green red Time Compare With Group By Cluster Optimization The OnDemand This report analyzes your cluster workload over a specified period. It provides insights and configuration recommendations to optimize your cluster throughput, resources, and performance. Currently this feature only supports Hive on MapReduce. You can these reports you can: Fine tune your cluster to maximize its performance and minimize your costs, and Compare your cluster's performance between two (2) time periods. Report are generated on an ad hoc or scheduled basis. All reports are archived and can accessed via the Reports Archive Download or Generate a Report Click Download JSON Reports Archive Click Generate New Report Date Range Run Running Run Generate New Report Click Schedule schedule your report Optimization Report The Report has three (3) sections. Contains the basic report information author, time run, and dates used to generate the report. Header KPIs Number of Jobs: per day average Number of vCore Hours: per day average Number of MapReduce Containers Percent used for Map Percent used for Reduce Amount of Memory from of MapReduce Containers Percent from Map containers Percent from Reduce containers The KPIs are a per-day average for the number of days in the report. In this case we generated a report for a two (2) day period. All the insights\/recommendations are based upon the analysis of all jobs, in this case113. Insights\/Recommendations This section contains a tab for each area, with the relevant properties under consideration for tuning. These are cluster wide properties and are the defaults for all applications. Applications, however, can override these properties on an application by application basis. MapReduce: mapreduce.map.memory.mb,mapreduce.reduce.memory.mb,mapreduce.input.fileinputformat.split.maxsize,mapreduce.job.reduce.slowstart.completedmaps Hive: hive.exec.reducers.bytes.per.reducer,hive.exec.parallel You can expand the insight tile to the full width of the window. Further below we go into greater detail on two of the insights to explain the contents. Insight\/Recommendations Tile Details Tune the size of the map containers Each tile is entitled with what's being tuned. Below is the expanded view of the first tile . Immediately below the title is the property to tune, in this case mapreduce.map.memory.mb. Click on on the Next ( 1 2 As expected 51% of the jobs (58) used the default, while 33% (37) used 512MB with the remaining jobs distributed across the remaining values. The graph shows Unravel's analysis of the property potential values. It shows each candidate % of memory saved for the input workload % of jobs from the workload that would still run with the candidate When there are tuning instructions it is noted above the graph ( 3 In this example, there is a recommendation but no tuning suggestions. Since there is only a low chance of improvement, Unravel does not recommend altering the property so provides no tuning suggestions. Tune the number of the map containers This analysis has additional information in addition a tuning suggestion and instruction. Click on it to see further information. Tune the number of reduce containers in Hive queries In this case, the information was simply informative. There can be cases where tuning suggestions for specific apps are offered. Queue Analysis The OnDemand You can generate a report of active queues for all your clusters or a particular cluster. The report analyzes queue activity by applications, vcores, memory., and disk. As with all reports. it can be generated on an ad hoc or scheduled basis.The tab opens displaying the last report, if any, generated. Reports are archived and can accessed via the Reports Archive Generate a Report Click t New Repor , Date Range . Run Running Run New Report Click Schedule schedule your report Report If the report was successfully generated a light green bar appears and a table listing all the queues in existence during the time range is displayed. The table lists each queue with its KPIs average ( Apps Running VCores Memory Disk Filter By Click Applications VCore Usage Memory Usage Disk Usage | Operations Usage Details Infrastructure Click on the expand arrows ( Cluster Workload Displays your cluster(s) yarn applications' workload across a date range using the following views: - by date, e.g., 10 October. Month - by hour regardless of date, e.g., 10.00 - 11.00. Hour - by weekday regardless of date, e.g., Tuesday. Day - by hour for a given weekday, e.g.,10.00 -11.00 on Tuesday. Hour\/Day You can filter each view by App Count Vcores Hour Memory Hour To measure the Vcores or Memory Hour usage is straightforward; at any given point the memory or vcore is being used or it's not. The App Count is not a count of unique app instances The App Count reflects the apps that were running within that interval up to and including the boundary, i.e., date, hour, day. Therefore an app can be counted multiple times in a view. on multiple dates, e.g., on 11 & 12 October (2 days) in multiple hours, e.g., 10pm, 11pm & 12am hours on multiple days, Thursday & Friday, and in multiple hour\/day slots. This results in anomalies where the Sum(24 hours in Hour\/Day App Count) Sum(App Counts for dates representing the day) App Count for Wednesdays (10, 17 & 24 October) = 2492, and App Count across Hour\/Day We point this out not because it necessarily has a significant impact in how you can use the data, but to inform you such variations exist. By default the tab opens in the Month App Count Date Range t App Coun Vcores Hour Memory Hour y View B Hour Day Hour\/Day Average Sum See Drilling Down Month Displays the jobs run on the particular date. The color indicates how the day's load compares with the other days within the date range. The day with the least jobs\/hours is Previous Next Hour, Day and Hour\/Day These graphs do not link jobs to any specific date at the graph level. For instance, the Hour at Day on Hour\/Day at a date Month above By default each view opens using the metric selected for the prior view. For instance, if Hour Vcores Month Day Vcores Hour When the Date Range - aggregated sum of job count, vcore or memory hour during the time range (default view), and Sum - Sum \/ (# of Days in Date Range) Average Hour Breaks out information by hour. The interval label indicates the start, i.e., 2AM is 2-3AM. Hover over an interval for its details. Click on the interval to drill down Day Displays the jobs run on a specific weekday. Hover over an interval for its details. Click on the interval to drill down Hour\/Day This views displays the intersection of Hour and Day graphs. The Hour Day Drilling Down in a Workload View Click on an interval to bring up its information. In our example, we selected 11 October in the Month App Count Click User Queue User Queue App Type: MR, User: HDFS, or User: ROOT, We selected the user ROOT so its row is highlighted. Immediately above table is noted what's being displayed. See Applications | Applications App Count r Vcores Hou Memory Hour " }, 
{ "title" : "Data Insights", 
"url" : "uguide/ug-rep-the-reports-page/ug-rep-data-insights.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Reports Page \/ Data Insights", 
"snippet" : "The first two tabs provide data level insights including a snapshot of tables and partitions over the last 24 hours within a historical context. -gives a quick view into the tables' and partitions' size Overview - Details - drills down into the tables. Details See Hive Metastore Configuration Hot Wa...", 
"body" : " The first two tabs provide data level insights including a snapshot of tables and partitions over the last 24 hours within a historical context. -gives a quick view into the tables' and partitions' size Overview - Details - drills down into the tables. Details See Hive Metastore Configuration Hot Warm Cold The last four provide disk management insights and help you manage your disk usage both in terms of capacity and cluster performance. In order to use Forecasting, Small Files, File Reports and Top X you must have the OnDemand - forecasts needed disk capacity based upon past performance Forecasting - generates a list of small files based upon user criteria Small Files - similar to Small Files, except canned reports for large, medium, tiny, and empty files. File Reports - top X Hive or Spark applications with respect to cluster usage, longest duration, and most data I\/O. Top X Click here Overview The Overview Dashboard gives a quick view into the tables' and partitions' size, usage, and KPIs. It is comprised of two (2) sections. Table KPI Partition The time period used to populate the page is noted in the upper right hand corner and the tool tips. Tables & Partitions Tiles Both Table and Partition KPIs sections contain: : Number of Tables\/Partitions accessed, # Accessed : Number of Tables\/Partitions created, # Created : Size of Tables\/Partitions created, and Size Created : Total Number of Tables\/Partitions currently in the system. Total Number The Table KPI's also contains: : Total number of queries accessing the tables, and Accessed Queries : Total Read IO due to accessing the tables. Total Read IO Donut Chart These display the Current Label Distribution Details The details tab has two sections, a graph and a table list. By default the graph uses the Total Users Total Users Total Users, Total Apps, Total Size Graph In this example, the first through third table are selected and graphed. Use the Metric Total Users lTotal Apps Total Size Reset Graph Total Users Table List You can Search Show All Read IO More Info Table Detail Download CSV Table Detail This view summarizes table usage and access metrics, allows you to browse trends (KPIs), and drill down into applications that used the table. lists both Hive and Impala queries The first table in the list above is used for the examples below. The pane's top row lists the table name, start date\/time and the name\/path. Hover over the name\/path to display the complete path. Four KPI’s are displayed: Users # Apps, Size There are three tabs, Table Detail Partition Detail Retention Detail Table Detail Metric Total Users Total Apps, Total Size Application Detail Partition Details Click the Partition Detail The top left of the tab notes the number of partitions loaded, the displayed partition's name, and the view type ( Partition Size MR jobs By default the 100 latest partitions are loaded with the first partition listed graphed in the Partition Size Load All Partitions MR Jobs Chose the partition to graph by selecting the check box to the left of the partition's name. Hovering over the partition name displays the complete name\/path. The partition list can be sorted on Last Access Created Current Size, Users Users Retention Tab This graph initially displays the number of Applications Partition Access View Configuration This pane allows you to define the rules for labeling a Table\/Partition either Hot Warm Cold While the labels are immediately associated with the Tables\/Partitions, the Overview Dashboard You access this modal pane from the Data Details From the pull down menus: chose Age (days) Last Access (days) chose the comparison operator: <= >=. Enter the number of days. To add a second rule: click on the Plus Select the AND OR Repeat steps 1 & 2. To delete a second rule, click on the Minus Click Save Forecasting (Disk Capacity) The OnDemand package must installed to use this report. See here for properties which control this report. It currently only works on Cloudera (CDH) and Hortonworks (HDP). This report helps you monitor HDFS disk capacity usage and plan for future needs. Unravel uses your historical usage to extrapolate capacity trends allowing you to more effectively plan for, and allocate your disk resources. The tab opens displaying the last forecasting report, if any, generated. The graph displays the trend from the historical range start date to the forecast range end date (x-axis). The trend line (in blue) shows the lower, middle, and upper bounds of Unravel's prediction. The y-axis is determined by your actual physical disk capacity. The report parameters are listed above the table headings. Click New Report History (Date Range) Forecasting (number of days) Run Schedule While Unravel prepares to generate the report Run Running New Report New Report You can download the report currently displayed by clicking the Download .csv All reports, whether scheduled or ad hoc, are archived. Successful reports can be viewed or downloaded from the Report Archives tab. Small Files The OnDemand package must be installed to be able to use this report. It requires hdfs privileges and currently only works on HDP\/CDH. If you can't grant hdfs privileges you must configure these properties. See here for properties which control this report. Each small file is accessed by a single mapper, therefore a large number of small files can lead to a large number of mappers. In turn, mappers are costly to run so applications using a large number of small files drive up your costs. This report helps you to identify users who create\/use an excessive amount of small files, allowing you to take corrective action such as: combine multiple files into large files, or notify, limit, or block\/ users who create\/use an excessive amount. in order to: correct and prevent future performance degradation, and lower your costs to run applications. The small file window by default opens with the last report that was generated, if any. The report parameters are listed above the table headings (1), and you can search the path list by string. The list is sorted in descending order of the total number of small files in the directory. Click Download CSV Click New Report : the average small file size in a directory. Uncheck the box to its right to enter a small file size to use instead. Average File Size (bytes) : which are in the directory. Minimum # of Small Files : is the maximum number of directories to display. # of Directories to Show : Advanced Options : minimum depth to start at, root + x descendants, i.e., 0=root, 1=root's children (\/one), etc. Min parent directory depth : maximum depth to end at, root + x descendants, i.e., 1=root's children (\/one), 2=root's grandchildren, (\/one\/two), etc. Max parent directory depth : determines how\/where the files are listed. Yes (default): lists the file in all it's ancestor's list. No: list file in it's directory list only. Drill down sub-directories Click Run Schedule While Unravel prepares to generate the report Run Running New Report New Report Click Download .csv All reports, whether scheduled or ad hoc, are archived. Successful reports can be viewed or downloaded from the Report Archives tab. File Reports The OnDemand package must be installed to use this report. It requires hdfs privileges and currently only works on HDP\/CDH. If you can't grant hdfs privileges you must configure these properties. See here and here for properties which control this report. This report is the same as Small Files except they are automatically generated using the File Reports properties. By default these reports are updated every 24 hours and are not archived. The default size for the files are: large file is any file with more than 100GB size, medium file is any file with 5GB - 10GB size, and tiny file is any file with less than 100KB size. Click on the size buttons ( Large Medium Tiny Empty Download CSV Top X TheOnDemand package must be installed to use this report. This report lists the top X Hive or Spark jobs which have consumed the most: duration, and data I\/O, and cluster usage. Click New Report History (Date Range) Top X Run Running Run New Report The report includes up to X apps (when available). If the report has been successfully completed a light green bar appears noting \"Top X Completed Successfully\". The report displays the Hive jobs by default and notes the reports parameters. Click the Filter By Download JSON The display is composed of three (3) tiles: Applications The Applications tile lists the app total along with successful and failed app count. Three tables list the top X apps by consumption type: Most Duration Most I\/O Most Cluster Usage Duration App Parent Query Snippet Note: The applications in each table are not necessarily the same. The top X apps with the most duration are not necessarily the top X apps using the most I\/O. Resources and Data These tiles display the cumulative totals for the queries. The Data Read Write I\/O Resources Map Reduce Total Spark Slot Time " }, 
{ "title" : "Cloud Reports", 
"url" : "uguide/ug-rep-the-reports-page/uguide-reports-cloud.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Reports Page \/ Cloud Reports", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Overview", 
"url" : "uguide/ug-rep-the-reports-page/uguide-reports-cloud.html#UUID-dc6bff81-1c74-e182-7756-cbd10122bd18_N1552540149781", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Reports Page \/ Cloud Reports \/ Overview", 
"snippet" : "One of the more challenging aspects of optimizing the resources in your Hadoop cluster is determining how to migrate your on-prem cluster to the cloud to save money, reduce maintenance, and increase your agility. Cloud Reports helps you to understand your current cluster and plan your migration. -an...", 
"body" : "One of the more challenging aspects of optimizing the resources in your Hadoop cluster is determining how to migrate your on-prem cluster to the cloud to save money, reduce maintenance, and increase your agility. Cloud Reports helps you to understand your current cluster and plan your migration. -analyzes your on-prem cluster usage, workload patterns, and your hosts' hardware specs. Cluster Discovery -estimates the cost of moving to the cloud. Unravel analyzes three migration strategies by cloud provider (Azure, EC2, and EMR). Migration Analysis : one-to-one mapping of each existing host’s Lift and Shift capacity : one-to-one mapping of each existing host’s Cost Reduction actual usage : unlike the other methods this is not a one-to-one mapping. Unravel analyzes your workload for the time period and bases it recommendations on that workload. Unravel provides multiple recommendations based on the resources needed to meet X% of your workloads. It determines the optimal assignment of VM types to meet the requirements while minimizing cost. This method is typically the most cost effective method. Workload Fit Cloud Reports is generated base upon an analysis of your cluster workload over a specific date range. You can use it to: analyze on-prem clusters running Ambari running HDP 2.5 - 2.6, or Cloudera Manager running CDH 5 and examine the costs of running your clusters on three cloud providers: Azure HDInsight, Amazon EC2, and Amazon EMR. Limitations Cloud reports has not been thoroughly tested for clusters running Kerberos. Your cluster must have at least seven days of metrics for Unravel to generate useful reports. The pricing in reports use a static list of VM Instance type specs. Azure HDInsight: https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/hdinsight\/ Amazon EC2: https:\/\/aws.amazon.com\/ec2\/instance-types\/ Amazon EMR: https:\/\/aws.amazon.com\/emr\/pricing\/ Clusters running MapR Control System are not supported. In order to use Cloud Reports you must have OnDemand cluster manager See here - a dashboard containing information about your on-prem cluster Cluster Discovery - generates a report comparing costs for multiple strategies and aggregates the results by the VM type. Cloud Mapping per Instance - generates a report comparing costs for multiple strategies and shows the mapping for each individual host. Cloud Mapping per Host " }, 
{ "title" : "Cluster Discovery", 
"url" : "uguide/ug-rep-the-reports-page/uguide-reports-cloud.html#UUID-dc6bff81-1c74-e182-7756-cbd10122bd18_N1552539868963", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Reports Page \/ Cloud Reports \/ Cluster Discovery", 
"snippet" : "The dashboard provides overall information about your cluster and is comprised of six tiles. - cluster configuration details and host information. On-Prem Cluster Identity Overall cluster usage Applications submitted by App Type User Queue CPU Memory A Heatmap The tab opens showing the last report s...", 
"body" : "The dashboard provides overall information about your cluster and is comprised of six tiles. - cluster configuration details and host information. On-Prem Cluster Identity Overall cluster usage Applications submitted by App Type User Queue CPU Memory A Heatmap The tab opens showing the last report successfully generated, if any. Click New Report . History (Date Range) Run Running Run New Report If the report has been successfully generated NEW REPORT NEW REPORT DOWNLOAD JSON On-Prem Cluster Identity This tile contains information about your cluster. Click on Hosts Host Summary The table lists the hardware specs for each host and can be searched by name. Cluster overall usage of Applications grouped by App Type, User, and Queue. The donut graphs display the top ten (10) of each category. Cluster Resource Availability & Usage These graph your cluster's CPU and memory. The average usage is listed on the right hand side of the title bar. Hover over the the parenthetical text next to the resource to see Unravel's analysis of your usage. Below we see the allocated CPU resource is \"Very under-utilized and over-provisioned\". Cluster Heatmap The heatmap shows the actual CPU\/Memory usage and capacity by a weekday and hour, e.g., Monday between 5 and 6 pm.Each time slot in the heat map represents how relatively " }, 
{ "title" : "Generating Reports for Cloud Mapping per Instance and per Host Tabs", 
"url" : "uguide/ug-rep-the-reports-page/uguide-reports-cloud.html#UUID-dc6bff81-1c74-e182-7756-cbd10122bd18_N1552540242292", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Reports Page \/ Cloud Reports \/ Generating Reports for Cloud Mapping per Instance and per Host Tabs", 
"snippet" : "The below example is for Cloud Mapping per Instance; Cloud Mapping per Host differences only in the title. Click NEW REPORT Azure , EC2 . EMR Advanced Option Instance VM Type Run Running Run NEW REPORT DOWNLOAD JSON In the examples below we are using a report generated for EMR, with no instance type...", 
"body" : "The below example is for Cloud Mapping per Instance; Cloud Mapping per Host differences only in the title. Click NEW REPORT Azure , EC2 . EMR Advanced Option Instance VM Type Run Running Run NEW REPORT DOWNLOAD JSON In the examples below we are using a report generated for EMR, with no instance type picked. " }, 
{ "title" : "Cloud Mapping per Instance", 
"url" : "uguide/ug-rep-the-reports-page/uguide-reports-cloud.html#UUID-dc6bff81-1c74-e182-7756-cbd10122bd18_N1552540343800", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Reports Page \/ Cloud Reports \/ Generating Reports for Cloud Mapping per Instance and per Host Tabs \/ Cloud Mapping per Instance", 
"snippet" : "This window provides a summary of the reports in Cloud Mapping by Hosts. By default the tab opens displaying the Lift and Shift Total Hourly Cost Cost Reduction...", 
"body" : "This window provides a summary of the reports in Cloud Mapping by Hosts. By default the tab opens displaying the Lift and Shift Total Hourly Cost Cost Reduction " }, 
{ "title" : "Cloud Mapping per Host", 
"url" : "uguide/ug-rep-the-reports-page/uguide-reports-cloud.html#UUID-dc6bff81-1c74-e182-7756-cbd10122bd18_N1552540200429", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Reports Page \/ Cloud Reports \/ Generating Reports for Cloud Mapping per Instance and per Host Tabs \/ Cloud Mapping per Host", 
"snippet" : "This tab is where Unravel is generating the reports on the three migration strategies: -- a one-to-one mapping of each on-prem host to the best possible fit on the cloud based on capacity. Lift and Shift -- a one-to-one mapping of each on-prem host to the best possible fit on the cloud based on aver...", 
"body" : "This tab is where Unravel is generating the reports on the three migration strategies: -- a one-to-one mapping of each on-prem host to the best possible fit on the cloud based on capacity. Lift and Shift -- a one-to-one mapping of each on-prem host to the best possible fit on the cloud based on average utilization. Cost Reduction -- a mapping based upon meeting SLA percentiles. Workload Fit The tab opens displaying the Lift and Shift Cost Reduction t Workload Fi Below we examine the three migration reports mapping the cluster to EMR with no specific instance picked. The Total Hourly Cost Lift and Shift Workload Fit Lift and Shift and Cost Reduction The report layout are exactly the same. The basis for the mapping recommendation is listed under the selection tabs, the hourly cost to map all hosts is aggregated and displayed in the upper right above the report table. The table has four columns: - in this case the on-prem host, Instance - the host's resources which were actually utilized, Actual Usage - the resources the host has available, Capacity - the cloud instance Unravel recommends mapping the host to, and Recommendation of the instance. Cost\/Hour Lift and Shift Lift and Shift is matching capacity, and therefore not attempting to minimize costs. You can see below the on-prem host is very underutilized. Since Unravel is matching capacity instance the on-prem host is mapped to will also be underutilized. This report mapped 20 hosts, only the first is shown. The hourly cost per instance is $5.69 for a Total Hourly Cost Cost Reduction Cost Reduction is again mapping each host to an instance (1-1), this time based on actual usage not capacity. Again we are only showing the first mapping. The EMR instance Unravel recommends is c3.8xlarge instead of m5d.24xlarge recommended above. The cost savings is significant compared to Lift and Shift; the Total Hourly Cost Workload Fit In this report, Unravel is not looking at individual host capacity\/usage but finding the optimal allocations of VM to meet a percentage of the workloads, specifically 80 - 100% in 5 point increments. For specifics details click on SLA x% mapping scroll through the window on the left size or click directly on the percentage. Below we see to meet 100% SLA – Unravel is recommends the same cloud server instance as in Lift and Shift, but instead of mapping to 1-1, it recommends using just six (6). The bar graph shows the SLA\/hour costs at a glance. You quickly can see there are costs saving from 100% to 95% (5%) and 95% to 90% (11%) but further reducing the SLA percentile results little to no savings. When comparing the total hourly cost of 100% SLA Workload Fit to Lift and Shift and Cost Reduction shows savings of 75% and 25% respectively. " }, 
{ "title" : "Scheduling Reports", 
"url" : "uguide/ug-rep-the-reports-page/ug-rep-scheduling-reports.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Reports Page \/ Scheduling Reports", 
"snippet" : "Instead of generating your report immediately you can schedule the report to run on an ongoing basis. For any given report, you enter the necessary parameters and then click Schedule You can not alter the Report Notification Save Schedule...", 
"body" : "Instead of generating your report immediately you can schedule the report to run on an ongoing basis. For any given report, you enter the necessary parameters and then click Schedule You can not alter the Report Notification Save Schedule " }, 
{ "title" : "Reports Archive\/Scheduled Reports", 
"url" : "uguide/ug-rep-the-reports-page/ug-rep-reports-archive-scheduled-reports.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Reports Page \/ Reports Archive\/Scheduled Reports", 
"snippet" : "Table of Contents Reports Archive Scheduled Reports Note Click here...", 
"body" : " Table of Contents Reports Archive Scheduled Reports Note Click here " }, 
{ "title" : "Reports Archive", 
"url" : "uguide/ug-rep-the-reports-page/ug-rep-reports-archive-scheduled-reports.html#UUID-089a5064-271d-02dc-ea61-bab791d85249_N1552537890247", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Reports Page \/ Reports Archive\/Scheduled Reports \/ Reports Archive", 
"snippet" : "Lists all reports and attempts to generate a report, whether scheduled or on ad hoc basis. The status of the report is color coded; in the example below, three reports succeeded while one failed. You can Filter By . Search Successful reports can be viewed or downloaded. Click on the Report ID to vie...", 
"body" : "Lists all reports and attempts to generate a report, whether scheduled or on ad hoc basis. The status of the report is color coded; in the example below, three reports succeeded while one failed. You can Filter By . Search Successful reports can be viewed or downloaded. Click on the Report ID to view and Example of Capacity Forecasting .csv file. Date Max Capacity Observed Capacity Trend Lower Trend Upper Trend 08\/23\/18 02:00 AM 3.172 kB 528.000 B 528.098 B 527.120 B 528.970 B 08\/29\/18 02:00 AM 3.172 kB 527.000 B 528.171 B 527.248 B 529.097 B 08\/28\/18 01:00 PM 3.172 kB 527.000 B 526.385 B 525.443 B 527.316 B 08\/27\/18 06:00 PM 3.172 kB 528.000 B 528.332 B 527.420 B 529.231 B 08\/29\/18 07:00 PM 3.172 kB 532.000 B 530.328 B 529.377 B 531.289 B 08\/26\/18 11:00 PM 3.172 kB 528.000 B 527.933 B 527.033 B 528.801 B 08\/26\/18 04:00 AM 3.172 kB 528.000 B 528.101 B 527.199 B 529.004 B 08\/22\/18 11:00 AM 3.172 kB 528.000 B 527.806 B 526.847 B 528.694 B 08\/24\/18 12:00 PM 3.172 kB 528.000 B 527.651 B 526.691 B 528.616 B 08\/26\/18 05:00 AM 3.172 kB 528.000 B 527.818 B 526.921 B 528.706 B 08\/29\/18 12:00 PM 3.172 kB 527.000 B 528.742 B 527.871 B 529.649 B 08\/28\/18 07:00 AM 3.172 kB 527.000 B 525.778 B 524.834 B 526.672 B Small File Report Small Files is a .csv file. The report is sorted on the number of file in descending order. Directory Number of Files Avg File Size Total File Size Min File Size Max File Size \/apps 26554 219.7 kB 5.6 GB 0 B 192.7 MB \/apps\/hbase 14074 637.0 B 8.6 MB 0 B 3.5 MB \/apps\/hbase\/data 14074 637.0 B 8.6 MB 0 B 3.5 MB \/apps\/hbase\/data\/data 14063 637.0 B 8.6 MB 0 B 3.5 MB \/apps\/hbase\/data\/data\/default 14044 378.0 B 5.1 MB 0 B 2.5 MB \/apps\/hive 12476 435.3 kB 5.2 GB 13.0 B 48.5 MB \/apps\/hive\/warehouse 12476 435.3 kB 5.2 GB 13.0 B 48.5 MB " }, 
{ "title" : "Scheduled Reports", 
"url" : "uguide/ug-rep-the-reports-page/ug-rep-reports-archive-scheduled-reports.html#UUID-089a5064-271d-02dc-ea61-bab791d85249_N1552537990066", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Reports Page \/ Reports Archive\/Scheduled Reports \/ Scheduled Reports", 
"snippet" : "Lists all the currently scheduled reports by report type: Cluster Optimization, Capacity Forecasting, or Small Files. At a quick glance you can see a report's Schedule Next Run Edit ( schedule More info ( Close...", 
"body" : "Lists all the currently scheduled reports by report type: Cluster Optimization, Capacity Forecasting, or Small Files. At a quick glance you can see a report's Schedule Next Run Edit ( schedule More info ( Close " }, 
{ "title" : "The Application Managers", 
"url" : "uguide/uguide-apms.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Application Managers", 
"snippet" : "Applications Managers Spark Application Manager...", 
"body" : " Applications Managers Spark Application Manager " }, 
{ "title" : "Applications Managers", 
"url" : "uguide/uguide-apms/uguide-apms-managers.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Application Managers \/ Applications Managers", 
"snippet" : "Note See here See here See here...", 
"body" : " Note See here See here See here " }, 
{ "title" : "Typical Application Manager's Layout", 
"url" : "uguide/uguide-apms/uguide-apms-managers.html#UUID-0e90f639-3b9e-ee42-c9b7-ec4ea2fcad08_N1552637634699", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Application Managers \/ Applications Managers \/ Typical Application Manager's Layout", 
"snippet" : "A black title bar notes the application type, i.e., Spark, Impala, MapReduce, Fragment, etc) and the job ID. On the right side of the title bar are glyphs for adding a comment, and to minimize or close the tile if possible. If the jobs has a parent, i.e., Hive, Pig, there will be a arrow with the pa...", 
"body" : " A black title bar notes the application type, i.e., Spark, Impala, MapReduce, Fragment, etc) and the job ID. On the right side of the title bar are glyphs for adding a comment, and to minimize or close the tile if possible. If the jobs has a parent, i.e., Hive, Pig, there will be a arrow with the parent's type. Clicking on it brings up its APM. If it is a running yarn job (MR, Tez, or Spark) there is an action box ( Unravel's Intelligence Engine can provide insights into an application and may provide recommendations, suggestions and insights on how to improve the application's run. When there are insights a bar appears immediately below the title bar. If Unravel has recommendations the insight bar is orange, otherwise it's blue. For more information about events, see the Event Panel Example The next section contains general job information and Key Performance Indicators (KPIs) (as applicable) : notes the number of events the job had. Event icon No Events Event Panel & Insights notes the job type and status. The box is colored code to indicate as the application's status. Job icon: Next to the job name will be an auto actions glyph ( Job Name: : job number, owner, queue, cluster and start\/stop time. Job Information these vary by job type. KPIs: The last section, typically divided into two, has specific information related to job. Each Application-Specific Manager Section goes into detail about this section. If the job is composed of tasks\/jobs\/stages they appear on the the left under Navigation Common Tabs: The Hive, MapReduce and Workflow APMs contain this tab. It lists all errors associated with the job. Like job status, the errors are color coded and number for each type (fatal, errors, warnings) are noted for each job. The top line list the number of all jobs\/task. The errors are grouped by tasks\/jobs and then by severity. For each job\/task the total and type of errors are noted. Time, keywords (if any) and a brief message is displayed for the error. Errors: Keywords : The MapReduce, Spark and Tez APMs contain this tab. It lists the configuration parameters for the task\/job being displayed and their values. The parameters vary according to task\/job. Conf\/Configuration : All APM's except Pig and Cascading contain this tab. It lists the defined tag keys and associated values for the application. The example below has two tag keys, Tags project dept group11 hr " }, 
{ "title" : "Cascading and Pig Application Managers", 
"url" : "uguide/uguide-apms/uguide-apms-managers.html#UUID-0e90f639-3b9e-ee42-c9b7-ec4ea2fcad08_N1552637799617", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Application Managers \/ Applications Managers \/ Cascading and Pig Application Managers", 
"snippet" : "The only Key Performance Indicators : The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the query. Duration : Total data read and written by the query. Data I\/O : The number of apps that make up the workflow. Number of Yarn Apps By defaul...", 
"body" : "The only Key Performance Indicators : The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the query. Duration : Total data read and written by the query. Data I\/O : The number of apps that make up the workflow. Number of Yarn Apps By default the window open up displaying the Navigation and Task Attempts. Left Tabs : Exceptions, errors, and warnings associated with this application. See here for an example. Gantt Chart : See here for an example. Tags Right Tabs : Displays Map and Reduce task attempts by success, failed, and killed status.The data displayed is for the entire HIVE job. To see the details for a specific MapReduce task click on the job in the Navigation tab. The Pig APM above shows the Task Attempts. Task Attempts : Graphs the Map and Reduce task slot usage over the duration of the job. The wall clock time is noted in the upper left hand corner. The computer slot usage is noted below the graph. Attempts " }, 
{ "title" : "Hive Application Manager", 
"url" : "uguide/uguide-apms/uguide-apms-managers.html#UUID-0e90f639-3b9e-ee42-c9b7-ec4ea2fcad08_N1552637860661", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Application Managers \/ Applications Managers \/ Hive Application Manager", 
"snippet" : "The Hive Application Manager provides a detailed view into the behavior of Hive queries. Typical users are Hadoop DBAs or application owners (engineers, BI team, analysts). You can use this view to: Resolve inefficiencies, bottlenecks and reasons for failure within applications. By default the Hive ...", 
"body" : "The Hive Application Manager provides a detailed view into the behavior of Hive queries. Typical users are Hadoop DBAs or application owners (engineers, BI team, analysts). You can use this view to: Resolve inefficiencies, bottlenecks and reasons for failure within applications. By default the Hive APM opens displaying the Navigation Query Key Performance Indicators : The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the application to complete execution. Duration : Total data read and written by the application. Data I\/O : The number of YARN apps making up the query. Number of YARN apps Left Tabs : List all the MapReduce jobs associate with the query. Click on the job name to bring up job in the Navigation MapReduce Application Manage : Shows detailed information about the MapReduce jobs and their relationship with one another. Execution Graph The graph provides a quick and intuitive way to understand the MapReduce jobs. Upon opening the tab you immediately see the MR jobs (1) in relation to each other along some job info: tables used, the job length in absolute and relative value to the whole. Clicking on the job brings up a box with more Table KPI's, forward path(s) for the Map and Reduce operations, and input paths (should you want to show them). Click on a table name to bring up the table information Close Click on a path point (3) drill deeper. The resulting text box notes the operation type (i.e., MapJoin, ReduceSink, etc.), and various key information about the operation. The information displayed is specific to that operation at that time. : Shows job sequencing using a gantt chart. Gantt Chart : Exceptions, errors, and warnings associated with this application. See Errors here : Lists defined tag keys and associated values. See Tags here Right Tabs : Shows the Hive Query. See the Hive Application Manager Query window Copy Query A list tables accessed by Application. Tables: : Displays MapReduce task attempts by success, failed, and killed status. The data displayed is for the Task Attempts entire : Graphs the Map and Reduce task slot usage over the duration of the job. The wall clock time the job started is listed in the upper left hand corner. The total Map and Reduce slot duration time is noted below the graph. Attempts " }, 
{ "title" : "Impala Application Manager", 
"url" : "uguide/uguide-apms/uguide-apms-managers.html#UUID-0e90f639-3b9e-ee42-c9b7-ec4ea2fcad08_N1552637997239", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Application Managers \/ Applications Managers \/ Impala Application Manager", 
"snippet" : "The Impala Application Manager provides a detailed view into the behavior of Impala queries. Key Performance Indicators : The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the query. Duration : Total data read and written by the query. Da...", 
"body" : "The Impala Application Manager provides a detailed view into the behavior of Impala queries. Key Performance Indicators : The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the query. Duration : Total data read and written by the query. Data I\/O : Total number of query fragments. Number of Fragments : Total number of operators in this query. Number of Operators Left Tabs : Displays a table with information about each fragment associated with this query. Click on Fragments More L ess This window shows the Fragment and it's KPIs. It defaults to the table of the Fragment's Operators with the associated KPIs for the operations. Clicking on the operator brings up the operator window. (See Operators Query Plan lists each instances with it's KPI's. Instance View: : Displays a list of all operators for all fragments. You can search the operators name. Click on the operator to display its details. Operators Scan HDFS details Aggregate Details Exchange Details : Charts the fragments and the time spent on each operation. Hover over a section see the operation and it's KPI's. Gannt Chart : Shows the query plan in fragment or operator view. Both the fragment and operator view are shown below. Hover over the operator to get detailed information. Click on the button to switch views. Query Plan : Lists defined tag keys and associated values. See Tags here Right Tabs : Shows the query plan code. Click on Query Query Copy window : Graphs the Memory Usage by peak usage. Notes the maximum memory used on what host and the estimated memory per host. Mem Usage " }, 
{ "title" : "Kafka Application Manager", 
"url" : "uguide/uguide-apms/uguide-apms-managers.html#UUID-0e90f639-3b9e-ee42-c9b7-ec4ea2fcad08_N1552637973828", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Application Managers \/ Applications Managers \/ Kafka Application Manager", 
"snippet" : "The Kafka Application Manager provides Multi-Cluster support for monitoring: Multi Cluster Metrics Monitoring, and Multi Cluster Consumer Offset\/Lag Monitoring. See Kafka Insights lagging or stalled | Operations Charts Kafka Configured Kafka Clusters Key Performance Indicators Bytes in\/sec Bytes out...", 
"body" : "The Kafka Application Manager provides Multi-Cluster support for monitoring: Multi Cluster Metrics Monitoring, and Multi Cluster Consumer Offset\/Lag Monitoring. See Kafka Insights lagging or stalled | Operations Charts Kafka Configured Kafka Clusters Key Performance Indicators Bytes in\/sec Bytes out\/sec Messages in\/sec Total Fetch Requests per \/sec Number of Active Controller Number of Under Replicated Partitions Number of Offline Partitions Click on the Cluster Name to bring up the Cluster View Cluster View This view has three sections: Key Performance Indicators Metric Graphs kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions kafka.controller:type=KafkaController,name=ActiveControllerCount kafka.server:type=KafkaRequestHandlerPool,name=RequestHandlerAvgIdlePercent kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec kafka.server:type=ReplicaManager,name=PartitionCount kafka.server:type=ReplicaManager,name=LeaderCount kafka.controller:type=KafkaController,name=OfflinePartitionsCount kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Fetch kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Fetch kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce kafka.log:type=LogFlushStats,name=LogFlushRateAndTimeMs kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Produce kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=Fe Kafka Topics List consumed by a Consumer Group (CG) with relevant KPIs. Organized by Topic Topic Brokers Kafka Topic test2 demo test-consumer-group. Consumer Group Consumer Group View Key Performance Indicators Number of Topics Number of Partitions The Topic lists displays the KPIs; when details are available a more info Partition Detail You can chose both the Partition Metric th offset Partition Details' Topic View The Kafka View has two tabs, Topic Detail Partition Detail Consumer Details' Kafka Topic Detail By default the Kafka Topic Detail Topic Detail Kafka Partition Detail You can chose both the Partition Metric th offset " }, 
{ "title" : "Unravel Insights for Kafka", 
"url" : "uguide/uguide-apms/uguide-apms-managers.html#UUID-0e90f639-3b9e-ee42-c9b7-ec4ea2fcad08_N1552637358106", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Application Managers \/ Applications Managers \/ Kafka Application Manager \/ Unravel Insights for Kafka", 
"snippet" : "Auto-detection of Lagging\/Stalled Consumer Groups Unravel determines Consumer status by evaluating the consumer's behavior over a sliding window. For example, we use average lag trend for 10 intervals (of 5 minutes duration each), covering a 50 minute period. Consumer Status is evaluated on several ...", 
"body" : "Auto-detection of Lagging\/Stalled Consumer Groups Unravel determines Consumer status by evaluating the consumer's behavior over a sliding window. For example, we use average lag trend for 10 intervals (of 5 minutes duration each), covering a 50 minute period. Consumer Status is evaluated on several factors during the window for each partition it is consuming. For a topic partition Consumer status is if: Stalled Consumer commit offset for the topic partition is not increasing and lag is greater than zero. if: Lagging Consumer lag for the topic partition is increasing consistently, and, An increase in lag from the start of the window to the last value is greater than lag threshold (e.g., 250). The information is distilled down into a status for each partition, and then into a single status for the consumer. A consumer is either in one of the following states: OK : the consumer is working, but falling behind Warning : the consumer has stopped or stalled. Error " }, 
{ "title" : "MapReduce Application Manager", 
"url" : "uguide/uguide-apms/uguide-apms-managers.html#UUID-0e90f639-3b9e-ee42-c9b7-ec4ea2fcad08_N1552636437831", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Application Managers \/ Applications Managers \/ MapReduce Application Manager", 
"snippet" : "The MapReduce Application Manager provides and easy way to understand the breakdown of the application. You can use this view to: Drill down into MapReduce jobs that make up the application, and Resolve inefficiencies, bottlenecks and reasons for failure within applications. It contains similar sect...", 
"body" : "The MapReduce Application Manager provides and easy way to understand the breakdown of the application. You can use this view to: Drill down into MapReduce jobs that make up the application, and Resolve inefficiencies, bottlenecks and reasons for failure within applications. It contains similar sections to the Hive Application Manager and additionally shows the timeline view of MapReduce job execution, logs and configuration. Key Performance Indicators : The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the application to complete execution. Duration : Total data read and written by the application. Data I\/O Tabs By default the MapReduce APM opens in the Graphs | Attempts : Has four (4) sub tabs. Graphs : Number of task attempts are charted in \"wall-clock\" time. The aggregated time of all tasks running in on the Map\/Reduce slot duration is noted below the graph. Attempts , Containers Vcores, : Displays the details of each MapReduce job by showing the execution of each task on the machine it was executed on. Timeline The Timeline tab is divided into two sections: a Distribution Map Reduce a bottom table which lists either the tasks by stages on servers or the list of tasks and their associated KPIs' The default displays the Map jobs and the timeline. You can change the Distribution Charts by selecting Map Reduce Timeline Selected : The metrics, their definitions and values. Metrics Lists the available logs by Map, Reduce and Application Master. Click on the tab to see the listing for that type (Map, Reduce, or Application Master). Click on an item to see the log. Logs: The defined parameters and their values. Configuration: : Resource Usage Initially all the executors are displayed using the Metric Metric nly Show All : Exceptions, errors, and warnings associated with this application. See Errors here : Lists defined tag keys and associated values. See Tags here " }, 
{ "title" : "Tez Application Manager", 
"url" : "uguide/uguide-apms/uguide-apms-managers.html#UUID-0e90f639-3b9e-ee42-c9b7-ec4ea2fcad08_N1552637938468", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Application Managers \/ Applications Managers \/ Tez Application Manager", 
"snippet" : "The Tez Application Manager provides a detailed view into the behavior of Hive queries as a DAG (Directed Acyclic Graph). To troubleshoot Tez data collection issues, check \/usr\/local\/unravel\/logs\/unravel_ew_1.log Key Performance Indicators : The number, if any, of Unravel insights for this query. Se...", 
"body" : "The Tez Application Manager provides a detailed view into the behavior of Hive queries as a DAG (Directed Acyclic Graph). To troubleshoot Tez data collection issues, check \/usr\/local\/unravel\/logs\/unravel_ew_1.log Key Performance Indicators : The number, if any, of Unravel insights for this query. See Events Event Panel & Insights : Total time taken by the query. Duration : Total data read and written by the query. Data I\/O By default the Tez APM opens showing the Navigation and Program Tabs. Left Tabs : List the Dag jobs with KPIs, Duration and I\/O. See the Dag Detail information Navigation below : List the configuration parameters and their values. Configuration : Lists defined tag keys and associated values. See Tags here Right Tabs : Displays the query. Program : Has three (3) sub tabs. Graphs , Containers Vcores Memory : Graphs the resources consumed. By default the Resources Resource systemCpuLoad Select series Metric Get Data DAG Detail The DAG detail has six tabs: Displays the query. Query: Displays the vertices and their relationship to each other. Clicking on a node brings up the task details. Graph: : Lists all the relevant counters for the Tez-DAG and their values. Counter Vertex Timeline Wall Clock Total Run All Vertices : List all tasks, their status (failed, success, etc.), vertex name and other relevant information. The tasks are searchable by Task Id and Vertex name; Tasks containing the string will be displayed. All Task : List all attempts, their status (failed, success, etc.), vertex name and other relevant information. The task attempts are searchable by Attempt Id, Task Id and Vertex name; Task attempts containing the string will be displayed. All Task Attempts : Lists all relevant parameters and their value. Changed Configuration " }, 
{ "title" : "The Workflow Manager", 
"url" : "uguide/uguide-apms/uguide-apms-managers.html#UUID-0e90f639-3b9e-ee42-c9b7-ec4ea2fcad08_N1552637921907", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Application Managers \/ Applications Managers \/ The Workflow Manager", 
"snippet" : "The Workflow Manager provides a comprehensive view to understand workflows and their patterns of execution. It is used by Workflow (Pipelines) owners to identify anomalies, inefficiencies and bottlenecks in workflow instances. The Workflow Manager helps pipeline owners easily maintain SLAs. (Applica...", 
"body" : "The Workflow Manager provides a comprehensive view to understand workflows and their patterns of execution. It is used by Workflow (Pipelines) owners to identify anomalies, inefficiencies and bottlenecks in workflow instances. The Workflow Manager helps pipeline owners easily maintain SLAs. (Applications that have a Workflow parent will have a link to the workflow in the Goto Applications Applications Key Performance Indicators : The number, if any, of Unravel insights for this query. See G>Events Event Panel & Insights : Total time taken by the query Duration : Total data read and written by the query. Data I\/O : The number of apps that make up the workflow Number of Yarn Apps The APM opens showing the Navigation Compare Left Tabs : Provides an easy way to understand the breakdown of the workflow the applications which comprise the Workflow, i.e., Hive, Spark, MapReduce, Oozie. Click on Navigation More Below the second Oozienode is shown, it is comprised of one MapReduce job and three Hive jobs. The hive jobs comprise one or more tasks, so that too can be expanded. In the example below, the second Oozienode has been expanded along with the first hive job within it. You can click on any job to see the application manager for it. In the example, below you can click on the expanded hive job to bring up the hive application manager. Similarly you can click on the mapreduce job within the hive job to go directly to it. Click on Less : Displays the execution graph of the workflow. Click Execution : Exceptions, errors, and warnings associated with this application. See Errors here : See Tags here Right Tabs : Provides a quick way to understand how well a workflow run compares to its other runs. Hovering your pointer graph displays instances top KPIs such as Compare duration data I\/O, resources the number of jobs Metrics I\/O MR Jobs Resource Events above : Displays charts for Map Task, Reduce, and Spark Tasks, broken down by success, failed, and killed as appropriate. Task Attempts n\/a " }, 
{ "title" : "Spark Application Manager", 
"url" : "uguide/uguide-apms/uguide-apms-spark.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Application Managers \/ Spark Application Manager", 
"snippet" : "Table of Contents Overview The Spark Application Manager's Basic Layout Actions Common Tabs Scala, Java, PySpark, and SQL-Query Navigation Tab Common Tiles Spark - Scala, Java, and PySpark Spark - SQL-Query Spark - Streaming Overview A Spark Application consists of one or more Jobs, which in turn ha...", 
"body" : " Table of Contents Overview The Spark Application Manager's Basic Layout Actions Common Tabs Scala, Java, PySpark, and SQL-Query Navigation Tab Common Tiles Spark - Scala, Java, and PySpark Spark - SQL-Query Spark - Streaming Overview A Spark Application consists of one or more Jobs, which in turn has one or more Stages. : corresponds to a spark action, e.g., count, take, foreach. Job The Job Stage The Spark Application Manager (APM) allows you to: Quickly see which jobs and stages consumed the most resources, View your application as a RDD execution graph Drill into the source code from the stage tile, spark stream batch tile, or the execution graph to locate the problems. You can use the APM to analyze an application's behavior to: Resolve inefficiencies, bottlenecks, and reasons for failure within applications, Optimize resource allocation for Spark driver and executors, Detect and fix poor partitioning, Detect and fix inefficient and failed Spark apps, and Tune JVM settings for driver and executors. Unravel provides insights into Spark applications and potentially tuning recommendations. There are multiple Spark application types and the Spark APM information can vary by the application type. Currently Unravel distinguishes between: , Scala, Java, and PySpark , and, SQL-Query . Streaming Regardless of the application type and how they are are submitted (e.g., from Notebooks, spark shells, or spark-submit), the Spark APMs are similar and there are common tabs\/information across all types. The Spark Application Manager's Basic Layout A black title bar notes the type of tile (Spark, Job, Stage, etc). On the right side there are an actions Unravel's Intelligence Engine provides insights into an application and may provide recommendations, suggestions, or insights into how to improve the application's run. When there are insights, a bar appears immediately below the title bar. If Unravel has recommendations, the insight bar is orange, otherwise it's blue. For more information about events, see Events and Insights The next section contains the Key Performance Indicators (KPIs) and general application information. : notes the number of events the Spark application had. If there were none, Event icon No Events Event Panel notes the application's status and the window type (S-Spark, SJ-Spark Application and so on.). The box is colored code based upon its status. Application icon: notes the job type and status. The box is colored code to indicate as the application's status. Job icon: Next to the job name will be an auto actions glyph Job Name: : job number, owner, queue, cluster and start\/stop time. Job Information these vary by job type. KPIs: The last section, divided in half, has specific information related to the application. The sections for a specific Spark Application (e.g., Streaming) go into more detail. If the application is composed of tasks\/jobs\/stages they appear on the the left side under Navigation Stream Actions Click on When a yarn application is running you can kill or move it. After the application has stopped, you can load logs and diagnostics. Clicking Load Diagnostics Load Logs Logs Common Tabs Except for the Spark streaming Left Tabs - Lists all errors associated with the application. The errors are color coded (fatal Errors Keywords - Lists the critical logs that were collected for this Spark application. It is noted when no logs are available. Logs Click on the log name to see it, below is an excerpt of the executor-20 log. Lists the configuration parameters and their values. The parameters vary according to app\/task\/job. The tab opens listing all the properties. The number of properties displayed is noted above the list. Conf - You can narrow the list by choosing the configuration type to display; to see the spark version select metadata. Metadata and driver are selected in the example below and the list narrowed from 1042 to two (2) properties. Some properties appear in multiple categories, e.g., spark.executor.extraJavaOptions is listed under Memory, Driver and Executor. You can search by name; searching on yarn displays every property containing the word yarn. Click Reset Right Tabs - Displays the program's source code if available. Both the Spark Stage and Execution graph link to tab and display code associated with each. Note: Unravel allows you to upload Spark programs, see Program Uploading Spark Programs. - Graphically and textually notes the number of tasks and their status. The donut graphs show the percentage of successful (green), failed (orange), and killed (red) tasks. The legend on the right lists the number of each. The graph on the left shows a job in which all tasks succeeded, while the graph on the right has all failed tasks. Frequently, the result is a combination. Hovering over the chart tells you the percentage of each. Task Attempts There are three (3) types of graphs. Graphs use \"wall clock time\" as opposed to computer usage time. Graphs - : The number of running containers. Running Containers : Allocated vcores. Vcores : Allocated memory. Memory - Graphs the resources the application\/job\/stage consumed. By default the graph displays all executors using the metric Resource availableMemory Get Data You chose one or more series to display using the Select X Hovering only resource highlights it, clicking on it toggles the display, i.e., if currently displayed it is removed from the graph. Conversely if you click the Only Show All Below is an example of the JSON when clicking on Get Data Scala, Java, PySpark, and SQL-Query Navigation Tab - Lists the application's jobs with their relevant KPIs: Navigation Status Start Time Duration , Tasks Read Write Stages Start Time Tasks Stages In this example, 39,600 Tasks Stages Clicking on a job row brings up its details in a Job Block. You can open up the job even if it's running. The job block lists the KPIs Duration # of Stages , Stages Gannt Metadata ID Status Start Time Duration Tasks , Shuffle Read Shuffle Write Input Output Start Time Spark Stage This example is for Job 1 above, which is only 41% complete.Here the # of Stages Common Tiles Spark Job A job is created for every Spark action, e.g., count, take, foreach. A job is comprised of one or more stages. The job below has three stages, two (2) keyby Key Performance Indicators The wall-clock time it took to complete the job. Duration: Number of stages to the job. # of Stages: It has three tabs: his tab is the default view. It lists the stages with their KPIs and is initially sorted on - Stages Start Time - This tab again shows all the stages, graphically displaying the time spent in each. The stages are initially displayed in order of execution, i.e., first, second, ..., nth. Gannt Chart Lists all the attributes and values for the jobs. Metadata - Spark - Scala, Java, and PySpark Key Performance Indicators : is the number, if any, of Unravel insights for the query. See the Events here : The \"wall clock\" time it took for the application to complete. Duration : The memory used. Data I\/O : The number of stages in the query. Number of Stages Left Five Tabs - Lists the application's jobs with their relevant KPIs: Navigation Status Start Time Duration Partitions\/Tasks Read Write . # Stages above. - The execution graph shows a RDD DAG of the application. The application's Execution RDDs Operations Program If the program tab displays the code it is linked with the DAG. If you display the execution and program tab simultaneously, as shown below. Click on the vertex to highlight relevant code. Below we see the corresponding code for vertex 18. Below we expanded the above area, and vertices 15-19 are shown (the vertex number is noted in the circle). The vertex lists the type of RDD, partitions used, Spark call and finally the number of stages which were involved. Below RDD represented by vertices 17-16 involved two (2) stages, while 15-16 had five (5). Hover over the vertex to bring up an information box, containing the RDD description and CallSite (source line) which called the RDD transformation. Gantt Chart Displays the stages using a Gantt Chart. The table is sorted on Start Time - For an explanation of these tabs see Errors, Log and Conf Tabs Errors Logs Conf Four Right Tabs - When available it displays the program associated with the application. See Program above - For an explanation of these tabs see Task Attempts, Graphs and Resources Task Attempts Graphs Resource Spark - SQL-Query Key Performance Indicators : is the number, if any, of Unravel insights for the query. See the Events here : The \"wall clock\" time it took for the application to complete. Duration : The memory used. Data I\/O : The number of stages in the query. Number of Stages Left Five Tabs - Lists the application's jobs with their relevant KPIs: Navigation Status Start Time Duration Partitions\/Tasks Read Write . # Stages above. - A execution graph of the query. There are times when the DAG is too large to display and it will be noted. See Execution above - Displays the stages using a Gantt Chart. For more details see Gantt Chart above Tabs For an explanation of these tabs see Errors, Log and Conf Errors Logs Conf Four Right Tabs - This tab connects all the pieces of a SQL query. The table lists all queries with significant KPI's and the top five stages, i.e., the stages with the longest duration. The lower section contains two tabs, Program SQL Program By default: The Query table is sorted on the query's duration in descending order. Similarly the stages are sorted on duration in descending order left to right. The SQL tab is displayed using the query view. The query with longest duration (first row) is shown. Click on the Query ID Spark Stage Details Query Plan Copy The screenshot below is showing the default window, the SQL query for Query ID 4. Scroll down to see the entire SQL Plan. Click on query - For an explanation of these tabs see Task Attempts, Graphs and Resources Task Attempts Graphs Resource " }, 
{ "title" : "Spark Stage", 
"url" : "uguide/uguide-apms/uguide-apms-spark.html#UUID-3041356f-bcee-b5b7-5ba2-7cbaddcb3e36_N1552611722669", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Application Managers \/ Spark Application Manager \/ Spark Stage", 
"snippet" : "The Stage tile can help you pinpoint problems with the stage, i.e., skewing, and see the source code associated with the stage. Key Performance Indicators The computer time it took to complete the job. Duration: Date IO It has two tabs, by default it opens in the graph Graph Displays the number of t...", 
"body" : "The Stage tile can help you pinpoint problems with the stage, i.e., skewing, and see the source code associated with the stage. Key Performance Indicators The computer time it took to complete the job. Duration: Date IO It has two tabs, by default it opens in the graph Graph Displays the number of total tasks for the stage and number of total attempts made to run these tasks. The number of tasks is noted on the left side of the bar with the number of attempts on the right. The donut chart graphically displays the successful ( - Task Attempt Displays the stage's program details. Unravel extracts Source file name and call line. Click on the source file or line number to see program code. The Description shows the stage's Stack trace. The first line is always a call to the Spark library. The source file name and line number are extracted from the second line. - Program Details Time Line Tab The Time Line tab has two sections, , and Distribution Charts Three tabs below the chart: , Time Line and Timeline Breakdown . Selected Tasks Time Line The Time Line tab has two sections, , and Distribution Charts Three tabs below the chart: Time Line Timeline Breakdown Selected Tasks Be default the Time Line Distribution Charts ShuffleMap Disk Bytes Spilled Memory Bytes Spilled Read. Records The lower section opens displaying the Time Line , Timeline Breakdown This is useful to identify bottlenecks.For each executor used in the current stage, multiple metrics are graphed: , Scheduler Delay Executor Deserialization Time Fetch Wait Time Executor Computing Time JVM GC time Result Serialization Time Getting Result Time Using the ratio of Executor Computing Time performing actual work thrashing, or waiting for scheduling. A list of tasks, if any, for the stage. Selected Tasks " }, 
{ "title" : "Spark - Streaming", 
"url" : "uguide/uguide-apms/uguide-apms-spark.html#UUID-3041356f-bcee-b5b7-5ba2-7cbaddcb3e36_N1552611588233", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ The Application Managers \/ Spark Application Manager \/ Spark - Streaming", 
"snippet" : "Key Performance Indicators The KPI's refer to the entire Spark job composed of jobs which in turn have stages. : is the number, if any, of Unravel insights for the query. See the Events here : Wall clock time time by the Spark job (the total time to process all stages – Duration : Total data read an...", 
"body" : " Key Performance Indicators The KPI's refer to the entire Spark job composed of jobs which in turn have stages. : is the number, if any, of Unravel insights for the query. See the Events here : Wall clock time time by the Spark job (the total time to process all stages – Duration : Total data read and written by the query. Data I\/O : The number of jobs that make up the streaming. Number of Jobs : The number of stages that make up the streaming. Number of Stages Unlike other Spark Application Managers this has a Stream tab Stream Program Left Four Tabs - Displays the core of an Streaming Application. From here you drill down into the batches, the main processing unit for Spark streaming. The graph displays the number of events\/second (the bars) with a superimposed line graph of the chosen metric. By default Stream Scheduling Delay Metric , Scheduling Delay Processing Time . Total Delay It has two sections; by default, they display the entire run over the last 7 days. You can zoom in on a section of graph by pulling the tabs left or right (2). The table lists the Completed Batches relevant to the time period selected. Each batch has its KPI's listed. In the view above, the entire stream time is displayed, therefore all Completed Batches are displayed and in this case there are seven (7) pages. In the example below, we have zoomed in on the last two minutes, the table now lists the batches completed in that time period. The tables now contains only one (1) page, versus the seven (7) above. The table lists only the first three batches, but you can page through the table (3). By default, the streams are sorted on start time in ascending order. When you sort the batches, they are sorted across all tables, i.e., if Start Time Click on a batch to bring up the Spark Stream Batch tile. You can only open one batch job at a time. The batch window lists all the jobs associated with the batch and the batch's metadata. The title bar notes it's a Spark Stream Batch view and that it's part of a Spark Streaming application. The KPI's, Duration Processing Delay Scheduling Delay Total Delay Output Operation Input The example below is of the batch in the first line of the table above. The Stream Batch has two calls and the first call has two (2) jobs. Since these jobs are run in parallel, the job with the longest time determines the duration of the batch. The description notes the RDD and the call line; clicking on the description displays the associated code in the program window. Click on the Job ID Spark Job The Input Tab -For an explanation of these tabs see Errors, Log and Conf Errors Logs Conf Right Four Tabs - The program (if uploaded by the user) is shown in this tab. Program - For an explanation of these tabs see Task Attempts, Graphs and Resources Task Attempts Graphs Resource . " }, 
{ "title" : "Events & Insights", 
"url" : "uguide/uguide-events.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Events & Insights", 
"snippet" : "Event Panel & Insights Impala Events...", 
"body" : " Event Panel & Insights Impala Events " }, 
{ "title" : "Event Panel & Insights", 
"url" : "uguide/uguide-events/uguide-events-panels-insights.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Events & Insights \/ Event Panel & Insights", 
"snippet" : "The Unravel intelligence engine helps you manage your applications more efficiently by providing insights into their run. The UI engine gives its insights and tuning suggesting via the Events Panel. Not all UI engine insights results in concrete recommendations, so to take full advantage you must re...", 
"body" : "The Unravel intelligence engine helps you manage your applications more efficiently by providing insights into their run. The UI engine gives its insights and tuning suggesting via the Events Panel. Not all UI engine insights results in concrete recommendations, so to take full advantage you must read the efficiency panel. There is not a 1-1 correspondence between the event and recommendation number. A single event might lead to no or many recommendations. For detail information Impala Insights see here Recommendations Lists the parameters to change, shows their current and recommended value. Efficiency The efficiency list details the inefficiencies. The UI engine then might make a recommendation and may note the expected result from such a change, make a suggestion, or note where to look to increase efficiency Below are two examples. Each type of job and instance of a job has events relevant to that particular job and instance. MapReduce Job Example This MapReduce job is part of a Hive Query. In this example the UI engine lists list four (4) events and has three (3) recommendations. Recommendations Efficiency 1: Used Too Many Reducers Resulted in the one recommendation (#1). Efficiency 2: Reduce Tasks that Start before Map Phase Finishes Resulted in one suggestion . Efficiency 3: Too Many Mappers Resulted in the two recommendations (#2 and #3). Efficiency 4: Large Data Shuffle from Map to Reduce Resulted in a suggestion. Tez DAG Example This Tez DAG job is part of a Hive Query. In this example the UI engine lists list three (3) events and has four (4) recommendations. Recommendations Efficiency 1: Tez DAG Map Vertex used too many tasks Resulted in two suggestions (#3 and #4) and explanation of the problem. Efficiency 2: Tez DAG Resulted in one recommendation (#1). Efficiency 3: hive.exec.parallel is set to false Resulted in one recommendation (#2). " }, 
{ "title" : "Impala Events", 
"url" : "uguide/uguide-events/uuguide-events-impala.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Events & Insights \/ Impala Events", 
"snippet" : "Impala events offers insights in your Impala Query's run. Unravel does not offer configuration recommendations; however, it does offer actions you can take in response to the event. When Unravel has insights, a blue bar under the title bar the event box note the number of events,. Clicking on the ev...", 
"body" : "Impala events offers insights in your Impala Query's run. Unravel does not offer configuration recommendations; however, it does offer actions you can take in response to the event. When Unravel has insights, a blue bar under the title bar the event box note the number of events,. Clicking on the event box brings up the event panel where you can scroll through the multiple insights. Each insight has two sections, the analysis (brown bar on the left), and actions you can take (green bar on the left). When Fragments and Operators are referred to, their ID's are always noted so you can refer back to their views\/tiles in the Impala APM Time Breakdown Analysis The analysis breaks down where the time is spent during the query execution across the following phases: Query planning, i.e., time spent on parsing the SQL and optimizing the query plan, Admission control, i.e., time spent waiting on a queue\/resource pool, Query execution time until the first row becomes available, and Results fetching, which is a combination of row fetching, execution, and idle time. Next, it displays the longest phase and provides additional analysis of where time spent as part of that phase. In the example below, the longest phase is query execution (56.829s). The details on the operator taking the longest time are noted; in this case the Hash Join took 41 out of the 57 seconds. When possible, the event has insights into why this operator took long time to run, and makes tuning suggestions. (See Impala Slow Operator Even Missing Optimizer Statistics Displays a list of tables accessed by the query that had missing statistics and recommends running \"COMPUTE STATS\" to collect statistics for these tables. Underestimated Count of Rows In this case, the query scanned a table for which the optimizer statistics were outdated resulting in the optimizer underestimating the number of rows to be returned. This, potentially, led to a bad execution plan and the query to run slower. Unravel recommends refreshing the statistics by running \"COMPUTE STATS\". Time Skew Analysis This insight detects whether one or more operator instances took substantially longer than other instances and provides a potential root cause for the time skew. It notes whether the time skew is correlated with either: a bottleneck node, e.g., due to slow disk I\/O on that particular node, or data skew, e.g., if the bottleneck node processed much larger volumes of data compared to the rest of the nodes. Slow Operator Analysis This insight indicates that a query operator took significant amount of time to execute. It shows details such as the operator execution time and the time spent in different phases of the operator's execution. Depending on the operator type, this insight provides a root cause analysis that can explain the operator's poor performance. For example, for a SCAN operator the following potential causes of poor performance are examined: Missing or stale statistics on the table the operator scanned, Less efficient storage format of the scanned table, Inefficient partition pruning, and Remote or non-circuit byte reads For a JOIN operator the following issues are examined: Inefficient join algorithm used, e.g., a broadcast join was executed when a partitioned join would be significantly faster, Inefficient order of joined tables in a hash join, e.g., the right hand side table contains more rows than the left hand side, and Missing or stale statistics on one or both joined tables. Based on the issues identified this insight provides corresponding recommendations to improve the performance of the operator. For example, if a partitioned (shuffle) join would be faster, we recommend you use the \/* +SHUFFLE *\/ as a hint in the query SQL text to guide the optimizer in selecting a partitioned join. Non-Columnar Storage Formats This lists the tables that were accessed which use a less efficient storage format, such as TEXT or (non-splittable) GZIP. This insight is generated when scans on these tables take long time to execute. In order to address this, we recommend modifying the storage format to a columnar one such as PARQUET or ORC. Inefficient Partition Pruning This insight shows the tables the query accessed which: have table partitions, and all the partitions were used during the query execution. This could lead to poor query performance for all scan operators accessing the tables, especially when the number of partitions is large. Often older partitions, e.g., partitions containing \"cold\" or outdated data, are not useful. In such cases we recommend rewriting the SQL query by adding a predicate on the partition key, such that the older\/cold partitions will get pruned. " }, 
{ "title" : "Event List", 
"url" : "uguide/uguide-events/uguide-events-list.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Events & Insights \/ Event List", 
"snippet" : "Hive IMPALA MR Spark Tez Workflow Hive Event Category Event Name Parameters Description of When Event Occurs Application failure HiveFailureBrokenPipeEvent N\/A Hive query fails with \"broken pipe\" exception. Application failure HIveFailureIncorrectHeaderEvent N\/A Hive query fails with \"incorrect head...", 
"body" : " Hive IMPALA MR Spark Tez Workflow Hive Event Category Event Name Parameters Description of When Event Occurs Application failure HiveFailureBrokenPipeEvent N\/A Hive query fails with \"broken pipe\" exception. Application failure HIveFailureIncorrectHeaderEvent N\/A Hive query fails with \"incorrect header check\" exception. Application failure HiveFailureReturnCodeEvent N\/A Hive query fails and shows return code. Application failure HiveMapJoinMemoryExhaustionEvent hive.auto.convert.join Hive query fails because it is out of memory in map join, and recommends turning off mapjoin. Application failure HiveOutOfMemoryErrorEvent N\/A Hive query fails because it is out of memory. Informational HiveKillFailEvent N\/A Hive query is killed or failed with lots of wasted work. Informational HiveSuccWithKillFailEvent N\/A Hive query is successful but has lots of killed or failed tasks, resulting in lots of wasted resources. Informational HiveShuffleBytesEvent N\/A Hive query has lots of data shuffle from map to reduce side. Informational HiveTimeBreakdownEvent N\/A Identifies where time is spent on for the query and points out significant events, including MR-level skew events. Speedup HiveExecuteParallelEvent hive.exec.parallel Detects that hive.exec.parallel is set to false and that the jobs in the query could be run in parallel if set to true. Speedup HiveSingleReduceCountDistinct N\/A Hive query has a job with a long single reducer because the query has \"count distinct\". Speedup HiveSingleReduceLongWait N\/A Hive query has a job with a long single reducer spending lots of time on waiting for data to arrive from the map side. Speedup HiveSingleReduceOrderBy N\/A IHive query has a job with a long single reducer because the query has \"order by\". Speedup HiveTooFewReduceEvent hive.exec.reducers.bytes.per.reducer Hive query is using too few reducers. Speedup\/ Resource Utilization HiveTooLargeMapEvent mapreduce.map.memory.mb mapreduce.map.java.opts yarn.scheduler.minimum-allocation-mb Mappers in the Hive query are requesting too much memory. Speedup\/ Resource Utilization HiveTooLargeReduceEvent mapreduce.reduce.memory.mb mapreduce.reduce.java.opts yarn.scheduler.minimum-allocation-mb Reducers in the Hive query are requesting too much memory. Speedup\/ Resource Utilization HiveTooManyMapEvent mapreduce.input.fileinputformat.split.maxsize mapreduce.input.fileinputformat.split.minsize Hive query is using too many mappers. Speedup\/ Resource Utilization HiveTooManyReduceEvent hive.exec.reducers.bytes.per.reducer Hive query is using too many reducers. IMPALA Event Category Event Name Parameters Description of When Event Occurs Application failure ImpalaFailureEvent N\/A Displays an error message obtained from Impala. If the error message is related to memory, this event also does a best-effort analysis and provides a reason for the error (if possible). Informational\/ Resource Utilization ImpalaNoFilterEvent N\/A An Impala query does not contain any filter conditions. Informational\/ Resource Utilization\/ Speedup\/ ImpalaTimeBreakdownEvent N\/A Displays the longest phase in the Impala query. If the longest phase is query execution then it displays the longest operator in the Impala query. If applicable, this event shows insights as to why this operator took the most time, and makes tuning suggestions. Resource Utilization ImpalaTooManyPartitionsEvent N\/A A table has too many (>30K) partitions and recommends using a different partitioning scheme based on frequently used filter conditions. Speedup ImpalaNonColumnarTablesEvent N\/A A table is in non-columnar format and a scan on this table represents significant percentage of the total query execution time. Speedup ImpalaNonPrunedPartitionsEvent N\/A A scan accessed all partitions of a table (i.e. no partition pruning occurred). Speedup ImpalaNonPartitionedTableEvent N\/A A table does not have any partitions and recommends creating partitions on this table based on frequently used filter conditions. Speedup ImpalaSlowAggregateOperatorEvent N\/A An aggregate operator took significant amount of time to execute and shows details such as the operator execution time and the time spent in different phases of the operator's execution. Speedup ImpalaSlowExchangeOperatorEvent N\/A An exchange operator took significant amount of time to execute and shows details such as the operator execution time and the time spent in different phases of the operator's execution. Speedup ImpalaSlowScanOperatorEvent N\/A Ascan operator took significant amount of time to execute and shows details such as the operator execution time and the time spent in different phases of the operator's execution. Additionally, this insight provides a root cause analysis that can explain the operator's poor performance and makes related tuning suggestions. For example if statistics for the scanned table are missing or stale, this insight recommends refreshing the statistics.. Speedup ImpalaSlowJoinOperatorEvent N\/A A join operator took significant amount of time to execute and shows details such as the size of the left and right hand side inputs, the join algorithm and type, and the time spent in different phases of the operator's execution. Additionally, this insight provides a root cause analysis that can explain the operator's poor performance and makes related tuning suggestions. For example if a broadcast join was used but a partitioned (shuffle) join would be faster, this insight recommends using \/* +SHUFFLE *\/ as a hint in the query SQL text to guide the optimizer in selecting a partitioned join instead. Speedup ImpalaSlowSortOperatorEvent N\/A A sort operator took significant amount of time to execute and shows details such as the operator execution time and the time spent in different phases of the operator's execution. Speedup ImpalaTablesMissingStatsEvent N\/A The Impala query accessed tables with missing optimizer statistics and recommends collecting these statistics. Speedup ImpalaTimeSkewEvent N\/A Detects whether one of the operator instances took much longer than the other instances and indicates the bottleneck node. If the skewness is significant enough, this event indicates whether the skewness is correlated with data skew (i.e. the bottleneck node had to process much larger volumes of data) or because of slow disk I\/O on the bottleneck node. Speedup ImpalaTooManyJoinsEvent N\/A An Impala query contains too many joins and recommends denormalizing some tables to reduce the number of joins. Speedup ImpalaUnderestimatedCounfOfRowsEvent N\/A An estimate is outdated and that statistics for the corresponding table should be refreshed. MR Event Category Event Name Parameters Description of When Event Occurs Application Failure MRClassNotFoundEvent N\/A A MR job failed due to a \"class not found\" exception. Application Failure MRFailureCompressLibNotAvailable N\/A A MR job failed due to a \"compression library not available\" exception. Application Failure MRFailureFileNotFoundEvent N\/A A MR job failed due to a \"file not found\" exception. Application Failure MRFailureIllegalArgumentEvent N\/A A MR job failed due to an \"illegal argument\" exception. Application Failure MRFailureNumberFormatEvent N\/A A MR job failed due to a \"number format\" exception. Application Failure MRFailureTimeOutEvent N\/A A MR job failed due to a \"time out\" exception . Application Failure MRGcOverheadLimitExceededMapEvent mapreduce.map.memory.mb mapreduce.map.java.opts yarn.scheduler.minimum-allocation-mb hive.auto.convert.join The MR job failed because the GC overhead limit is exceeded on map side. Application Failure MRGcOverheadLimitExceededReduceEvent mapreduce.reduce.memory.mb mapreduce.reduce.java.opts yarn.scheduler.minimum-allocation-mb hive.auto.convert.join A MR job failed because the GC overhead limit is exceeded on reduce side. Application Failure MRJavaOutOfMemoryMapEvent mapreduce.map.memory.mb mapreduce.map.java.opts yarn.scheduler.minimum-allocation-mb A MR job failed because it is out of memory on map side. Application Failure MRJavaOutOfMemoryReduceEvent mapreduce.reduce.memory.mb mapreduce.reduce.java.opts yarn.scheduler.minimum-allocation-mb A MR job failed because it is out of memory on reduce side. Informational MRKillFailEvent N\/A MR job was killed or failed with lots of wasted work. Informational MRSuccWithKillFailEvent N\/A MR job is successful but has lots of killed or failed tasks, resulting in lots of wasted resources. Informational MRShuffleBytesEvent N\/A MR job has lots of data shuffle from map to reduce side. Informational MRTimeBreakdownEvent N\/A Identifies where time is spent on for the job and points out significant events. Speedup MRLongTasksFromSlowNodeEvent N\/A MR job has long-running map\/reduce tasks from slow nodes. Speedup MRMapSkewDataIOEvent N\/A The map phase of the MR job has a time skew with strong correlation with IO. Speedup MRReduceSkewDataIOEvent N\/A The reduce phase of the MR job has a time skew with strong correlation with IO. Speedup MRTooFewReduceEvent hive.exec.reducers.bytes.per.reducer The MR job is using too few reducers. Speedup\/ Resource Utilization MRTooLargeMapEvent mapreduce.map.memory.mb mapreduce.map.java.opts yarn.scheduler.minimum-allocation-mb Mappers in the MR job are requesting too much memory. Speedup\/ Resource Utilization MRTooLargeReduceEvent mapreduce.reduce.memory.mb mapreduce.reduce.java.opts yarn.scheduler.minimum-allocation-mb Reducers in the MR job are requesting too much memory. Speedup\/ Resource Utilization MRTooManyMapEvent mapreduce.input.fileinputformat.split.maxsize mapreduce.input.fileinputformat.split.minsize The MR job is using too many mappers. Speedup\/ Resource Utilization MRTooManyReduceEvent hive.exec.reducers.bytes.per.reducer The MR job is using too many reducers. Spark Event Category Event Name Parameters Description of When Event Occurs Application Failure DriverOomeEvent spark.driver.memory (As a suggestion) A driver failed with \"OutOfMemory\" error. Application Failure ExecutorOomeEvent spark.executor.memory (As a suggestion) An executor failed with \"OutOfMemory\" error. Application Failure YarnContainerKilledEvent (As a suggestion) spark.driver.memory spark.executor.memory spark.yarn.executor.memoryOverhead There are containers killed by YARN. Resource Utilization ContainerSizingUnderutilizationEvent spark.driver.memory spark.executor.memory spark.executor.cores spark.yarn.executor.memoryOverhead spark.executor.instance spark.default.parallelism spark.sql.shuffle.partitions Container resources are underutilized. Resource Utilization InefficientInputSplitSizeEvent spark.default.parallelism mapreduce.input.fileinputformat.split.minsize An inefficient input split size. Resource Utilization TooFewPartitionsEvent spark.executor.instance spark.default.parallelism spark.sql.shuffle.partitions Indicates that there are too few partitions with respect to available parallelism. Resource Utilization UnderutilizedCpuEvent spark.executor.cores (As a suggestion) There is low utilization of CPU resources. Resource Utilization UnderutilizedNodeMemoryEvent NA There is low utilization of memory resources. Resource Utilization UnderutilizedStorageMemoryEvent NA The Spark storage memory has low utilization. More RDDs can be cached in memory. Speedup CachingOpportunityEvent NA There is an (unused) opportunity for RDD caching. Speedup ContendedCpuEvent NA There is contention for CPU resources. Speedup ExcessiveGcEvent spark.executor.memory (As a suggestion) There is high garbage collection overhead. Speedup ExecutorImbalanceEvent NA There is load imbalance among executors. Speedup ExhaustedStorageEvent NA The Spark storage memory is getting exhausted. Speedup LightExecutorEvent NA There is large idle time for one or several executors. Speedup LongStageEvent NA There is load imbalance among tasks for the longest stage of the application. Speedup ContendedDriverEvent NA The driver is a bottleneck in the application. It monitors data transfers to the driver (i.e., the time spent in fetching the data from executors). Tez In addition to TEZ Events Hive-On-Tez APM supports all failure events received from Unravel hive hook Event Category Event Name Parameters Description of When Event Occurs Application Failure TezIllegalArgumentEvent (As a suggestion) tez.runtime.io.sort.mb hive.tez.java.opts Application failed because tez.runtime.io.sort.mb less was set outside the allowed memory limit. Application Failure TezBlockMissingEvent N\/A Indicates that an HDFS disk is missing or corrupted Application Failure TezOutOfMemoryErrorEvent N\/A Application ran out of memory. Application Failure TezUncheckedExceptionEvent N\/A Application failed with some internal error. Resource Utilization\/ Inefficiency TezNoDAGEvent (As a suggestion) tez.am.container.idle.release-timeout-min.millis tez.am.container.idle.release-timeout-max.millis Session was created but no DAG was not submitted. Speedup HiveExecuteParallelEvent hive.exec.parallel Detects that hive.exec.parallel is set to false and that stages in the query could be run in parallel if set to true. Speedup TezNoColStatsEvent N\/A The Hive query accessed tables with missing column statistics and recommends collecting these statistics. Speedup\/ Resource Utilization TezMapVertexTooFewTaskEvent tez.grouping.min-size tez.grouping.max-size TDAG is using too few map tasks. Speedup\/ Resource Utilization TezMapVertexTooManyTaskEvent tez.grouping.min-size tez.grouping.max-size DAG is using too many map tasks. Speedup\/ Resource Utilization TezReduceVertexTooFewTaskEvent hive.exec.reducers.bytes.per.reducer hive.tez.min.partition.factor hive.tez.max.partition.factor DAG is using too few reducer tasks. Speedup\/ Resource Utilization TezReduceVertexTooManyTaskEvent hive.exec.reducers.bytes.per.reducer hive.tez.min.partition.factor hive.tez.max.partition.factor DAG is using too many reducer tasks. Speedup\/ Resource Utilization TezAutoParallelismEvent hive.tez.auto.reducer.parallelism Detects that auto reducer parallelism is not enabled and recommends setting hive.tez.auto.reducer.parallelism to true Speedup\/ Resource Utilization TezJobReducesEvent (As a suggestion) mapreduce.job.reduces mapred.reduce.tasks Detects that a fixed number of tasks are generated for every reducer because mapreduce.job.reduces or mapred.reduce.tasks are manually set, and recommends removing these properties. Speedup\/ Resource Utilization TezReduceVertexTasksReducedEvent hive.exec.reducers.bytes.per.reducer hive.tez.min.partition.factor hive.tez.max.partition.factor A reducer vertex was initiated with too many tasks, and that after sampling the parallelism was auto-reduced. Recommends increasing the value of hive.exec.reducers.bytes.per.reducer. Informational TezHeavyWeightVerticesEvent N\/A The execution of a Tez DAG was dominated by a few vertices. Informational TezVertexTaskTimeSkewEvent N\/A A large variation in task run times occurred for some vertices during the execution of a Tez DAG. Informational TezBreakdownEvent hive.exec.parallel hive.tez.auto.reducer.parallelism Provides a summary of the DAGs that were run inside a Tez session, along with their status and recommendations. Workflow Catogory Name Parameters Description of When Even Occurs Informational WorkflowTimeBreakdownEvent N\/A Identifies the top 3 components that consume the most time along the critical path. If there are fewer than 3 components, this event is not triggered. Directs users to check out the critical path information. Informational\/ Application Failure WorkflowGeneralFailureEvent N\/A For Oozie workflows, this event displays error messages extracted from the Oozie log. For tagged workflows, this event simply indicates that the workflow has failed. Informational\/ Resource Utilization WorkflowIRSummaryEvent N\/A Displays the top 3 components of the workflow and its subworkflows, to show what can be tuned to save the most resources. Informational\/ SLA analysis WorkflowDurationAnomalousEvent com.unraveldata.analytics.max.past.samples com.unraveldata.analytics.min.past.samples com.unraveldata.analytics.anomaly.threshold If duration of a workflow instance is anomalous with respect to its past runs, then this event is generated. Informational\/ Speedup WorkflowIASummaryEvent N\/A Displays the top 3 components of the workflow and its subworkflows, to show what can be tuned to save the most running time. " }, 
{ "title" : "Auto Actions", 
"url" : "uguide/uguide-aa.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Auto Actions", 
"snippet" : "Auto Actions Overview Creating Auto Actions Expert Rule Same Logical Operator Snooze Feature Sample Auto Actions Running Auto Action Demos Supported cluster metrics...", 
"body" : " Auto Actions Overview Creating Auto Actions Expert Rule Same Logical Operator Snooze Feature Sample Auto Actions Running Auto Action Demos Supported cluster metrics " }, 
{ "title" : "Auto Actions Overview", 
"url" : "uguide/uguide-aa/uguide-auto-actions-overview.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Auto Actions \/ Auto Actions Overview", 
"snippet" : "Unravel's auto actions automates the monitoring of your compute cluster by allowing you to define complex actionable rules on different cluster metrics. You can use an auto action to alert you to a situation needing manual intervention, e.g., resource contention or stuck jobs. Additionally, it can b...", 
"body" : "Unravel's auto actions automates the monitoring of your compute cluster by allowing you to define complex actionable rules on different cluster metrics. You can use an auto action to alert you to a situation needing manual intervention, e.g., resource contention or stuck jobs. Additionally, it can be set to automatically kill an application or move it to a different queue. The Unravel Server processes auto actions by: Collecting various metrics from the cluster(s), Aggregating the collected metrics according to user-defined rules, Detecting rule violations, and Applying defined actions for each rule violation. Each rule consists of: A logical expression that is used to aggregate cluster metrics and to evaluate the rule. A rule has two conditions: : The conditions which cause a violation, e.g., the number of jobs running, memory used. Prerequisite conditions : Who\/what\/when can cause the violation, e.g., user, applications Defining conditions for Unravel Server to execute whenever it detects a rule violation. Actions Manage | Auto Actions The auto actions tab provides a quick way to view auto actions and quickly see their status, along with its defined actions and scope. The tab displays all defined auto actions separated into an Active and Inactive list. You enable\/disable by clicking the check box on the left. You can edit ( define new auto actions Hovering over the auto action's name gives you the description which was entered when defining the auto action. Hovering over action or scope glyph brings up its detail. For example, for the active auto action above: rule description: email action: an email is sent to only one (1) person, queue scope: is three queues: The Actions Scope quicktest must Expert Rule quicktest History of Runs By default all actions are off. Possible actions are: Send an Email ( Kill the App ( Move the app to another queue ( Send a Http post ( By default the various scopes apply to all, i.e., all applications and constantly on. The scopes are: User ( Queue ( Cluster ( Application ( Time ( Sustained Violation: This is not shown in the auto actions list. If you have not defined a particular action or scope, i.e., it's using the default, the glyph is grey ( Click on the History of Runs Click on a run's Link Operations Usage Details | Infrastructure | Applications Applications Notifications 'Snoozing' Auto Actions The snooze function prevents automatic actions from repeating during a specified period of time, if and only if it is the same violation context and the action adds no further information to the violation, i.e., the new violation is essentially noise. See Snooze Feature Property\/Definition Default com.unraveldata.auto.action.default.snooze.period.ms The time repeated violations are be ignored for the violator, i.e., app, user. If the violation is still occurring when awakened the Auto Action executes the action(s) and the violator is once again snoozed.An auto action containing a kill or move action is never snoozed. 0: snooze is turned off &amp;gt; 0: snooze is on, there is no upper bound 3,600,000 (1 hour) When you change the snooze time period all applications currently snoozed are reset. Upon next violation the application is \"snoozed\" using new snooze value. To change the snooze time On Unravel Server open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.auto.action.snooze.period.ms com.unraveldata.auto.action.snooze.period.ms=7200000 Restart the JCS2 daemon. # service unravel_jcs2 restart " }, 
{ "title" : "Limitations", 
"url" : "uguide/uguide-aa/uguide-auto-actions-overview.html#UUID-ea3f812a-70bb-cd8f-b19c-ba0e1d73e2f5_N1552724699500", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Auto Actions \/ Auto Actions Overview \/ Limitations", 
"snippet" : "Alerting on Running Apps Applications of the following types do not provide any means for real-time alerts, i.e. when in the running state. Once the application has finished the alerts are generated notifying users about policy violations that already occurred. Hive Impala Kill and Move to Queue not...", 
"body" : " Alerting on Running Apps Applications of the following types do not provide any means for real-time alerts, i.e. when in the running state. Once the application has finished the alerts are generated notifying users about policy violations that already occurred. Hive Impala Kill and Move to Queue not Supported The following applications types do not support Kill Move to Queue Hive Impala Workflow Running Duration versus Final Duration Inconsistency Unravel calculates and publishes internally the current duration for applications of the following types in real-time, i.e., when in the running state. Upon the application completion Unravel receives the actual end time and performs the final duration calculation. This can lead to an inconsistency where the duration aggregated and published during the running state is greater than the duration published upon the application's completion. Workflow Missing Auto Action Violation Badge The badge is not displayed for following application types. For example, in the cluster view above Hive Workflow \"Cloud\" Type Setups are not Supported Currently, auto actions are not triggered for applications on a “cloud” type setup (EMR, HDI, Qubole, etc.) of Unravel; therefore the Kill Move to Queue etl Auto Actions' Properties See here " }, 
{ "title" : "Creating Auto Actions", 
"url" : "uguide/uguide-aa/uguide-auto-actions-create.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Auto Actions \/ Creating Auto Actions", 
"snippet" : "See here 1. Select Manage | Auto Actions. The tab displays all defined auto actions separated into an Active and Inactive list. You can enable\/disable an auto action by clicking the check box on the right side of its row. You can edit or delete an auto action regardless of its status by clicking on ...", 
"body" : " See here 1. Select Manage | Auto Actions. The tab displays all defined auto actions separated into an Active and Inactive list. You can enable\/disable an auto action by clicking the check box on the right side of its row. You can edit or delete an auto action regardless of its status by clicking on edit ( To create a new auto action, click either Create from Template, Build Rule, Expert Rule - provides a partially completed template designed for the task. The only ruleset options available are relevant to the task being defined. Fields which you need to fill in are highlighted. Create from Template - provides an empty template, with all the options available. Build Rule - provides only a text box for defining your auto action using JSON. Expert Rule 2. Using the Create from Template or Build Rule. Whether using Create from Template Build Rule The sections are: Name and Description The name is mandatory and is used by the UI for all auto actions' displays; we recommend using a name which reflects the auto action's purpose.or Create from Template Build Rule Expert Rule The description is optional, but we recommend completing it with a succinct description of the action. When users hover over the action's name the description is displayed. This example is from Create from Template Build Rule Ruleset At least one rule type ( User Queue, Apps Expert Rule metric type state Expert Rule Build Rule defines a rule \"metric\" \"comparision operator\" \"value\". Metric See Supported Cluster Metrics metrics The comparison value: any valid numeric value. The default value is 0; were you to leave it the auto action would constantly trigger. The Type mapreduce, yarn, tez, spark, impala, workflow and hive The State new, new_saving, submitted, accepted, scheduled, allocated, allocatedSaving, launched, running, finishing, finished, killed, failed, undefined, newAny, allocatedAny, pending, and * (all). Multiple rule types are evaluated in conjunction with each other using: Or, And Same and Or work And Same Logical Operator Same Using Create From Template This template has the Ruleset metric comparison type state Metric Type or State metric Same'd Same Logical Operator Using Build Rule The Ruleset User Queue, Cluster App Expert Rule Add Queue metric type state Expert Rule must In the example below, Metric Type Queue metric comparison operator type state above , Apps , Same Or And Same Logical Operator Same Or And . Click Close trash Using Expert Rule You simply have an empty text box. Options Define the scope ( User Queue Cluster Application Name Time Sustained Selected options default to All, Time Sustained Violation When using Create from Template the box next to the option name to select it. Check You can narrow the scope of User Queue Cluster Application Name Only Except Only rule Except all but those specified Transform Application Name t Except Add Application except Create from Template All The Time start end click specifies a length of time violation must occur before the auto action is triggered. This allows time for specified by the option to \"self correct\" and lowers the number of false positives The default is zero, i.e., all auto actions are immediately triggered upon violation and the specified action is carried out. You can select Sustained violation minimum maximum sustained mode triggers the action(s) only if this violation was continuously detected for at least the specified period of time. This allows you to suppress triggering of violation actions for “on-offs” and metric spikes. These are normal in multi-tenant cluster environments can retun back to normal operation on their own. If a violation stops before the minimum time period, the clock is reset for that application. For instance, if the minimum time is one (1) hour and the application violates the auto action for 58 minutes and then returns to normal – no action is taken and the time period for that application resets to 0. Minimum sustained mode triggers the action(s) only if this violation is continuously detected for less than the specified period of time. This suppresses the triggering of violations for long running applications and triggers on auto action rule scope on ad-hoc short lived user applications. Maximum Actions Defines the actions to take when the auto action is triggered. and Build Rule Create from Template, Send an email Http Post Post to Slack Move App to Queue Kill App Move Kill App Build Rule Expert Action. Expert Mode Auto Actions and Pagerduty can not You can chose one of more actions. Check the text box to chose that action. If you chose no actions, the UI simply records the violation and saves the data for the cluster view. For Send Email Add Recipient Include Owner For HTTP post Add URL uses the official Slack API available at Post to Slack https:\/\/slack.com\/api\/chat.postMessage public Slack channel, private channel, or direct message\/IM channel, It provides a better integration with Slack Service and allows you to send a direct message to the owner of Hadoop job's violating the Auto Action. You must generate the token via a Slack service website. For more information on generating the token see: - generic user icon and \"bot\" username test tokens - generic bot icon, with generic \"bot\" username custom bot user token with Slack App user token chat:write:bot - inherits Slack App's icon, with generic \"bot\" username Slack App bot user token or Move app to queue Kill App. The Move App Kill App Kill App Move App Have directly caused the rule violation, and Have allocated resources, i.e. in allocated or running states. is a non-destructive action that should not affect the cluster performance and its availability to the user; however we suggest using it with caution. Move App is a destructive action. It can affect the cluster performance and its availability to the users. This option is primarily to kill rogue applications that are causing contention of a cluster resources. Kill App The Build Rule Expert Rules 3. Create an auto action. Using Create From Template Click on Create a Template above Resource contention - monitors overall memory or vcore usage on a cluster and number of pending\/running jobs in order to detect when the cluster is struggling to accommodate all submitted jobs and falls into a resource contention pitfall. Once detected the specified auto action(s) are taken. Resource contention in cluster - monitors overall memory or vcore usage in a queue and number of pending\/running jobs in order to detect a state when the queue is struggling to accommodate all submitted jobs and falls into a resource contention pitfall. Once detected the specified auto action(s) are taken. Resource contention in queue Rogue Identification - identifies so-called “rogue” users on a cluster who can potentially affect other users and the cluster as a whole, i.e. users who are submitting jobs that are using too much of the cluster resources (memory or vcores). Once the rogue user is detected the specified auto action(s) are taken. Rogue user - identifies so-called “rogue” applications on a cluster which can potentially affect other applications and the cluster as a whole, i.e. applications that are using too much of the cluster resources (memory or vcores). Once the rogue application the specified auto action(s) are taken. Rogue application - identifies so-called “rogue” impala queries on a cluster which can potentially affect other applications and the cluster as a whole, i.e. queries that are using too much of the cluster resources (memory or vcores). Once the rogue application the specified auto action(s) are taken. Rogue Impala query (HDFS Read\/Write) Long Running Jobs - monitors elapsed time of a running YARN application (i.e., MapReduce, Spark, Hive, etc) and executes the action(s) if the job runs longer than desired. Long running YARN application - tracks Hive jobs by monitoring the total elapsed time of a running Hive query and executes the action(s) if the job runs longer than desired. Long running Hive query - tracks workflow jobs by monitoring total elapsed time of a running workflow and executes the action(s) if the job runs longer than desired. Long running workflow - tracks impala jobs by monitoring total elapsed time of a running workflow and executes the action(s) if the job runs longer than desired. Long Impala query Using Build Rule Click on Build Rule above Using Expert Rule The Auto Actions engine is capable of much more than is available through the templates. Expert Rule is a very powerful mode giving you total access to all of the Auto Actions engine capabilities. You can create complex rulesets to accommodate almost any cluster monitoring requirements. JSON is used to define the rules and actions. Before using this mode, you should have clear understanding of auto actions concepts. See the Expert Rule Sample Auto Actions 4. Click on Save Auto Action Your auto action is now listed in the Manage Auto " }, 
{ "title" : "Expert Rule", 
"url" : "uguide/uguide-aa/uguide-auto-actions-expert.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Auto Actions \/ Expert Rule", 
"snippet" : "Overview Expert Rule is a very powerful mode of operation which provides you with greater flexibility than is available through the templates. Using the Expert Rule you can create complex rulesets to accommodate almost any cluster monitoring requirements. Before using this mode, you should have clea...", 
"body" : " Overview Expert Rule is a very powerful mode of operation which provides you with greater flexibility than is available through the templates. Using the Expert Rule you can create complex rulesets to accommodate almost any cluster monitoring requirements. Before using this mode, you should have clear understanding of auto actions concepts and capabilities, along with JSON, which is used to define the auto action. This mode's flexibility and power makes it dangerous and capable of wreaking havoc. Consult with the Unravel team before attempting to use the Expert Rule. Before using the Expert Rule, look at Build Rule See here In this mode you must specify : boolean conditions that must be met for the Unravel Server to evaluate the Auto Action's defining conditions, e.g., Auto Action should run from 8.00 and 14.00. Prerequisite condition s : boolean conditions that must be met for the Unravel Server to execute the corresponding action, e.g., the application can't run more than 50 mapper tasks. Defining conditions : steps to be taken when the prerequisite and defining conditions evaluate to true, e.g., send an email to admin. Actions When using Expert Rule must rules actions {\n \/\/ header is required\n 'HEADER' \n\n \/\/ Rules - at least one must be defined. Two or must be joined using an operator.\n \"rules\":[\n { scope } | \"operator\" [ { scope } { scope } ... \n ]\n\n \/\/ Prerequisite Conditions - at least one\n 'OPTIONS - POLICY\/SCOPE'\n \n \/\/ Actions - at least one\n \"actions\":[\n { action } \n ]\n} basic auto action information, name, etc., including status (in\/active). Header: : the rules for the scope. You must define at least one rule. Rules who, what, where causes a violation and when. You must specify at least one. Policy\/Scope: Options - actions to perform when a violation triggers the auto action. If none are defined the UI still implements and tracks auto actions. Actions: Defining Your Auto Action Header You must define a header. The only item not required is the Description. Attributes Name Definition Possible Value Default Value enabled enabled Whether the auto action is active or not. True: active\/enabled. False: inactive\/disabled True | False - policy_name Value defined by Unravel. AutoActions2 AutoActions2 policy_id Value defined by Unravel. 10 10 instance_id Any unique value - name_by_user Any unique string. The name is used when the auto action is displayed in the UI. - description_by_user Description of the auto action. - created_by Value defined by Unravel. admin admin last_edited_by Value defined by Unravel. admin admin created_at Time created. Date and time is in the form of a Epoch\/Unix timestamp. - updated_at Time updated. Date and time is in the form of a Epoch\/Unix timestamp. - \"enabled\": true,\n\"policy_name\": \"AutoActions2\",\n\"policy_id\": 10,\n\"instance_id\": 273132543512,\n\"name_by_user\": \"aa_Sample_Test\",\n\"description_by_user\": \"long running workflow\",\n\"created_by\": \"admin\",\n\"last_edited_by\": \"admin\",\n\"created_at\": 1524220191137,\n\"updated_at\": 1524220265920, Rules: Defining conditions Field Name Definition Possible Values Required\/ Required by Default Value scope The rule scope. app, apps, multi_app, by_name, cluster, clusters, multi_cluster, container ,containers, multi_ containers queue, queues, multi_queue, user, users, multi_user : apps==multi_app, users==multi-user, etc Note √ - target Application name any valid application name when scope is by_name - metric Metric used for comparison. see supported metrics per type - comparison Comparison operator >, >=, ==, <=, < metric - value Value for comparison. The value form varies by metric. number metric - state Scope state new, new_saving, submitted, accepted, scheduled, allocated, allocated_saving, launched, running, finishing, finished, killed, failed, and * - type Job type mapreduce, yarn, tez, spark, workflow, hive - Logical Operators for Evaluating Multiple Rules Operator Condition for a Violation OR At least one rule evaluates to true. AND All rules evaluate to true. SAME All the rules evaluate to true and See Same Logical Operator You must define at least one rule. A Single Rule \"rules\": [\n \/\/ rule\n {\n \"scope\":\"\",\n \/\/ at least one of the following\n \/\/metric\n \"metric\":\"\",\n \"compare\":\"\",\n \"value\":,\n \"state\":\"\",\n \"type\":\"\"\n }\n] Violation occurs when the application is a pending workflow with a duration > 10. \"rules\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"duration\",\n \"compare\":\">\",\n \"value\":10,\n \"state\":\"pending\",\n \"type\":\"workflow\"\n }\n] Violation occurs when the application is a workflow with a duration > 10. Removing state \"rules\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"duration\",\n \"compare\":\">\",\n \"value\":10,\n \"state\":\"\",\n \"type\":\"workflow\"\n }\n] Violation occurs when the application has a duration > 10. Removing state type \"rules\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"duration\",\n \"compare\":\">\",\n \"value\":10,\n \"state\":\"\",\n \"type\":\"\"\n }\n] A Rule Array Two or more rules combined with an operator. \"rules\": [\n {\n \"operator\": [\n \/\/ rule 1\n {\n\n }\n \/\/ rule 2\n {\n }\n \/\/ rule n\n {\n }\n ]\n }\n]\n\n Note is equivalent to the plural of Multi_X X. Take the following two rules: \/\/ apps (allocatedMB >=1024)\n{\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n}\n\n\/\/ apps (allocatedVCores > 100)\n{\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n}\n OR Example When they are OR'ed a violation occurs if at least one rule evaluates to true. \"rules\":[\n {\n \"OR\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n] AND Example When AND'ed a violation occurs if both rules evaluate to true. \"rules\":[\n {\n \"AND\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n] SAME Example When SAME'd a violation occurs if both rules evaluate to true and the violations are within the same scope. \"rules\":[\n {\n \"SAME\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n] Using the above example, if My_App only violates rule 1 (allocatedMB), and Your_App only violates rule 2 (allocatedVcores) the auto action is not triggered because the violations occurred in different scopes, i.e., My_App and Your_App. However if My_App violates both rules (allocatedMB andallocatedVcores), and Your_App only violates rule 2 (allocatedVcores) the auto action is triggered for My_App but not Your_App. Given the same ruleset, evaluation becomes more restrictive. : the auto action is triggered if one or more of conditions is true. OR : the auto action is triggered if all of conditions are true. AND : the auto action is triggered if all of the conditions are true SAME within Options - Policy\/Scope: Prerequiste conditions Who\/what can cause the violation and when. You must define at least one option - policy\/scope. Field Name\/Definition Required\/ Required by Possible Values Default Value {X}_mode where {X} The mode defines how the rules are applied to type {X} 0 - the rules aren't evaluated. 1 - the rules are evaluated for all type {X} 2 - the rules are evaluated for only those in {X} {X {X} {X} You must define at least one option\/policy. 0, 1, 2, 3 Default: 0 0 {X}_list A list of {X} type if mode is set to 2 (only) or 3 (except). Applicable Only if {X} {X}_transform empty, single item or comma separated list. - {X}_transform A list of regex used to generate a list of {X} if mode is set to 2 (only) or 3 (except). Applicable Only if {X} {X} empty, single regex or comma separated regex list - Time The daily time the Auto Action is trigger. any time period spanning less than 24 hours. - Sustained Violation Set a minimum or maximum time period for the auto action to be triggered. See here any time period less than 24 hours. - Options - Policy\/Scope Rule where X \"X_mode\": \"\",\n\n\/\/ at least one of the following if X_mode = 2|3\n\"X_list\": \"\" ,\n\"X_mode\": \"\" , - does not apply to any clusters. Cluster \"cluster_mode\": 0,\n\"cluster_list\":\"\",\n\"cluster_transform\":\"\", - applies all queues. Queue \"queue_mode\": 1,\n\"queue_list\":\"\",\n\"queue_transform\":\"\", - applies User only \"user_mode\": 2,\n\"user_list\": [userA, userB],\n\"user_transform\":\"\", - applies to all applications Application Name except \"app_mode\": 3,\n\"app_list\": [userA, userB],\n\"app_transform\":\"\", - applies User only \"user_mode\": 2,\n\"user_list\": [userA, userB],\n\"user_transform\":\"regex\", Actions: action(s) to implement upon violation You do not have to define any actions, but it defeats to purpose not to. If no actions are defined, the UI keeps track of the auto action and when triggered, who triggers it. Both the prerequisite defining Field Name\/Definition Required\/ Required by Possible Value Default Value action The action to be taken. at least one send_email, http_post, post_in_slack, move_to_queue, kill_app - to Email recipients. send_mail if to_owner One or more recipients in a comma delimited list. - Send email to owner to_owner send_mail if to false: do not send email true: send email false urls URLs for Http post http_post One or more URLs in a comma delimited list. - token Token generated by slack post_in_slack Slack token - channels Slack channel. post_in_slack One or more channels in a comma delimited list - queue Queue name. move_to_queue The name of a valid queue to move the app to. - Single Action \"actions\": [\n {\n \"action\": \"\"\n \/\/ if required action options\n }\n] Multiple Actions \"actions\": [\n \/\/ action 1 \n {\n }\n\n \/\/ action n\n {\n }\n] Actions can be Ignored When in Conflict Below we specified two actions, move_to_queue kill_app kill_app move_to_queue actions\":[\n {\n {\n \"action\":\"move_to_queue\",\n \"queue\":\"sample\"\n }, \n \"action\":\"kill_app\"\n }\n }\n] Action(s) Fail if the Required Information is Invalid or not Specified. Below are two actions with invalid information. In send_mail http_post Operations Dashboard , history of runs cluster view \"actions\":[\n {\n \"action\":\"send_email\",\n \"to\": [aBadEmailAddress.mycompany.com,anotherBadAddress.mycompany.com\n ],\n \"to_owner\":false\n },\n {\n \"action\":\"http_post\",\n \"urls\":[https:\/\/nonexistentURL\n ]\n }\n] Example Actions There are five (5) main actions : send_email http_post post_in_slac , move_to_queue kill_app and send_email, http_post, post_in_slack Creating Auto Actions send_email http_post post_in_slack Note You must take care when entering information. A specified action fails if you enter the incorrect information, i.e., bad email address\/URL\/channel, wrong or non-existent queue. Send_email Unlike when using Create from Template Build Rule to_owner \"actions\":[\n {\n \"action\":\"send_email\",\n \"to\": [\"myMail@mycompany.com,ThisPerson@theircompany.com,TheBoss@mycompany.com\"\n ],\n \"to_owner\":false\n }\n] http_post Just like send_email \"actions\": [\n {\n \"action\": \"http_post,\n \"to\": [\"https:\/\/test24:3000\/post\/\"\n ]\n }\n] post_in_slack Verify that your token is correct, and the channels are entered correctly. You can enter multiple channels using a comma delimited list. \"actions\": [\n {\n \"action\": \"post_in_slack\",\n \"token\": \"xyz\",\n \"channels\": [ \"auto-action-2\"\n ]\n }\n] >move_to_queue Be sure to enter an existing and correct queue. This is non-destructive but none-the-less may affect the cluster performance and its availability to the users. \"actions\": [\n {\n \"action\": \"move_to_queue\",\n \"queue\": \"sample\"\n }\n] kill_app This is straight forward, but kill_app is a destructive action \"actions\": [\n {\n \"action\": \"kill_app\"\n }\n] An Expert Rule Example This auto action triggers on applications using (memoryMB >= 1024), has (allocatedVcores >100), and which occur within the same scope, except for the applications, myApp, yourApp, and theirApp. Upon triggering a notification is posted to a Slack channel and the application is moved to the slow_queue. {\n \/\/ Header\n \"enabled\":true,\n \"policy_name\":\"AutoActions2\",\n \"policy_id\":10,\n \"instance_id\":273132543512,\n \"name_by_user\":\"aa_Sample_Test\",\n \"description_by_user\":\"long running workflow\",\n \"created_by\":\"admin\",\n \"last_edited_by\":\"admin\",\n \"created_at\":1524220191137,\n \"updated_at\":1524220265920,\n\n \/\/ Defining Conditions \n \"rules\":[\n {\n \"SAME\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n ] \n \/\/ Prerequisite Conditions\n \"app_mode\":3,\n \"app_list\":\"myApp, yourApp, theirApp\",\n\n \/\/ Actions\n \"actions\":[\n {\n \"action\":\"post_in_slack\",\n \"token\":\"xyz\",\n \"channels\":[\n \"auto-action-2\"\n ]\n },\n {\n \"action\":\"move_to_queue\",\n \"queue\":\"slow_queue\"\n }\n ]\n} Auto Actions Examples See Sample Auto Actions Build Rule " }, 
{ "title" : "Same Logical Operator", 
"url" : "uguide/uguide-aa/uguide-auto-actions-same.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Auto Actions \/ Same Logical Operator", 
"snippet" : ": logically SAME and Example - A Rule Designed to Alert on Rogue Users. Human-readable form If any user is running more than 10 jobs on a cluster and the same More formally (any user has > 10 running apps) SAME JSON definition “rules”:[ “SAME”:[ { “scope”:”users”, “metric”:”appCount”, “operator”:”>”...", 
"body" : ": logically SAME and Example - A Rule Designed to Alert on Rogue Users. Human-readable form If any user is running more than 10 jobs on a cluster and the same More formally (any user has > 10 running apps) SAME JSON definition “rules”:[\n “SAME”:[\n {\n “scope”:”users”,\n “metric”:”appCount”,\n “operator”:”>”,\n “value”:10,\n state”:”running”\n },\n {\n “scope”:”users”,\n “metric”:”appCount”,\n “operator”:”>”,\n “value”:5,\n “state”:”pending”\n }\n ]\n] Implementation Internally the back-end uses a clustering technique to implement the SAME Assume the above rule, three users (A, B, and C), and the following conditions user A has 12 running and 3 pending apps user B has 7 running and 1 pending apps user C has 21 running and 11 pending apps First, the two simple rules are evaluated: does user have more than 10 apps running? User A has 12 → TRUE User B has 7 → FALSE User C has 21 → TRUE does user have more than 5 apps pending? User A has 3 → FALSE User B has 1 → FALSE User C has 11 → TRUE Second, it applies clustering by scope and for each cluster it counts the number rules triggered. In the back-end code this procedure is called “linking” of rules (see Ruleset.java). Cluster “User A”, link count = 1. User A > 10 running apps? → TRUE User A > 5 pending apps? → FALSE Cluster “User B”, link count = 0. User B > 10 running apps? → FALSE User B > 5 pending apps? → FALSE Cluster “User C”, link count = 2. User C > 10 running apps? → TRUE User C > 5 pending apps? → TRUE Third, all groups with less than the needed number of links (2 in this case) are discarded. If some of the rules were triggered, that rule reset for the group. Cluster “User A” has a link count = 1 so it's reset and discarded. User A > 10 running apps? → TRUE   User A > 5 pending apps? → FALSE Cluster “User B”, link count = 0 so it's discarded. User B > 10 running apps? → FALSE User B > 5 pending apps? → FALSE Finally, only the users that have triggered all rules remain. Cluster “User C”, link count = 2: User C > 10 running apps? → TRUE User C > 5 pending apps? → TRUE User C meets the criteria for the Rogue User Auto Action, therefore User C triggers the Auto Action and the alert is sent and\/or the actions performed. Comparison to AND Both User A and User C would have triggered the above rule were AND SAME any AND any To achieve the same result as the above example using AND SAME each and every user ( Username AND Username " }, 
{ "title" : "Snooze Feature", 
"url" : "uguide/uguide-aa/uguide-auto-actions-snooze.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Auto Actions \/ Snooze Feature", 
"snippet" : "The snooze function prevents automatic actions from being repeated during a specified period of time, if and only if, it is the same violation context and the action adds no further information to the violation, i.e., essentially noise. For example, alerts of user A violating rule Y are snoozed App ...", 
"body" : "The snooze function prevents automatic actions from being repeated during a specified period of time, if and only if, it is the same violation context and the action adds no further information to the violation, i.e., essentially noise. For example, alerts of user A violating rule Y are snoozed App An auto action specifying a Kill App or Move App action can not be snoozed. Snooze is set the first time the app violates the rule. The auto action itself continues to run uninterrupted whether zero (0) or all apps currently covered by the auto action are snoozed. The auto action takes action for any app not snoozing If an app is still violating upon awaking, snoozed Example : if app uses memory > 1 GB send email Rule\/Action : A & B Two apps : 30 minutes Snooze time at 20:00 A > 1GB → email is sent + snooze set (snoozed until 20:30). B < 1GB → application is not violating so nothing is done. at 20:10 A > 1GB → snoozing B > 1GB → email is sent + snooze set (snoozed until 20:40). at 20:20 A > 1GB → snoozing B > 1GB → snoozing at 20:30 A > 1GB → application wakes >B > 1GB → snoozing at 20:40 A > 1GB → snoozing, >B < 1GB → app wakes To change the snooze time On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.auto.action.snooze.period.ms. com.unraveldata.auto.action.default.snooze.period.ms=7200000 Restart the JCS2 daemon. # service unravel_jcs2 restart " }, 
{ "title" : "Example Auto Actions", 
"url" : "uguide/uguide-aa/uguide-auto-actions-examples.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Auto Actions \/ Example Auto Actions", 
"snippet" : "Limitations on auto actions can be found here here Sample JSON rules Unless otherwise noted, all JSON rules are entered into the Rule Box in the Expert Mode Alert Examples Alert if Hive query duration > 10 minutes { \"scope\": \"multi_app\", \"user_metric\": \"duration\", \"type\": \"HIVE\", \"state\": \"RUNNING\",...", 
"body" : " Limitations on auto actions can be found here here Sample JSON rules Unless otherwise noted, all JSON rules are entered into the Rule Box in the Expert Mode Alert Examples Alert if Hive query duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"user_metric\": \"duration\",\n \"type\": \"HIVE\",\n \"state\": \"RUNNING\",\n \"compare\": \">\",\n \"value\": 600000\n} Alert if Tez query duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"user_metric\": \"duration\",\n \"type\": \"TEZ\",\n \"state\": \"RUNNING\",\n \"compare\": \">\",\n \"value\": 600000\n} Alert if any workflow's duration > 20 minutes {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"state\": \"RUNNING\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n} Alert if workflow named “foo” and duration > 10 minutes {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"state\":\"RUNNING\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":600000\n} Alert if workflow named “foo” and totalDfsBytesRead > 100 MB and duration > 20 minutes {\n \"AND\":[\n {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":1200000\n },\n {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\">\",\n \"value\":104857600\n }\n ]\n}\n Alert if Hive query in Queue “foo” and duration > 10 minutes {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"state\": \"RUNNING\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 600000\n}\n And select global rule condition Queue only “foo”: Kill App Example When workflow name is “prod_ml_model” and duration > 2h then kill jobs with allocated_vcores >= 20 and queue != ‘sla_queue’ In Rule Box enter: {\n \"scope\": \"by_name\",\n \"target\": \"prod_ml_model\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 7200000\n} In Action Box enter: {\n \"action\": \"kill_app\",\n \"max_vcores\": 20,\n \"not_in_queues\": [\"sla_queue\"],\n \"if_triggered\": false\n} Auto Actions Rules, Predefined Templates v Expert Mode Auto actions demo package documentation is here Predefined templates cover a variety of jobs, yet they can lack the specificity or complexity you need for monitoring. For instance, you can use the Rogue Application Expert Mode Below are a variety of Auto Action written using JSON. Map Reduce Alert on Map Reduce jobs using > 1 TB of memory. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Alert on Map Reduce jobs using > 1000 vcores. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert on Map Reduce jobs running more than 1 hour. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"elapsed_time\",\n \"compare\": \">\",\n \"value\": 3600000\n} Alert on MapReduce jobs that may affect any production SLA jobs running on a cluster. Check for MapReduce jobs not in the SLA queue, running between 12 am and 3 am, and using > 1TB of memory. Use the JSON rule specifying Map Reduce jobs using > 1 TB and set the rule conditions as shown. Alert on ad-hoc MapReduce jobs use a majority of cluster resources which may impact the cluster performance. Check for MapReduce Jobs in the “root.adhocd” queue, running between 1 am and 5 am, and using > 1TB of memory. Use the JSON rule specifying Map Reduce jobs using > 1 TB and set the rule conditions as shown. Spark The JSON rules to alert if a Spark application is grabbing majority of cluster resources are exactly like the Map Reduce rules for except Alert on only Spark jobs using > 1 TB of memory. {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Alert on only Spark jobs using > 1000 vcores. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert if a Spark SQL query has unbalanced input vs output, which may indicate inefficient or “rogue” queries. Check if any Spark application is generating lots of rows in comparison with input, i.e. ‘outputToInputRowRatio’ > 1000 {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"user_metric\": \"outputToInputRowRatio\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert if a Spark SQL has lots of output partitions. Check if any Spark application ‘outputPartitions’ > 10000. {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"user_metric\": \"outputPartitions\",\n \"compare\": \">\",\n \"value\": 10000\n}\n Hive Alert if a Hive query duration is running longer than expected. Check if a Hive query duration > 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n}\n Alert if SLA bound query is taking longer than expected. Check if a Hive query started between 1 am and 3 am in queue ‘prod’ runs longer than > 20 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n}\n Set the rule conditions as shown. Check if any Hive query is started between 1 am and 3 am in any queue except ‘prod’. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"metric\": \"app_count\",\n \"compare\": \">\",\n \"value\": 0\n}\n Set the rule conditions as shown. Alert if a Hive query has extensive I\/O, wich may affect HDFS and other apps. Check if a Hive query writes out more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"totalDfsBytesWritten\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n Check if a Hive query reads in more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n Detect inefficient and “stuck” Hive queries, i.e., alert if a Hive query has not read lots of data but running for a longer time. Check if any Hive query has read less than 10GB in total and its duration is longer than 1 hour. {\n \"SAME\":[\n {\n \"scope\":\"multi_app\",\n \"type\":\"HIVE\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":3600000\n },\n {\n \"scope\":\"multi_app\",\n \"type\":\"HIVE\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\"<\",\n \"value\":10485760\n }\n ]\n}\n Tez Alert if a Tez query duration is running longer than expected. Check if a Tez query duration > 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n}\n Alert if SLA bound query is taking longer than expected. Check if a Tez query started between 1 am and 3 am in queue ‘prod’ runs longer than > 20 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n}\n Set the rule conditions as shown. Check if any Tez query is started between 1 am and 3 am in any queue except ‘prod’. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"metric\": \"app_count\",\n \"compare\": \">\",\n \"value\": 0\n}\n Set the rule conditions as shown. Alert if a Tez query has extensive I\/O, wich may affect HDFS and other apps. Check if a Tez query writes out more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"totalDfsBytesWritten\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n Check if a Tez query reads in more than 100GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n Detect inefficient and “stuck” Tez queries, i.e. for example alert if a Tez query has not read lots of data but running for a longer time. Check if any Tez query has read less than 10GB in total and its duration is longer than 1 hour. {\n \"SAME\":[\n {\n \"scope\":\"multi_app\",\n \"type\":\"TEZ\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":3600000\n },\n {\n \"scope\":\"multi_app\",\n \"type\":\"TEZ\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\"<\",\n \"value\":10485760\n }\n ]\n}\n Workflow Alert if a workflow is taking longer than expected. Check if any workflow is running for longer than 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n} Check if a SLA bound workflow named ‘market_report’ is running for longer than 30 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n} Alert if a SLA bound workflow is reading more data than expected. Check if workflow named '‘market_report’' and 'totalDfsBytesRead' > 100G. {\n \"scope\": \"by_name\",\n \"target\": \"market_report\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n} Alert if a SLA bound workflow is taking longer and kill bigger applications which are not run by the SLA user. Check if Workflow named ‘prod_ml_model’ and duration > 2h then kill jobs with allocated_vcores >= 20 and user != ‘sla_user'. {\n \"scope\": \"by_name\",\n \"target\": \"prod_ml_model\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 7200000\n} Enter the following code in the Export Mode {\n \"action\": \"kill_app\",\n \"max_vcores\": 20,\n \"not_in_queues\": [\"sla_queue\"],\n \"if_triggered\": false\n} USER UserAlert for Rogue User - Any user consuming a major portion of cluster resources. Check for any user where the allocated vcores aggregated over all their applications is > 1000. You can use the Rouge User or the JSON rule {\n \"scope\": \"multi_user\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} Check for any user where the allocated memory aggregated over all their applications is > 1TB. You can use the Rouge User {\n \"scope\": \"multi_user\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n} Queue Alert for Rogue Queue - Any queue consuming a major portion of cluster resources. Check for any queue where the allocated vcores aggregated over all its applications for any queue is > 1000. {\n \"scope\": \"multi_queue\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} Check for any queue where the allocated memory aggregated over all its applications is > 1TB. {\n \"scope\": \"multi_queue\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n} Applications While applications in quarantine queue continue to run, the queue is preemptable and has a low resource allocation. If any other queue needs resources, it can preempt applications in the quarantine queue. Moving rogue applications to quarantine queue frees resources for other applications. Below we are alerting on vcores; to alert on memory just subsitute memory for vcores in the following rules. Alert for Rogue application If any application (not sla bound) is consuming more than certain vcores at midnight, move it to a quarantine queue. You can use the Rogue Application Or the Expert Mode {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} Set Time rule condition as: Set Move app rule as: Any app needing greater than X amount of resources has to approved, otherwise the app is moved to quarantine queue. You can use the Rogue Application Or use the Expert Mode {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": [X]\n }\n Set Queue rule conditions as: Set Move app action as: Related articles Page: Running Auto Action Demos " }, 
{ "title" : "Running Auto Action Demos", 
"url" : "uguide/uguide-aa/uguide-auto-actions-demos.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Auto Actions \/ Running Auto Action Demos", 
"snippet" : "The Demos program provides you a way to understand and experiment with auto actions and their triggering. Example auto actions included are for Map Reduce, Hive, Spark and Workflow jobs. Scripts are provided that will trigger a violation in order to demonstrate the Actions in \"action\". Text with bra...", 
"body" : "The Demos program provides you a way to understand and experiment with auto actions and their triggering. Example auto actions included are for Map Reduce, Hive, Spark and Workflow jobs. Scripts are provided that will trigger a violation in order to demonstrate the Actions in \"action\". Text with brackets ( { } ), unless otherwise noted, indicates where you must substitute your particular values for the text, including the brackets. is the complete path of the auto-actions-demos directory. DEMO_PATH Unpack and Install the Auto Action Demos Put the auto-actions-demos.tgz Navigate to the directory and unpack the demos. # tar -xvzf auto-actions-demo.tgz Tar creates and unpacks the files into auto-actions-demos The directory should contain the following files. # ls auto-actions-demos\ndemos\/ setup\/ Go to DEMO_PATH setup Open .\/settings Execute the .\/setup-all # .\/setup-all The auto action rules that include time specification are automatically adjusted to the current time period, e.g., from CURRENT_HOUR:00 to CURRENT_HOUR+2:00. After running the script go the the Unravel Server UI and select Manage Auto Actions You should see all the auto action demos listed under Active Auto Actions. Each Auto Action is entitled AA-tag AA-Spark-1c Map-1b Executing the demos Go to DEMO_PATH demos Manage Auto Actions For example, in the UI the auto action named AA-Spark-1c demo-Spark-1c # cd {DEMO_PATH}\/setup \n# ls\ndemo-Hive-1a demo-MR-1a demo-MR-2b demo-Spark-1b\ndemo-Hive-2a demo-MR-1b demo-MR-3a demo-Spark-1c\ndemo-Hive-2b demo-MR-1c demo-MR-3b run-all-demos\ndemo-Hive-3a demo-MR-2a demo-Spark-1a scripts\/ Execute .\/demo-tag AA-tag AA-Spark-1c demo-Spark-1c Some auto action's demo scripts trigger multiple auto actions. This side effect can happen when running your own defined auto actions due to auto actions having overlapping definitions. Cleaning up demos Run .\/clean-all script. # cd {DEMO_PATH}\/setup \n# .\/clean-all This will remove all the demo Auto-Actions from the Unravel Server. If you want to run the demos at a later date, simply follow this script again from 1.3 Auto Actions demos list Application and Alert Type Use case Auto Action Triggering Script   Notes MapReduce Alert if a MapReduce job is grabbing majority of cluster resources and may affect other users jobs at any time. Alert if any MapReduce job allocated memory > 20GB. AA-MR-1a Demo-MR-1a Submits to “root.sla” queue. Alert if any MapReduce job allocated vcores > 10. AA-MR-1b Demo-MR-1b Submits to “root.sla” queue. Alert if any MapReduce job is running for longer than 10 minutes. AA-MR-1c Demo-MR-1c Submits to “root.sla” queue. May trigger MR-1b. MapReduce Alert if a MapReduce job may affect any production SLA jobs running on a cluster. Alert if any application not in the queue ‘sla_queue’ and running between X and Y and allocated memory > 20GB. AA-MR-2a Demo-MR-2a Will also trigger MR-1a as well. Alert if any application not in the queue ‘sla_queue’ and running between X and Y and allocated vcores greater than 10. AA-MR-2b Demo-MR-2b Will also trigger MR-2a as well. MapReduce Alert if an ad-hoc MapReduce job is grabbing majority of cluster resources and may affect cluster performance. Alert if any MapReduce job allocated vcores > 10 between X and Y in queue ‘root.adhoc’. AA-MR-3a Demo-MR-3a Submits to “root.adhoc” queue. Will also trigger MR-1a and MR-2a. Alert if any MapReduce job allocated memory > 20GB between X and Y in queue ‘root.adhoc’. AA-MR-3b Demo-MR-3b Submits to “root.adhoc” queue. Will also trigger MR-1b and MR-2b. Spark Alert if a Spark application is grabbing majority of cluster resources and may affect other users jobs at any time. Alert if any Spark application has allocated more than 20GB of memory. AA-Spark-1a Demo-Spark-1a Alert if any Spark application allocated vcores > 8. AA-Spark-1b Demo-Spark-1b Alert if any Spark application is running longer than 10 minutes AA-Spark-1c Demo-Spark-1c Spark Alert if a Spark SQL query has unbalanced input vs output, which may point to an inefficient or “rogue” queries. Alert if any Spark application is generating lots of rows in comparison with input,i.e. ‘outputToInputRowRatio’ > 1000. TBD Hive Alert if a Hive query duration is running longer than expected. Alert if a Hive query duration > 5 minutes. AA-Hive-1a Demo-Hive-1a You can Ctrl-C the query once it triggers the AA. Hive Alert if SLA bound query is taking longer than expected. Alert if a Hive query started between A:00 and B:00 in queue ‘root.prod’ and duration > 10 minutes. AA-Hive-2a Demo-Hive-2a You can Ctrl-C the query once it triggers the AA. Alert if any Hive query is started between A:00 and B:00 in any queue except ‘root.prod’. AA-Hive-2b Demo-Hive-2b Very short query. Hive Alert if a Hive query is writing lots of data. Alert if a Hive query writes out more than 200MB in total. AA-Hive-3a Demo-Hive-3a Alert if a Hive query reads in more than 10GB in total. AA-Hive-3b Demo-Hive-3b Hive Detect inefficient and “stuck” Hive queries. Alert if any Hive query has read less than 10MB in total and its duration is longer than 10 minutes. AA-Hive-4a Demo-Hive-4a Alert if any Hive query in the queue 'root.adhoc' is running for longer than 2 minutes. AA-Hive-4b Demo-Hive-4b Workflow Alert if a workflow is taking longer than expected. Alert if any workflow is running for longer than 10 minutes, might be stuck. AA-WF-1a Demo-WF-1a You can Ctrl-C the query once it triggers the AA. Alert if a SLA bound workflow named ‘market_report’ is running for longer than 5 minutes. AA-WF-1b Demo-WF-1b You can Ctrl-C the query once it triggers the AA. Workflow Alert if a workflow is reading more data than expected. Related articles Page: Auto Actions Overview Page: Running Auto Action Demos Page: Snooze Feature Page: Sample Auto Actions Page: Setting Up Email for Auto Actions and Collaboration " }, 
{ "title" : "Supported cluster metrics", 
"url" : "uguide/uguide-aa/uguide-auto-actions-metrics.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Auto Actions \/ Supported cluster metrics", 
"snippet" : "[empty] Auto Actions by default collect metrics from the YARN Resource Manager and MapReduce Application Master for all applications running (or submitted) on the target cluster. MapReduce Application Master (AM) also maintains various counters. Users can use these metrics and the counters when defi...", 
"body" : "[empty] Auto Actions by default collect metrics from the YARN Resource Manager and MapReduce Application Master for all applications running (or submitted) on the target cluster. MapReduce Application Master (AM) also maintains various counters. Users can use these metrics and the counters when defining an Auto Actions rule. Additionally there are Hive\/Workflow and Spark metrics which can used to define Auto Actions rules. Monitoring is performed on most live running here Monitoring is only Hive\/Workflow Metrics Metric Definition duration total time taken by the application totalDfsBytesRead total hdfs bytes read totalDfsBytesWritten total hdfs bytes written MapReduce Application Master and Map Reduce Metrics Type Metric Definition elapsedAppTime time since the application was started Map mapsCompleted number of completed maps mapsPending number of maps still to be run mapsRunning number of running maps mapsTotal total number of maps Map Attempts failedMapAttempts number of failed map attempts killedMapAttempts number of killed map attempts newMapAttempts number of new map attempts runningMapAttempts number of running map attempts Reduce reducesCompleted number of completed reduces reducesPending number of reduces still to be run reducesRunning number of running reduces reducesTotal total number of reduces Reduce Attempts failedReduceAttempts number of failed reduce attempts killedReduceAttempts number of killed reduce attempts newReduceAttempts number of new reduce attempts runningReduceAttempts number of running reduce attempts successfulReduceAttempts number of successful reduce attempts For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-mapreduce-client\/hadoop-mapreduce-client-core\/MapredAppMasterRest.html#Jobs_API MapReduce and File System Counters Metric Definitions fileBytesRead mount of data read from local file system fileBytesWritten amount of data written to local file system fileReadOps number of read operations from local file system fileLargeReadOps number of read operations of large files from local file system fileWriteOps number of write operations from local file system hdfsBytesRead amount of data read from HDFS hdfsBytesWritten amount of data written to HDFS hdfsReadOps number of read operations from HDFS hdfsLargeReadOps number of read operations of large files from HDFS hdfsWriteOps number of write operations to HDFS Job Counters Type Metric Definition Map dataLocalMaps number of map tasks which were launched on the nodes containing required data mbMillisMaps total megabyte-seconds taken by all map tasks millisMaps total time spent by all map tasks slotsMillisMaps total time spent by all executing maps in occupied slots vcoresMillisMaps total vcore-seconds taken by all map tasks Reduce mbMillisReduces total megabyte-seconds taken by all reduce tasks millisReduces total time spent by all reduce tasks slotsMillisReduces total time spent by all executing reduces in occupied slots totalLaunchedReduces total number of launched reduce tasks vcoresMillisReduces total vcore-seconds taken by all reduce tasks File Input\/Output Format Counters Metric Definition bytesRead amount of data read by every tasks for every filesystem bytesWritten amount of data written by every tasks for every filesystem For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-mapreduce-client\/hadoop-mapreduce-client-core\/MapredAppMasterRest.html#Job_Counters_API Map-Reduce Framework Counters Type Metric Definition Map failedShuffle total number of mappers which failed to undergo through shuffle phase mapInputRecords total number of records processed by all of the mappers mapOutputBytes total amount of (uncompressed) data produced by mappers mapOutputMaterializedBytes amount of (compressed) data which was actually written to disk mapOutputRecords total number of records produced by by all of the mappers mergedMapOutputs total number of mapper output files undergone through shuffle phase shuffledMaps total number of mappers which undergone through shuffle phase Reduce reduceInputGroups total number of unique keys reduceInputRecords total number of records processed by all reducers reduceOutputRecords total number of records produced by all reducers reduceShuffleBytes amount of data processed in shuffle and reduce phase Records combineInputRecords total number of records processed by combiners combineOutputRecords total number of records produced by combiners spilledRecords total number of map and reduce records that were spilled to disk Time gcTimeMillis wall time spent in Java Garbage Collection cpuMilliseconds cumulative CPU time for all tasks Memory committedHeapBytes total amount of memory available for JVM physicalMemoryBytes total physical memory used by all tasks including spilled data splitRawBytes amount of data consumed for metadata representation during splits virtualMemoryBytes total virtual memory used by all tasks Shuffle Errors Metric Definition badId total number of errors related with the interpretations of IDs from shuffle headers connection total number of established network connections ioError total number of errors related with reading and writing intermediate data wrongLength total number of errors related to compression and decompression of intermediate data wrongMap total number of errors related to duplication of the mapper output data wrongReduce total number of errors related to the attempts of shuffling data for wrong reducer Spark Metrics In addition to the metric set supported by MapReduce applications, Spark applications can be polled on: Type Metric Definition Join joinInputRowCount the total input rows of the first join of the SQL query, aggregated for all the queries that are part of the application totalJoinInputRowCount total number of input rows count for all join operators of all SQL queries that are part of the application totalJoinOutputRowCount total number of output rows count for all join operators of all SQL queries that are part of the application joinOutputRowCount the total output rows of the first join of the SQL query, aggregated for all the queries that are part of the application Partitions inputPartitions total number of input partitions for all SQL queries that are part of the application outputPartitions total number of output partitions for all SQL queries that are part of the application Records inputRecords cumulative number of input records for all SQL queries that are part of the application (collected at stage level) outputRecords cumulative number of output records for all SQL queries that are part of the application (collected at stage level) outputToInputRecordsRatio outputRecords \/ inputRecords if inputRecords > 0, else 0 YARN Resource Manager metrics Metric Definition allocatedMB The sum of memory in MB allocated to the application’s running containers allocatedVCores The sum of virtual cores allocated to the application’s running containers appCount total number of applications elapsedTime The elapsed time since the application started (in ms) runningContainers The number of containers currently running for the application memorySeconds The amount of memory the application has allocated (megabyte-seconds) vcoreSeconds The amount of CPU resources the application has allocated (virtual core-seconds) For more details see: https:\/\/hadoop.apache.org\/docs\/r2.7.3\/hadoop-yarn\/hadoop-yarn-site\/ResourceManagerRest.html#Cluster_Applications_API " }, 
{ "title" : "Use Cases", 
"url" : "uguide/uguide-usecase.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Use Cases", 
"snippet" : "Unravel Web WI assists you in the detection and resolution of problems. Detecting Resource Contention in the Cluster Detect and resolve resource contention. Identifying Rogue Applications Identify rogue application, i.e., jobs running to long or applications using too many vcores. Optimizing the Per...", 
"body" : "Unravel Web WI assists you in the detection and resolution of problems. Detecting Resource Contention in the Cluster Detect and resolve resource contention. Identifying Rogue Applications Identify rogue application, i.e., jobs running to long or applications using too many vcores. Optimizing the Performance of Spark Applications Identify and optimize underperforming Spark apps. Kafka Insights Identity lagging or stalled Consumer Groups within a cluster. " }, 
{ "title" : "Identifying Rogue Applications", 
"url" : "uguide/uguide-usecase/uguide-usecase-id-rouge-apps.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Use Cases \/ Identifying Rogue Applications", 
"snippet" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue a...", 
"body" : "Rogue applications can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue applications include jobs that take too long to run or applications that use too many vcores. Unravel Web UI makes identifying rogue apps easy: Click Operations Charts Resources In the Cluster VCore Cluster MemoryMB Unravel Web UI displays the list of applications running or pending in the cluster at the spike's timestamp, at the bottom of the page. Click the application which has allocated the highest number of vcores. In this example, there is a MapReduce application which has allocated 240 vcores of the cluster. Check the Application page to see Unravel's recommendations for improving the efficiency of this MapReduce application. For example: Set up an AutoAction to proactively alert if a rogue application is occupying the cluster. " }, 
{ "title" : "Detecting Resource Contention in the Cluster", 
"url" : "uguide/uguide-usecase/uguide-usecase-res-content.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Use Cases \/ Detecting Resource Contention in the Cluster", 
"snippet" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pend...", 
"body" : "If your cluster has a lot of apps these apps will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Click Operations Charts Resources In the Cluster VCores Cluster MemoryMB Unravel Web UI displays the list of applications running or pending in the cluster at the spike's timestamp, at the bottom of the page. When you see many applications in the ACCEPTED state (not RUNNING), it means they are waiting for resources. For example, the screenshot below shows that only one Spark application is RUNNING (consuming resources) and four MR applications are ACCEPTED (waiting for resources). Now you can take steps to resolve the problem. Setting Up Auto Actions (Alerts) To define an action for Unravel to execute automatically when it detects resource contention in the cluster: Click Manage Auto Actions Select Resource Contention Specify the rules for triggering this auto action, such as a memory threshold, job count threshold, and so on: Select the Send Email " }, 
{ "title" : "Optimizing the Performance of Spark Applications", 
"url" : "uguide/uguide-usecase/uguide-usecase-opt-spark.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Use Cases \/ Optimizing the Performance of Spark Applications", 
"snippet" : "Unravel Web UI makes it easy for you to identify under-performing Spark apps: click Operations Dashboard INEFFICIENT APPLICATIONS The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web...", 
"body" : "Unravel Web UI makes it easy for you to identify under-performing Spark apps: click Operations Dashboard INEFFICIENT APPLICATIONS The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before Tuning Before tuning, Unravel Web UI indicated that this app had a running duration of 34 min 11sec: In addition, Unravel Web UI captured details as shown in the following events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY CONTENTION FOR CPU RESOURCES OPPORTUNITY FOR RDD CACHING Save up to 9 minutes by caching at PetFoodAnalysisCaching.scala:129,with StorageLevel.MEMORY_AND_DISK_SER TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 2 to 127, partitions from 2 to 289, adjust driver memory (to1161908224)and yarn overhead (to 819 MB). After Tuning After tuning, Unravel Web UI indicates that this app now has a running duration of 1min 19sec: Unravel Web UI displays these events: LOW UTILIZATION OF MEMORY RESOURCES LOW UTILIZATION OF SPARK STORAGE MEMORY LARGE IDLE TIME FOR EXECUTORS TOO FEW PARTITIONS W.R.T. AVAILABLE PARALLELISM Change executor instances from 127 to 138 " }, 
{ "title" : "Kafka Insights", 
"url" : "uguide/uguide-usecase/uguide-usecase-kafka.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Use Cases \/ Kafka Insights", 
"snippet" : "Overview The Unravel Intelligence engine provide insights into a cluster's activities through the status of the Consumer Group's (CG). A CG's state is determined on a partition by partition basis, specifically by its commit offset relative to the producers log commit offset for a particular partitio...", 
"body" : " Overview The Unravel Intelligence engine provide insights into a cluster's activities through the status of the Consumer Group's (CG). A CG's state is determined on a partition by partition basis, specifically by its commit offset relative to the producers log commit offset for a particular partition, : commit offset is in pace with the log end offset. OK : the Consumer is lagging behind the Producer. Both the consumer's and log commit offsets are increasing but the Producer’s is increasing faster. When graphed over time, the slope of the Producer is greater than the slope of the consumer. Lagging : the Consumer has essentially stopped while the Producer is still active. The consumer's offset is not increasing while the log commit offset is increasing. Graphically, the slope of the consumer is essentially zero. Stalled A Topic's status is set to the lowest status among its Consumer Groups and the Consumer Group's status is set to the lowest status among its partitions. You must drill down from the Cluster in order to determine where Topic\/Consumer Group is lagging or stalled. Use Case Example 1. Go to Operation Charts Kafka 2. Use the graph scroll-bar to view graphs of the cluster metrics. The Topic table lists all topics with their consumers and the topic's status. Click within a graph to see what topics were available at that point in time. Click the topic name to examine it. In this case, topic test2 , demo test-consumer-group Consumers with the same name are grouped together into one consumer group. Choosing all clusters 3. The Topic View opens with Topic Detail tab displaying the brokers KPIs. The Consumer Details table lists active Consumers for that point in time with it's status. The Consumer Group(s) KPI's are across all partitions. Click within the graph to see what Consumers were running at that point in time. Below test2 demo demo 4. Click on the Partition Detail tab to view the Consumer(s) information per partition. The Consumer Details table now lists the KPIs and status for all consumer groups on the partition displayed. Click within the graph to see what Consumer(s) were running at that point in time on that partition. Partition 0 is initially displayed using the metric , offset test-consumer-group demo 5. Use the Partition Metric Offset Consumer Lag Go To Consumer Lag test-consumer-group 6. The CG view lists the Topics the group is consuming and opens with graphs of its broker(s) KPI’s. Just as a Topic can have multiple consumers with varying states, a Consumer Group can be consuming multiple topics with varying degrees of success. In this case, there is only one Topic being consumed and the CG is stalled. 7. Click on the Partition Detail tab to see partition(s). The Partition Details table lists the partitions, their KPIs, and their status 8. Use the pull down menus to change Metric or Partition used for the graph. The eye ( consumer lag. " }, 
{ "title" : "Resource Metrics", 
"url" : "uguide/uguide-resource-metrics.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Resource Metrics", 
"snippet" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description allocatedBytes bytes Accumulated number of allocated bytes availableMemory bytes An estimate of memory avail...", 
"body" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description allocatedBytes bytes Accumulated number of allocated bytes availableMemory bytes An estimate of memory available for launching new processes avgFullGcInterval nanoseconds (DURATION) Average interval between two subsequent full GCs. Might not be available for particular GC algorithms avgMinorInterval nanoseconds (DURATION) Average interval between two subsequent minor GCs. Might not be available for particular GC algorithms blockingRatio PERCENTS Estimated percentage of CPU time spent in kernel blocking operations committedHeap bytes Committed heap size committedNonHeap bytes Committed non-heap size committedVirtualMemory bytes The committed virtual memory in the operating system currentThreadCpuTime nanoseconds (DURATION) Current thread CPU time elapsed since the start of the measurement. Might not be available on some operating systems currentThreadUserTime nanoseconds (DURATION) Current thread user time elapsed since the start of the measurement. Might not be available on some operating systems edenPeakUsage bytes Maximum memory usage in the eden space freePhysicalMemory bytes The free physical memory in the operating system freeSwap bytes The free swap size fullGcCount COUNT Number of full GC runs fullGcTime nanoseconds (DURATION) Accumulated time spent in full GC gcEdenSurvivedAvg bytes Average number of bytes moved from eden to survivor space. Might not be available for particular GC algorithms gcLoad PERCENTS Percentage of CPU time spent in GC gcOldLiveAvg bytes Average number of bytes alive in the old generation. Might not be available for particular GC algorithms gcSurvivorPromotedAvg bytes Average number of bytes moved from survivor to old space. Might not be available for particular GC algorithms gcYoungLiveAvg bytes Average number of bytes alive in the young generation (eden + survivor spaces). Might not be available for particular GC algorithms initHeap bytes Initial heap size initNonHeap bytes Initial non-heap size maxHeap bytes Maximum heap size maxNonHeap bytes Maximum non-heap size minorGcCount COUNT Number of minor GC runs minorGcTime nanoseconds (DURATION) Accumulated time spent in minor GC oldPeakUsage bytes Maximum memory usage in the old space processCpuLoad PERCENT Average process CPU load for the last minute (all cores) snapshotTs milliseconds (TIMESTAMP) The time the metric was read startTs milliseconds (TIMESTAMP) The time when the collection process started survivorPeakUsage bytes Maximum memory usage in the survivor space systemCpuLoad PERCENT Average system CPU load for the last minute (all cores) totalPhysicalMemory bytes The total physical memory in the operating system totalSwap bytes The total swap size usedHeap bytes Used heap size usedNonHeap bytes Used non-heap size vmRss bytes The resident set size of the complete process tree vmRssDir bytes The resident set size of the process " }, 
{ "title" : "Some Keywords and Error Messages", 
"url" : "uguide/uguide-resource-metrics/some-keywords-and-error-messages.html", 
"breadcrumbs" : "Unravel 4.5 \/ User Guide \/ Resource Metrics \/ Some Keywords and Error Messages", 
"snippet" : "Table of Contents Spark Keywords Spark Error Messages MapReduce\/Hive Keywords Commonly searched keywords\/terms and error messages organized by job type. Spark Keywords Spark Key Term Explanation Deploy mode Specifies where the driver runs. In “cluster” mode the driver runs on the cluster. In “client...", 
"body" : " Table of Contents Spark Keywords Spark Error Messages MapReduce\/Hive Keywords Commonly searched keywords\/terms and error messages organized by job type. Spark Keywords Spark Key Term Explanation Deploy mode Specifies where the driver runs. In “cluster” mode the driver runs on the cluster. In “client” mode the driver runs on the edge node, outside of the cluster. Driver Process that coordinates the application execution Executor Process launched by the application on a worker node Resilient Distributed Dataset (RDD) Fault tolerant distributed dataset spark.default.parallelism Default number of partitions spark.dynamicAllocation.enabled Enables dynamic allocation in Spark spark.executor.memory Related to executor memory spark.io.compression.codec Codec used to compress RDDs, the event log file, and broadcast variables spark.shuffle.service.enabled Enables the external shuffle service to preserve shuffle files even when executors are removed. It is required by dynamic allocation. spark.shuffle.spill.compress Specifies whether to compress the shuffle files spark.sql.shuffle.partitions Number of SparkSQL partitions spark.yarn.executor.memoryOverhead YARN memory overhead SparkContext Main Spark entry point; used to create RDDs, accumulators, and broadcast variables SparkConf Spark configuration object SQLContext Main Spark SQL entry point StreamingContext Main Spark Streaming entry point Spark Error Messages Spark Error Messages Explanation Container killed by YARN for exceeding memory limits. The amount of off-heap memory was insufficent at container level. \"spark.yarn.executor.memoryOverhead\" should be increased to a larger value. java.io.IOException: Connection reset by peer Connection reset by peer. Generally occurs in the driver logs when some of the executors fail or are shutdown unexpectedly. java.lang.OutOfMemoryError Out of memory error, insufficient Java heap space at executor or driver levels. org.apache.hadoop.mapred.InvalidInputException Input path does not exist org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Buffer overflow org.apache.spark.sql.catalyst.errors.package$TreeNodeException Exception observed when executing a SparkSQL query on non existing data. MapReduce\/Hive Keywords Key Term Explanation hive.exec.parallel Whether to execute jobs in parallel. hive.exec.reducers.bytes.per.reducer Size per reducer io.sort.mb The total amount of buffer memory to use while sorting files, in megabytes io.sort.record.percent The percentage of io.sort.mb dedicated to tracking record boundaries. mapreduce.input.fileinputformat.split.maxsize Maximum chunk size map input should be split into mapreduce.input.fileinputformat.split.minsize Minimum chunk size map input should be split into mapreduce.job.reduces Default number of reduce tasks per job. mapreduce.map.cpu.vcores Number of virtual cores to request from the scheduler for each map task. mapreduce.map.java.opts JVM heap size for each map task mapreduce.map.memory.mb The amount of memory to request from the scheduler for each map task. mapreduce.reduce.cpu.vcores Number of virtual cores to request from the scheduler for each reduce task. mapreduce.reduce.java.opts JVM heap size for each reduce task mapreduce.reduce.memory.mb The amount of memory to request from the scheduler for each reduce task. " }, 
{ "title" : "Advanced Topics", 
"url" : "adv.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics", 
"snippet" : "Backing-up, Disaster Recovery and Reverting to Prior Version Backing-up, Disaster Recovery, and Reverting to Prior Version copy 1 Cluster Wide Report Configurations Custom Configurations Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Ser...", 
"body" : " Backing-up, Disaster Recovery and Reverting to Prior Version Backing-up, Disaster Recovery, and Reverting to Prior Version copy 1 Cluster Wide Report Configurations Custom Configurations Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Custom Banner Defining a Custom Web UI Port Email Alerts HBASE Configuration Run Unravel Daemons with Custom User Setting Retention Time in Unravel Server Spark Properties for Spark Worker daemon @ Unravel Using the Impala Daemon (impalad) as a Data Source Security Configurations Adding More Admins to Unravel Web UI Adding SSL and TLS to Unravel Web UI Alternate Kerberos Principal for Cluster Access on CDH Configure HDFS permission for Unravel on CDH Sentry Secured Cluster Configure Hive Metastore Permissions Disabling Browser Telemetry Disabling Support\/Comments Panel Enabling LDAP Authentication for Unravel Web UI Enabling SAML Authentication for Unravel Web UI Enabling TLS to Unravel Web UI Directly Encrypting Passwords in Unravel Properties and Settings Kafka Security Restricting Direct Access to Unravel UI Using a Private Certificate Authority with Unravel Autoscaling HDInsight Spark Cluster using Unravel API Creating Active Directory Kerberos Principals and Keytabs for Unravel Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace Enabling or Disabling Cluster Optimization Reports Enabling or Disabling Small Files Reports and Files Reports Enabling or Disabling Cloud Reports and Forecasting Reports Enabling or Disabling Queue Analysis Reports Enabling or Disabling Sessions Triggering an import of FSImage Configure JVM Sensor CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR) HDP Enable JVM Sensor Cluster-Wide for MapReduce2 (MR) Connectivity Connecting to\/Configuration of a Kafka Stream Hive Metastore Access Connecting to a Hive Metastore Hive Metastore Configuration How to Write Jolokia JMX MBean Deploying Unravel over SELinux Monitoring Workflows Monitoring Oozie Workflows Monitoring Airflow Workflows Roles and Role Based Access Control RBAC Roles Configure RBAC Configure LDAP or SAML RBAC Properties Creating Read-only Admins Example RBAC Configurations RBAC UI Manage Page Running Verification Scripts and Benchmarks Tagging What is Tagging? Tagging Workflows Tagging a Hive on Tez Query Tagging Applications APIs REST API Sign In Apps APIs Auto Actions APIs Applications APIs Data APIs Operational APIs Reports APIs Workflow APIs Use Case - Auto Actions and Pagerduty Unravel Monitoring Service Configuration Disk Monitoring JMX Client Monitors REST API Unravel Properties Configurations for OnDemand Reports Sensors Unravel Servers and Sensors Installing Sensors Individual Applications Submitted Through spark-submit Individual Hive Queries Upgrading the Unravel Server and Sensors Uploading Spark Programs to Unravel Uninstalling Unravel Server " }, 
{ "title" : "Empty or missing topic", 
"url" : "adv/empty-or-missing-topic.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Empty or missing topic", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Cluster Wide Report", 
"url" : "adv/adv-cluster-wide-report.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Cluster Wide Report", 
"snippet" : "Unravel's Cluster Wide Report is a java App designed to help you fine tune your cluster wide parameters to maximize its efficiency based upon your cluster's typical workload. Cluster Wide Report collects performance data of prior completed jobs, analyzes the jobs in relation to the cluster's current...", 
"body" : "Unravel's Cluster Wide Report is a java App designed to help you fine tune your cluster wide parameters to maximize its efficiency based upon your cluster's typical workload. Cluster Wide Report collects performance data of prior completed jobs, analyzes the jobs in relation to the cluster's current configuration, generates recommended cluster parameter changes, and predicts and quantifies the impact the changes will have on future runs of the jobs. The majority of these recommendations revolve around these parameters MapSplitSizeParams HiveExecReducersBytesParam HiveExecParallelParam MapReduceSlowStartParam MapReduceMemoryParams You can chose to implement some or all of the recommended settings. " }, 
{ "title" : "Step-by-step guide", 
"url" : "adv/adv-cluster-wide-report.html#UUID-3b31a398-1e68-ff7b-3ff4-39555bca00d8_id_ClusterWideReport-Step-by-stepguide", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Cluster Wide Report \/ Step-by-step guide", 
"snippet" : "Download the report tar ball from the unravel preview site. # curl -v https:\/\/preview.unraveldata.com\/img\/ClusterReportSetup.tar.gz -o ClusterReportSetup.tar.gz Unpack and run the setup script to install app in \/usr\/local\/unravel\/install_bin # tar zxvf ClusterReportSetup.tar.gz # cd ClusterReportSet...", 
"body" : " Download the report tar ball from the unravel preview site. # curl -v https:\/\/preview.unraveldata.com\/img\/ClusterReportSetup.tar.gz -o ClusterReportSetup.tar.gz Unpack and run the setup script to install app in \/usr\/local\/unravel\/install_bin # tar zxvf ClusterReportSetup.tar.gz\n# cd ClusterReportSetup\n# sudo .\/setup.sh \/usr\/local\/unravel\/install_bin The app is now installed in \/usr\/local\/unravel\/install_bin\/ClusterReport cd # ls\ndbin\/ etc\/ origJars\/ \ndlib\/ logs\/ origJars.tar.gz to cd dbin Input.txt # cd \/usr\/local\/unravel\/install_bin\/ClusterReport\/dbin \n# vi Input.txt Configure Input.txt cluster_id =\nqueue =\nstart_date = 2018-01-01\nend_date = 2018-03-28\nmapreduce.map.memory.mb = 2048\nmapreduce.reduce.memory.mb = 2048\nhive.exec.reducers.bytes.per.reducer = 268435456\nmapreduce.input.fileinputformat.split.maxsize = 256000000 Run the report. # su - hdfs .\/cluster_report.sh " }, 
{ "title" : "Configurations", 
"url" : "adv/adv-conf.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations", 
"snippet" : "Custom Configurations Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Custom Banner Defining a Custom Web UI Port Email Alerts HBASE Configuration Run Unravel Daemons with Custom User ...", 
"body" : " Custom Configurations Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Custom Banner Defining a Custom Web UI Port Email Alerts HBASE Configuration Run Unravel Daemons with Custom User Setting Retention Time in Unravel Server Spark Properties for Spark Worker daemon @ Unravel Using the Impala Daemon (impalad) as a Data Source Security Configurations Adding More Admins to Unravel Web UI Adding SSL and TLS to Unravel Web UI Alternate Kerberos Principal for Cluster Access on CDH Configure HDFS permission for Unravel on CDH Sentry Secured Cluster Configure Hive Metastore Permissions Disabling Browser Telemetry Disabling Support\/Comments Panel Enabling LDAP Authentication for Unravel Web UI Enabling SAML Authentication for Unravel Web UI Enabling TLS to Unravel Web UI Directly Encrypting Passwords in Unravel Properties and Settings Kafka Security Restricting Direct Access to Unravel UI Using a Private Certificate Authority with Unravel Autoscaling HDInsight Spark Cluster using Unravel API Creating Active Directory Kerberos Principals and Keytabs for Unravel Creating an AWS RDS CloudWatch Alarm for FreeStorageSpace Enabling or Disabling Cluster Optimization Reports Enabling or Disabling Small Files Reports and Files Reports Enabling or Disabling Cloud Reports and Forecasting Reports Enabling or Disabling Queue Analysis Reports Enabling or Disabling Sessions Triggering an import of FSImage " }, 
{ "title" : "Custom Configurations", 
"url" : "adv/adv-conf/adv-conf-cust.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations", 
"snippet" : "Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Custom Banner Defining a Custom Web UI Port Email Alerts HBASE Configuration Run Unravel Daemons with Custom User Setting Retention Time...", 
"body" : " Configure Permission for Unravel daemons on CDH Sentry Secured Cluster Configuring Multiple Hosts for Unravel Server Creating Multiple Workers for High Volume Data Custom Banner Defining a Custom Web UI Port Email Alerts HBASE Configuration Run Unravel Daemons with Custom User Setting Retention Time in Unravel Server Spark Properties for Spark Worker daemon @ Unravel Using the Impala Daemon (impalad) as a Data Source " }, 
{ "title" : "Configure Permission for Unravel Daemons on a CDH Sentry-Secured Cluster", 
"url" : "adv/adv-conf/adv-conf-cust/adv-conf-custom-unravel-daemons-cdh-sentry-secured.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Configure Permission for Unravel Daemons on a CDH Sentry-Secured Cluster", 
"snippet" : "HDFS Permission Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl usernames com.unraveldata.login.admins.ldap.readonly=user1,user2,user3 HDFS folder path ...", 
"body" : "HDFS Permission Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl usernames com.unraveldata.login.admins.ldap.readonly=user1,user2,user3 HDFS folder path user access Purpose hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR hdfs or alternate user READ + WRITE data transfer from Hive jobs when Unravel is not up hdfs:\/\/user\/spark\/applicationHistory hdfs or alternate user READ Spark event log hdfs:\/\/user\/history\/done hdfs or alternate user READ MapReduce logs hdfs:\/\/tmp\/logs hdfs or alternate user READ YARN aggregation folder hdfs:\/\/user\/hive\/warehouse hdfs or alternate user READ Obtain table partition sizes with \"stat\" only hdfs:\/\/user\/spark\/spark2applicationHistory hdfs or alternate user READ Spark2 event log (only if Spark2 installed) Please see Alternate Kerberos Principal for Cluster Access on CDH HIVE Metastore Permission Unravel daemons need to be able to have READ permission on hive metastore. Create a new database user and grant the READ access to the hive metastore database. In CDH, use the following Cloudera Manager API to check hive configuration, that will provides you the hive metastore database name and port. By default in CDH and HDP, the hive metastore database name is hive. For TLS-secured CM # curl -k -u admin:admin \"https:\/\/cmhost.mydomain.com:7183\/api\/v11\/clusters\/$ClusterName\/services\/hive\/config\" For non-TLS secured CM # curl -k -u admin:admin \"http:\/\/cmhost_ip:7183\/api\/v11\/clusters\/$ClusterName\/services\/hive\/config\" SQL\/PostgresSQL Login, as admin or root user who has grant privilege, to database host that is hosting the hive metastore database , and create a new database user and grant select privilege on the hive metastore database. For MySQL, the following is the mysql commands to create a new user unravelka MySQL GRANT SELECT ON hive.* to 'unravelk'@'%' identified by 'unravelk_password';\nflush privileges; PostgreSQL CREATE USER unravelk WITH ENCRYPTED PASSWORD 'unravelk_password';\n\nGRANT CONNECT ON DATABASE hive TO unravelk;\nGRANT USAGE ON SCHEMA public TO unravelk;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO unravelk;\nGRANT SELECT ON ALL SEQUENCES IN SCHEMA public TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT USAGE ON SEQUENCES TO unravelk; The default PostgreSQL admin password created by Cloudera Manager is stored on \/var\/lib\/cloudera-scm-server-db\/data\/generated_password.txt command login as admin user psql cloudera-scm # psql -U cloudera-scm -p 7432 -h localhost -d postgres Update HIVE metastore access information on Unravel UI See Connecting to a Hive Metastore \/usr\/local\/unravel\/etc\/unravel.properties " }, 
{ "title" : "Configuring Multiple Hosts for Unravel Server", 
"url" : "adv/adv-conf/adv-conf-cust/adv-config-multiple-host-unravel-server.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Configuring Multiple Hosts for Unravel Server", 
"snippet" : "This topic explains how to configure multiple hosts for Unravel Server. This is useful for volume\/throughput\/reliability requirements. Prerequisites If you use Kerberos, set that up first on host1 Install the same Unravel RPM on one or two additional hosts, hereafter referred to as host2 host3 Each ...", 
"body" : "This topic explains how to configure multiple hosts for Unravel Server. This is useful for volume\/throughput\/reliability requirements. Prerequisites If you use Kerberos, set that up first on host1 Install the same Unravel RPM on one or two additional hosts, hereafter referred to as host2 host3 Each host is assigned unique roles identified by daemon names that start with unravel_ The internal DNS or IP address of a host is specific to your installation. Expected Result When you complete the steps below, the expected result is Multiple Unravel hosts work together in an ensemble. Each host has a unique role, and is identified by a daemon named unravel_xyz unravel_xyz_n n Unravel Web UI ( unravel_tc Port 4043 unravel_lr If you do not use an external database (db), unravel_db unravel_db is identical on all Unravel hosts in the ensemble. \/usr\/local\/unravel\/etc\/unravel.properties The file unravel.properties unravel.properties unravel.properties Daemons are enabled\/disabled via chkconfig chkconfig Stop Unravel Server On each Unravel host, run this command: # sudo \/etc\/init.d\/unravel_all.sh stop Modify unravel.properties on host1. Pick a machine to be host1, where the Unravel Web UI will run. If the bundled db is in use, edit \/usr\/local\/unravel\/etc\/unravel.properties must be a fully qualified DNS or IP address. Replace UNRAVEL_HOST_IP  3316 unravel_mysql_prod To find your fully qualified hostname Replace all text in brackets (including brackets) with your local variables. # {hostname} -I \n unravel.jdbc.url=jdbc:mysql:\/\/{UNRAVEL_HOST_IP}:{3316}\/{unravel_mysql_prod} Copy host1's unravel.properties to other hosts. Copy \/usr\/local\/unravel\/etc\/unravel.properties \/usr\/local\/unravel\/etc\/unravel.ext.sh \/etc\/unravel_ctl # scp \/usr\/local\/unravel\/etc\/unravel.properties {host2}:\/usr\/local\/unravel\/etc\/\n# scp \/usr\/local\/unravel\/etc\/unravel.ext.sh {host2}:\/usr\/local\/unravel\/etc\/\n# scp \/etc\/unravel_ctl {host2}:\/etc\/ Verify that the ownership of unravel.properties unravel.ext.sh unravel:unravel \/etc\/unravel_ctl root:root. The scripts invoked below will make an identical change to the unravel.properties Assign Roles Use these scripts to assign unique roles to the hosts. To reduce the chance of errors, the command line arguments are the same on each host, but notice that the script name is different. The arguments are the hostnames IP addresses of the hosts in the ensemble. These scripts establish the roles each host plays in the ensemble. The main effect is to assign specific Unravel logical daemons to one host and only one host. Note that some daemons have names like unravel_xyz_1 unravel_xyz_2 xyz The switch_to_* unravel.properties unravel.id.properties For a 2-host ensemble (substitute host on host1 # sudo \/usr\/local\/unravel\/install_bin\/switch_to_1of2.sh host1 host2 on host2 # sudo \/usr\/local\/unravel\/install_bin\/switch_to_2of2.sh host1 host2 For a 3-host ensemble (substitute host on host1 # sudo \/usr\/local\/unravel\/install_bin\/switch_to_1of3.sh host1 host2 host3 on host2 # sudo \/usr\/local\/unravel\/install_bin\/switch_to_2of3.sh {host1} {host2} {host3} on host3 #sudo \/usr\/local\/unravel\/install_bin\/switch_to_3of3.sh {host1} {host2} {host3} Set Up Zookeeper and Kafka Assign Kafka Partitions Kafka partition assignment (for 3 host installs) is done by evenly distributing a topic over the hosts that exist at topic create time. Topics must be created anew when a new host is added in order to have proper distribution. Redistribute Zookeeper Topics Perform these steps, in sequential order on the specific hosts host3 Stop all and clear Zookeeper and Kafka data areas on each host: on host1 # sudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh on host2 # sudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh on host3 # sudo \/usr\/local\/unravel\/install_bin\/kafka_clear.sh Start up Zookeeper ensemble: on host1 # sudo \/etc\/init.d\/unravel_all.sh start-zk on host2 # sudo \/etc\/init.d\/unravel_all.sh start-zk on host3 # sudo \/etc\/init.d\/unravel_all.sh start-zk Wait 15 seconds for Zookeeper quorum to settle: # sleep 15 Start up Kafka ensemble: on host1 # sudo \/etc\/init.d\/unravel_all.sh start-k on host2 # sudo \/etc\/init.d\/unravel_all.sh start-k on host3 # sudo \/etc\/init.d\/unravel_all.sh start-k Wait 10 seconds for Kafka coordination: # sleep 10 Create the Kafka topics (only on one host): on host1 # sudo \/usr\/local\/unravel\/install_bin\/kafka_create_topics.sh Start Unravel Server Finish multi-host installation by starting up Unravel Server: on host1 # sudo \/etc\/init.d\/unravel_all.sh start \necho \"http:\/\/{UNRAVEL_HOST_IP}:3000\/\" on host2 # sudo \/etc\/init.d\/unravel_all.sh start Edit Hive-site Snippet for Hive-Hook The port 4043 is on host2 hive-site.xml \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip host2 hive-site.xml host1 host2 Snapshot unravel.properties as new golden file " }, 
{ "title" : "Creating Multiple Workers for High Volume Data", 
"url" : "adv/adv-conf/adv-conf-cust/adv-conf-cust-multiple-workers-high-volume.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Creating Multiple Workers for High Volume Data", 
"snippet" : "These instructions apply to single host Unravel deployments only; for multi-host deployments, please contact . Based upon your workload run one set of the below commands. 10000-20000 jobs per day # sudo chkconfig --add unravel_ew_2 # sudo chkconfig --add unravel_jcw2_2 # sudo chkconfig --add unravel...", 
"body" : " These instructions apply to single host Unravel deployments only; for multi-host deployments, please contact . Based upon your workload run one set of the below commands. 10000-20000 jobs per day # sudo chkconfig --add unravel_ew_2 \n# sudo chkconfig --add unravel_jcw2_2\n# sudo chkconfig --add unravel_sw_2 \n# sudo chkconfig --add unravel_ma_2 20000-30000 jobs per day # sudo chkconfig --add unravel_ew_3 \n# sudo chkconfig --add unravel_jcw2_3\n# sudo chkconfig --add unravel_sw_3\n# sudo chkconfig --add unravel_ma_3 Greater than than 30000 jobs per day # sudo chkconfig --add unravel_ew_4 \n# sudo chkconfig --add unravel_jcw2_4\n# sudo chkconfig --add unravel_sw_4\n# sudo chkconfig --add unravel_ma_4 Start Unravel Server. Start the workers' daemons. # sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Custom Banner", 
"url" : "adv/adv-conf/adv-conf-cust/adv-conf-custom-custom-banner.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Custom Banner", 
"snippet" : "You can define a custom banner which appears below Unravel's title bar; it remains open until the user closes it. The banner is displayed until com.unraveldata.custom.banner.end.date The following properties define the banner....", 
"body" : "You can define a custom banner which appears below Unravel's title bar; it remains open until the user closes it. The banner is displayed until com.unraveldata.custom.banner.end.date The following properties define the banner. " }, 
{ "title" : "Custom UI Banner\/Notification", 
"url" : "adv/adv-conf/adv-conf-cust/adv-conf-custom-custom-banner.html#UUID-7f059e75-e067-3c06-72f5-6efca9d53442_UUID-7a680b81-a73b-ac1d-f073-2de13d2c803f", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Custom Banner \/ Custom UI Banner\/Notification", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.custom.banner.display Displays a banner at the top of the Unravel UI. : banner displays text until true end.date : no change to UI false boolean false com.unraveldata.custom.banner.text Text to display when display A banner displays the text ...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.custom.banner.display Displays a banner at the top of the Unravel UI. : banner displays text until true end.date : no change to UI false boolean false com.unraveldata.custom.banner.text Text to display when display A banner displays the text until end.date The text and end.date must both be defined for the banner to be displayed Optional string - com.unraveldata.custom.banner.end.date Date and Time to stop displaying the banner. There is no date\/time limit. Format: YYYYMMDDTHHMMSSZ-000000 The text and end.date must both be defined for the banner to be displayed. Optional string (date) - " }, 
{ "title" : "Defining a Custom Web UI Port", 
"url" : "adv/adv-conf/adv-conf-cust/adv-conf-custom-defining-custom-web-ui-port.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Defining a Custom Web UI Port", 
"snippet" : "These instructions apply to any platform. Port numbers under 1024 are restricted to root setuid Edit \/usr\/local\/unravel\/etc\/unravel.ext.sh export NGUI_PORT=18080 After making this change, restart the affected daemon. # sudo \/etc\/init.d\/unravel_ngui restart...", 
"body" : " These instructions apply to any platform. Port numbers under 1024 are restricted to root setuid Edit \/usr\/local\/unravel\/etc\/unravel.ext.sh export NGUI_PORT=18080 After making this change, restart the affected daemon. # sudo \/etc\/init.d\/unravel_ngui restart " }, 
{ "title" : "Email Alerts", 
"url" : "adv/adv-conf/adv-conf-cust/email-alerts.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Email Alerts", 
"snippet" : "Configure the following properties to set up email alerts. For a complete listing of email properties see here Property Description Default com.unraveldata.monitoring.alert.email.enabled Email is sent when corresponding rule action is triggered. true: false: true com.unraveldata.report.user.email.do...", 
"body" : "Configure the following properties to set up email alerts. For a complete listing of email properties see here Property Description Default com.unraveldata.monitoring.alert.email.enabled Email is sent when corresponding rule action is triggered. true: false: true com.unraveldata.report.user.email.domain Default email domain used for email alerts. Note: This is required for Auto Action email alerts. - com.unraveldata.login.admins Comma separated list of email recipients. A Recipient can be defined by complete email address or by email local-part, e.g., admin,support@ unraveldata.com When no email domain is specified then default domain is used. admin mail.smtp2.from Used for email \"from\" and \"reply-to\" headers when at least one email recipient has unraveldata.com unravel.noreply@unraveldata.com mail.smtp.from Used for email \"from\" and \"reply-to\" headers when no email recipienthas unraveldata.com Note: This is required for Auto Action email alerts unravel.noreply@unraveldata.com " }, 
{ "title" : "HBASE Configuration", 
"url" : "adv/adv-conf/adv-conf-cust/adv-conf-custom-hbase-configuration.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ HBASE Configuration", 
"snippet" : "Please see here Edit \/usr\/local\/unravel\/etc\/unravel.properties If a property is not found, add it, be sure to substitute your local values for the bracketed text red \\AMBARI and CDH You must com.unraveldata.hbase.source.type=hbase_source_type com.unraveldata.hbase.rest.url=Ambari_or_Cloudera_base_ur...", 
"body" : "Please see here Edit \/usr\/local\/unravel\/etc\/unravel.properties If a property is not found, add it, be sure to substitute your local values for the bracketed text red \\AMBARI and CDH You must com.unraveldata.hbase.source.type=hbase_source_type \ncom.unraveldata.hbase.rest.url=Ambari_or_Cloudera_base_url\ncom.unraveldata.hbase.rest.user=hbase_rest_username\ncom.unraveldata.hbase.rest.pwd=hbase_rest_password\ncom.unraveldata.hbase.clusters=comma_separated_cluster_names\n You can optionally com.unraveldata.hbase.service.name=comma_separated_clustername_and_servicename\ncom.unraveldata.hbase.rest.ssl.enabled=true_or_false \ncom.unraveldata.hbase.master.port=Port# \ncom.unraveldata.hbase.regionserver.port=Port# \ncom.unraveldata.hbase.metric.poll.interval=Seconds \ncom.unraveldata.hbase.http.conn.timeout=Seconds \ncom.unraveldata.hbase.http.poll.parallelism=Number \n JMX You must com.unraveldata.hbase.source.type=JMXcom.unraveldata.hbase.clusters=comma_separated_cluster_names\ncom.unraveldata.hbase.cluster_name.node.http.apis=comma_separated_base_node_http_api\n You can optionally com.unraveldata.hbase.metric.poll.interval=Seconds com.unraveldata.hbase.http.conn.timeout=Seconds com.unraveldata.hbase.http.read.timeout=Seconds com.unraveldata.hbase.http.poll.parallelism=Number com.unraveldata.hbase.alert.average.threshold=Number\n Restart the HBase service unravel_us_1. # sudo service unravel_us_1 restart Verifyunravel_us_1is running. # sudo service unravel_us_1 status unravel_us_1 is running Verify themetrics are collected in Elasticsearch. A separate index file is created for each week with an alias hb-search,e.g., hb-20180813_19. Troubleshooting If unravel_us_1 >\/usr\/local\/unravel\/logs\/unravel_us_1.out \n \/usr\/local\/unravel\/logs\/unravel_us_1.log\n \n If hb-* index is not created or no data in Elasticsearch, verify the following daemons are running. (Step 1 above) unravel_us_1 (elastic service) unravel_s_1 (hitdocloader sevice) unravel_hl (kafka service) unravel_k_1 Resources : Service name unravel_us_1 : Service logs \/usr\/local\/unravel\/logs\/unravel_us_1.log, \/usr\/local\/unravel\/logs\/unravel_us_1.out : Configuration file \/usr\/local\/unravel\/etc\/unravel.properties Elasticsearch: : Template File \/usr\/local\/unravel\/etc\/template_hbase_metrics.json : Index Name hb-* " }, 
{ "title" : "Run Unravel Daemons with Custom User", 
"url" : "adv/adv-conf/adv-conf-cust/run-unravel-daemons-with-custom-user.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Run Unravel Daemons with Custom User", 
"snippet" : "Unravel Server daemons run as a local user unravel Run-as hdfs mapr Run-as a customized service account with a name aligned with your local policies the Run-as user should match the user you targeted for setfacl Use the procedure below to change which user Unravel utilizes. This change only needs to...", 
"body" : "Unravel Server daemons run as a local user unravel Run-as hdfs mapr Run-as a customized service account with a name aligned with your local policies the Run-as user should match the user you targeted for setfacl Use the procedure below to change which user Unravel utilizes. This change only needs to be done once; it will be preserved by RPM upgrades. Procedure to Switch User Run the following command to switch running Unravel daemons to user {USER} and with group {GROUP}. Replace both with valid names, without the curly braces; see the scenarios below. # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh {USER} {GROUP} Scenario USER GROUP MapR installation mapr mapr CDH or HDP with simple Linux security hdfs hadoop or hdfs Kerberos enabled on CDH\/HDP and Sentry\/Ranger\/setfacl access already enabled for custom local user \"foo\" in group \"foo\" foo foo Kerberos enabled on CDH\/HDP and Sentry\/Ranger\/setfacl access already enabled for local user \"hdfs\" in group \"hadoop\" hdfs hadoop Start Unravel daemons. # sudo \/etc\/init.d\/unravel_all.sh start Effect The effect of the switch_to_user.sh \/etc\/unravel_ctl RUN_AS USE_GROUP HDFS_KEYTAB_PATH and HDFS_KERBEROS_PRINCIPAL env vars are removed from \/usr\/local\/unravel\/etc\/unravel.ext.sh \/usr\/local\/unravel\/ \/srv\/unravel\/* RUN_AS: \/srv\/unravel\/tmp_hdfs\/ logs in \/srv\/unravel\/log_hdfs \/usr\/local\/unravel\/logs \/srv\/unravel\/log_hdfs the umask of the run-as the chmod bits of \/usr\/local\/unravel \/srv\/unravel " }, 
{ "title" : "Setting Retention Time in Unravel Server", 
"url" : "adv/adv-conf/adv-conf-cust/setting-retention-time-in-unravel-server.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Setting Retention Time in Unravel Server", 
"snippet" : "To adjust the retention time ( time horizon \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.retention.max.days com.unraveldata.history.maxSize.weeks When changing these settings, be aware that long retention requires significant disk space. As a rule of thumb, each map-reduce or Spark job ...", 
"body" : "To adjust the retention time ( time horizon \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.retention.max.days com.unraveldata.history.maxSize.weeks When changing these settings, be aware that long retention requires significant disk space. As a rule of thumb, each map-reduce or Spark job requires about 1MB of disk space; you can store approximately 1000 jobs per 1GB of disk. Open \/usr\/local\/unravel\/etc\/unravel.properties file. # vi \/usr\/local\/unravel\/etc\/unravel.properties Search for and set the following properties. If not found, add the properties. com.unraveldata.retention.max.days com.unraveldata.history.maxSize.weeks com.unraveldata.retention.max.days After changing any of the properties, restart Unravel for the change to take effect. # sudo \/etc\/init.d\/unravel_all.sh restart " }, 
{ "title" : "Spark Properties for Spark Worker daemon @ Unravel", 
"url" : "adv/adv-conf/adv-conf-cust/spark-properties-for-spark-worker-daemon---unravel.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Spark Properties for Spark Worker daemon @ Unravel", 
"snippet" : "Table of Contents Live Pipeline Event log processing Executor log processing Tagging Events Related Other Properties S3 specific properties EMR\/HDInsight specific properties Block storage specific properties (for HDInsight) Data Lake (ADL) specific data properties Live Pipeline Property Definition D...", 
"body" : " Table of Contents Live Pipeline Event log processing Executor log processing Tagging Events Related Other Properties S3 specific properties EMR\/HDInsight specific properties Block storage specific properties (for HDInsight) Data Lake (ADL) specific data properties Live Pipeline Property Definition Default com.unraveldata.spark.live.pipeline.enabled Specifies when to process stages and jobs. Job\/stage data is sent by the Unravel sensor. True False False com.unraveldata.spark.live.pipeline.maxStoredStages Maximum number of jobs\/stages stored in the DB. If an application has jobs\/stages > maxStoredStages maxStoredStages This setting affects only the live pipeline 1000 Event log processing Property Definition Default com.unraveldata.spark.eventlog.location All the possible locations of the event log files. Multiple locations are supported as a comma separated list of values. This property is used only when hdfs:\/\/\/user\/spark\/applicationHistory\/ com.unraveldata.spark.eventlog.maxSize Maximum sizeof the event log file that will be processed by the Spark worker daemon. Event logs larger than MaxSize 1000000000 (~1GB) com.unraveldata.spark.eventlog.appDuration.mins Maximum duration (in minutes) of application to pull Spark event log. 1440 (1 day) com.unraveldata.spark.hadoopFsMulti.useFilteredFiles Specifies how to search the event log files. True False Prefix + suffix search is faster as it avoidslistFiles()API which may take a long time for large directories on HDFS. This search requires that all the possible suffixes com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes False com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes Specifies suffixes used for prefix+suffix search of the event logs when com.unraveldata. spark.hadoopFsMulti.useFilteredFiles NOTE value:,,.lz4,.snappy,.inprogress (This makes sure that uncompressed, lz4 and snappy compressed and inprogress event log files are processed) com.unraveldata.spark.appLoading.maxAttempt Maximum number of attempts for loading the event log file from HDFS\/S3\/ ADL\/WASB etc. 3 com.unraveldata.spark.appLoading.delayForRetry Delay used among consecutive retries when loading the event log files. The actual delay is not constant, it increases progressively by 2^attempt delayForRetry 2000 (2 s) com.unraveldata.spark.tasks.inMemoryLimit Number of tasks to be kept in memory and DB per stage. All stats are calculated for all the task attempts but only the configured number of tasks will be kept in memory\/DB. 1000 Executor log processing Property Definition Default com.unraveldata.job.collector.log.aggregation.base Base path to look for aggregated executor logs Note { yarn.nodemanager.remote-app-log-dir}\/*\/{ yarn.nodemanager.remote-app-log-dir-suffix \/tmp\/logs\/*\/logs\/ \"*\" is replaced by the user running the application com.unraveldata.max.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a successful application 500000000 (~500 MB) com.unraveldata.max.failed.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a failed application 2000000000 (~2GB) com.unraveldata.min.job.duration.for.attempt.log Minimum duration of a successful application or which executor logs will be processed (in milliseconds) 60000 (10 mins) com.unraveldata.min.failed.job.duration.for.attempt.log Minimum duration of failed\/killed application for which executor logs will be processed (in milliseconds) 6000 (1 min) com.unraveldata.attempt.log.max.containers Maximum number of containers for the application. If application has more that configured number of containers then the aggregated executor log will not be processed for the application 500 com.unraveldata.spark.master Default master for spark applications. (Used to download executor log using correct APIs) Valid Options yarn Tagging Property Definition Default com.unraveldata.tagging.enabled Enables tagging functionality. True com.unraveldata.tagging.script.enabled Enables tagging. False com.unraveldata.app.tagging.script.path Specifies tagging script path to use when com.unraveldata.tagging.script.enabled=True \/usr\/local\/unravel\/etc\/apptag.py com.unraveldata.app.tagging.script.method.name Method name that will be executed as part of the tagging script. generate_unravel_tags Events Related Property Definition Default com.unraveldata.spark.events.enableCaching Enables logic for executing caching events. False Other Properties Property Definition Default com.unraveldata.spark.appLoading.maxConcurrentApps Specifies the maximum number of applications stored in the in-memory cache of AppStateInfo object of the Spark worker. 5 com.unraveldata.spark.time.histogram Specifies whether the timeline histogram is generated or not. Note: False S3 specific properties Property Definition Default com.unraveldata.s3.profile.config.file.path The path to the s3 profile file, e.g. , \/usr\/local\/unravel\/etc\/s3ro.properties. - com.unraveldata.spark.s3.profilesToBuckets Comma separated list of profile to bucket mappings in the following format: <s3_profile>:<s3_bucket>, i.e., com.unraveldata.spark.s3.profileToBuckets=profile-prod:com.unraveldata.dev,profile-dev:com.unraveldata.dev IMPORTANT Ensure that the profiles defined in the property above are actually present in the s3 properties file and that each profile has associated a corresponding pair of credentials aws_access_ke aws_secret_access_key access_key\/secretKey - EMR\/HDInsight specific properties Property Definition Default com.unraveldata.onprem Specifies whether the deployment is on premise or on cloud. Important On EMR \/ HDInsight set to False True The following properties are set with values obtained from Microsoft's Azure. See Finding Unravel Properties' Values in Microsoft Azure Block storage specific properties (for HDInsight) For a storage account with the name STORAGE_NAME fs.azure.accountkey.STORAGE_NAME.blob.core.windows.net. Property Definition Default com.unraveldata.hdinsight.storage-account-name-1 Storage account name retrieve from Microsoft Azure com.unraveldata.hdinsight.primary-access-key Storage account access key1 retrieve from Microsoft Azure com.unraveldata.hdinsight.storage-account-name-2 Storage account name set to com.unraveldatahdinsight.storage-account-name-1 com.unraveldata.hdinsight.secondary-access-key Storage account access key2 retrieve from Microsoft Azure Data Lake (ADL) specific data properties Property Definition Default com.unraveldata.adl.accountFQDN The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net retrieve from Microsoft Azure com.unraveldata.adl.clientId Also known as the application Id. An application registration has to be created in the Azure Active Directory retrieve from Microsoft Azure com.unraveldata.adl.clientKey Also known as the application access key. A key can be created after registering an application retrieve from Microsoft Azure com.unraveldata.adl.accessTokenEndpoint It is the OAUTH 2.0 Token Endpoint. It is obtained from the application registration tab. retrieve from Microsoft Azure com.unraveldata.adl.clientRootPath It is the path in the Data lake store where the target cluster has been given access. For instance, on our deployment cluster “spk21utj02” has been given access to “\/clusters\/spk21utj02” on Data Lake store. retrieve from Microsoft Azure " }, 
{ "title" : "Using the Impala Daemon (impalad) as a Data Source", 
"url" : "adv/adv-conf/adv-conf-cust/using-the-impala-daemon--impalad--as-a-data-source.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Custom Configurations \/ Using the Impala Daemon (impalad) as a Data Source", 
"snippet" : "It's best practice to use Cloudera Manager (CM) as the data source for Impala queries, but if you prefer to use the impala daemon ( impalad impalad https:\/\/www.cloudera.com\/documentation\/enterprise\/5-2-x\/topics\/impala_security_webui.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics...", 
"body" : "It's best practice to use Cloudera Manager (CM) as the data source for Impala queries, but if you prefer to use the impala daemon ( impalad impalad https:\/\/www.cloudera.com\/documentation\/enterprise\/5-2-x\/topics\/impala_security_webui.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/impala_webui.html CM as the data source Before following these instructions, follow the steps in Step 1: Install Unravel Server on CDH+CM On Unravel Server, add the following properties in \/usr\/local\/unravel\/etc\/unravel.properties Property Description com.unraveldata.data.source Set this to impalad com.unraveldata.impalad.nodes A comma-separated list of impalad IP:port,IP:port,IP:port com.unraveldata.data.source=impalad \ncom.unraveldata.impalad.nodes=IP:port,IP:port,IP:port\n Change the Impala Lookback Window By default, when Unravel Server starts, it retrieves the last 5 days of Impala queries. To change this, do the following: Open unravel.properties Change com.unraveldata.cloudera.manager.impala.look.back.day For example, com.unraveldata.cloudera.manager.impala.look.back.days=-7 Include a minus sign in front of the new value. Restart unravel_us References https:\/\/www.cloudera.com\/documentation\/cdh\/5-1-x\/Impala\/Installing-and-Using-Impala\/ciiu_install.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-2-x\/topics\/impala_security_webui.html https:\/\/www.cloudera.com\/documentation\/enterprise\/5-3-x\/topics\/impala_webui.html " }, 
{ "title" : "Security Configurations", 
"url" : "adv/adv-conf/security-configurations.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations", 
"snippet" : "Adding More Admins to Unravel Web UI Adding SSL and TLS to Unravel Web UI Alternate Kerberos Principal for Cluster Access on CDH Configure HDFS permission for Unravel on CDH Sentry Secured Cluster Configure Hive Metastore Permissions Disabling Browser Telemetry Disabling Support\/Comments Panel Enabl...", 
"body" : " Adding More Admins to Unravel Web UI Adding SSL and TLS to Unravel Web UI Alternate Kerberos Principal for Cluster Access on CDH Configure HDFS permission for Unravel on CDH Sentry Secured Cluster Configure Hive Metastore Permissions Disabling Browser Telemetry Disabling Support\/Comments Panel Enabling LDAP Authentication for Unravel Web UI Enabling SAML Authentication for Unravel Web UI Enabling TLS to Unravel Web UI Directly Encrypting Passwords in Unravel Properties and Settings Kafka Security Restricting Direct Access to Unravel UI Using a Private Certificate Authority with Unravel " }, 
{ "title" : "Adding More Admins to Unravel Web UI", 
"url" : "adv/adv-conf/security-configurations/adv-config-security-adding-admins.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Adding More Admins to Unravel Web UI", 
"snippet" : "On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties with vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins com.unraveldata.login.admins=admin,admin1,admin2 Restart unravel_ngui # sudo service unravel_ngu...", 
"body" : " On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties with vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins com.unraveldata.login.admins=admin,admin1,admin2 Restart unravel_ngui # sudo service unravel_ngui restart " }, 
{ "title" : "Adding SSL and TLS to Unravel Web UI", 
"url" : "adv/adv-conf/security-configurations/adv-config-security-adding-ssl-tls-to-ui.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Adding SSL and TLS to Unravel Web UI", 
"snippet" : "You can configure an Apache2 web server (httpd) as a reverse proxy to provide HTTPS (SSL\/TLS) security to Unravel Web UI. Follow the steps below to make this work. Alternatively, you can enable TLS directly in unravel_ngui Enabling TLS to Unravel Web UI Directly Secure cookies are NOT supported when...", 
"body" : "You can configure an Apache2 web server (httpd) as a reverse proxy to provide HTTPS (SSL\/TLS) security to Unravel Web UI. Follow the steps below to make this work. Alternatively, you can enable TLS directly in unravel_ngui Enabling TLS to Unravel Web UI Directly Secure cookies are NOT supported when using this Apache2 reverse-proxy method, see instead Enabling TLS to Unravel Web UI Directly These steps were tested with httpd 2.4 and support listening on port 443. Install needed packages. # sudo yum install httpd mod_ssl There is no need to change the default \/etc\/httpd\/conf\/httpd.conf Create \/etc\/httpd\/conf.d\/unravel_https.conf <VirtualHost *:80> \n ServerName unravelhost_FQDN \n Redirect permanent \/ https:\/\/ unravelhost_FQDN Adjust or add property in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.advertised.url=https:\/\/unravelhost_FQDN Restart the unravel_ngui # sudo service unravel_ngui restart Start the http # sudo service httpd start Visit https:\/\/unravelhost_FQDN (using value appropriate for your site) to test access. Troubleshooting To enable verbose logging in Apache2, add LogLevel LogLevel LogLevel debug Do not leave debug settings enabled long term because they add overhead and can fill up the log area if logs are not auto-rolled. To force HTTPS protocol, even if a user requests http:\/\/ Add the following line after the ServerName httpd RequestHeader set X-FORWARDED-PROTO 'https' Restart apache2. " }, 
{ "title" : "Alternate Kerberos Principal for Cluster Access on CDH", 
"url" : "adv/adv-conf/security-configurations/alternate-kerberos-principal-for-cluster-access-on-cdh.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Alternate Kerberos Principal for Cluster Access on CDH", 
"snippet" : "With Kerberos, for quick initial installation, we recommend using the keytab for 'hdfs' principal to enable Unravel server to collect data. For production use, you might want to create an alternate principal for 'unravel' that has read-only access to specific areas and use that keytab. The procedure...", 
"body" : "With Kerberos, for quick initial installation, we recommend using the keytab for 'hdfs' principal to enable Unravel server to collect data. For production use, you might want to create an alternate principal for 'unravel' that has read-only access to specific areas and use that keytab. The procedure is described here. The principal can be named whatever you like, we assume it is called \"unravel\" for it's short name. Be sure to set the principal in unravel.properties and unravel.ext.sh as described in part 1 of the install guide. The steps here apply only to CDH and have been tested using Cloudera Manager recommended setup for Sentry. The approach is to use ACLs on the HDFS filesystem to give the unravel principal access to the specific directories listed in part 2 of the installation guide. Text with brackets ( { } ), unless otherwise noted, indicates where you must substitute your particular values for the text and brackets. Check HDFS Default umask. For access via ACL, the group part of the HDFS default umask needs to have read and execute access. This will allow Unravel to see sub-directories and read files. In Cloudera Manager check the value of dfs.umaskmode hdfs-site.xml fs.permissions.umask-mode Enable ACL Inheritance. In Cloudera Manager, HDFS Configuration, search for \"namenode advanced configuration snippet\", and set dfs.namenode.posix.acl.inheritance.enabled hdfs-site.xml  https:\/\/issues.apache.org\/jira\/browse\/HDFS-6962 Restart cluster. When you are ready, restart the cluster to effect the change of dfs.namenode.posix.acl.inheritance.enabled Change ACL of Target HDFS directories. Run the following commands as global hdfs to grant unravel principal READ permission via ACLs on folders (do these in the order presented): Set ACL for future directories. The following example apply to CDH default setup. If you have Spark2 installed, you will need to apply permission to Spark2 application history folder # hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/spark\/applicationHistory\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/spark\/spark2ApplicationHistory\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/history\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/tmp\/logs\n# hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/hive\/warehouse Please make sure you set the permissions at the \/user\/history \/user\/history \/user\/history\/done Set ACL for existing directories # hadoop fs -setfacl -R -m user:unravel:r-x \/user\/spark\/applicationHistory\n# hadoop fs -setfacl -R -m user:unravel:r-x \/user\/spark\/spark2ApplicationHistory\n# hadoop fs -setfacl -R -m user:unravel:r-x \/user\/history\n# hadoop fs -setfacl -R -m user:unravel:r-x \/tmp\/logs\n# hadoop fs -setfacl -R -m user:unravel:r-x \/user\/hive\/warehouse Verify ACL of Target HDFS Directories Verify HDFS permission on folders: # hdfs dfs -getfacl \/user\/spark\/applicationHistory\n# hdfs dfs -getfacl \/user\/spark\/spark2ApplicationHistory\n# hdfs dfs -getfacl \/user\/history\n# hdfs dfs -getfacl \/tmp\/logs\n# hdfs dfs -getfacl \/user\/hive\/warehouse On the Unravel server, verify HDFS permission on folders as the target user ( unravel hdfs mapr KEYTAB_FILE PRINCIPAL # sudo -u unravel kdestroy\n# sudo -u unravel kinit -kt {KEYTAB_FILE} {PRINCIPAL}\n# sudo -u unravel hadoop fs -ls \/user\/history\n# sudo -u unravel hadoop fs -ls \/tmp\/logs\n# sudo -u unravel hadoop fs -ls \/user\/hive\/warehouse\n " }, 
{ "title" : "Configure HDFS Permission for Unravel on CDH Sentry-Secured Cluster", 
"url" : "adv/adv-conf/security-configurations/adv-conf-custom-hdfs-cdh-sentry-secured.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Configure HDFS Permission for Unravel on CDH Sentry-Secured Cluster", 
"snippet" : "The following instructions apply to Sentry with or without synchronized HDFS ACL. Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl $user_name export RUN_...", 
"body" : " The following instructions apply to Sentry with or without synchronized HDFS ACL. Verify the alternate user running unravel daemon which stated on \/etc\/unravel_ctl By default, some unravel daemons are run by hdfs user and you can change this by creating a file \/etc\/unravel_ctl $user_name export RUN_AS= $user_name HDFS folder path user Access Purpose hdfs:\/\/user\/unravel\/HOOK_RESULT_DIR hdfs or alternate user READ + WRITE data transfer from Hive jobs when Unravel is not up hdfs:\/\/user\/spark\/applicationHistory hdfs or alternate user READ Spark event log hdfs:\/\/user\/history\/done hdfs or alternate user READ MapReduce logs hdfs:\/\/tmp\/logs hdfs or alternate user READ YARN aggregation folder hdfs:\/\/user\/hive\/warehouse hdfs or alternate user READ Obtain table partition sizes with \"stat\" only hdfs:\/\/user\/spark\/spark2applicationHistory hdfs or alternate user READ Spark2 event log (only if Spark2 installed) To configure HDFS ACL permission for the folders above, see Alternate Kerberos Principal for Cluster Access on CDH To enable synchronized HDFS ACL with Sentry on CDH, see Cloudera Documentation here " }, 
{ "title" : "Configure Hive Metastore Permissions", 
"url" : "adv/adv-conf/security-configurations/adv-conf-custom-hive-metastore-permissions.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Configure Hive Metastore Permissions", 
"snippet" : "HIVE Metastore Permission Unravel daemons need to be able to have READ permission on hive metastore. Create a new database user and grant the READ access to the hive metastore database. In CDH, use the following Cloudera Manager API to check hive configuration, that will provides you the hive metast...", 
"body" : " HIVE Metastore Permission Unravel daemons need to be able to have READ permission on hive metastore. Create a new database user and grant the READ access to the hive metastore database. In CDH, use the following Cloudera Manager API to check hive configuration, that will provides you the hive metastore database name and port. By default in CDH and HDP, the hive metastore database name is hive. For TLS-secured CM # curl -k -u admin:admin \"https:\/\/cmhost.mydomain.com:7183\/api\/v11\/clusters\/ $ClusterName For non-TLS secured CM # curl -k -u admin:admin \"http:\/\/cmhost_ip:7183\/api\/v11\/clusters\/ $ClusterName SQL\/PostgresSQL Login to database host, as admin or root user who has grant privilege, that is hosting the hive metastore database , and create a new database user and grant select privilege on the hive metastore database. MySQL commands to create a new user unravelka GRANT SELECT ON hive.* to 'unravelk'@'%' identified by 'unravelk_password';\nflush privileges; PostgreSQL commands to create a new user unravelka CREATE USER unravelk WITH ENCRYPTED PASSWORD 'unravelk_password';\n\nGRANT CONNECT ON DATABASE hive TO unravelk;\nGRANT USAGE ON SCHEMA public TO unravelk;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO unravelk;\nGRANT SELECT ON ALL SEQUENCES IN SCHEMA public TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT USAGE ON SEQUENCES TO unravelk; The default PostgreSQL admin password created by Cloudera Manager is stored on \/var\/lib\/cloudera-scm-server-db\/data\/generated_password.txt psql command login as admin user cloudera-scm # psql -U cloudera-scm -p 7432 -h localhost -d postgres Update HIVE Metastore Access Information See Connecting to a Hive Metastore \/usr\/local\/unravel\/etc\/unravel.properties " }, 
{ "title" : "Disabling Browser Telemetry", 
"url" : "adv/adv-conf/security-configurations/adv-conf-security-disabling-browser-telemetry.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Disabling Browser Telemetry", 
"snippet" : "By default Unravel Web UI collects data about your use of Unravel. You can disable this functionality by following the steps below. If you have a multi-host Unravel install, you must Disable Mixpanel On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties # sudo vi \/usr\/local\/unravel\/etc\/u...", 
"body" : "By default Unravel Web UI collects data about your use of Unravel. You can disable this functionality by following the steps below. If you have a multi-host Unravel install, you must Disable Mixpanel On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Set com.unraveldata.do.not.track false If this property isn't in the file, add it and set it to false com.unraveldata.do.not.track=false Restart the Unravel UI. # sudo service unravel_ngui restart Re-Enable Mixpanel Follow the above steps but in step 2 set com.unraveldata.do.not.track true " }, 
{ "title" : "Disabling Support\/Comments Panel", 
"url" : "adv/adv-conf/security-configurations/adv-disable-support-comments-panel.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Disabling Support\/Comments Panel", 
"snippet" : "By default Unravel UI title bar has a support button that allows user to contact Unravel Data directly via a pop-up. See pop-up example below. You can hide the support button and disable this function within your UI if you chose. Hide\/Disable Support Button On Unravel Server, open \/usr\/local\/unravel...", 
"body" : "By default Unravel UI title bar has a support button that allows user to contact Unravel Data directly via a pop-up. See pop-up example below. You can hide the support button and disable this function within your UI if you chose. Hide\/Disable Support Button On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties vi # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.ngui.support.enabled com.unraveldata.ngui.support.enabled=false Restart the Unravel UI. # sudo service unravel_ngui restart Your title bar should be missing the support button like below. . Show\/Re-enable Support Button To enable the support\/comments panel, repeat the above steps 1-3, but in step 2 set com.unraveldata.ngui.support.enabled true unravel.properties Pop-up Support Box " }, 
{ "title" : "Enabling LDAP Authentication for Unravel UI", 
"url" : "adv/adv-conf/security-configurations/enabling-ldap-authentication-for-unravel-web-ui.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Enabling LDAP Authentication for Unravel UI", 
"snippet" : "You can configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel UI as follows. If you have HiveServer2 set up for AD-based LDAP use the the HiveServer2 settings If you do not use HiveServer2 LDAP, then modify unravel.properties In Unravel Server 4.3...", 
"body" : "You can configure lightweight directory access protocol (LDAP) or active directory (AD) authentication for Unravel UI as follows. If you have HiveServer2 set up for AD-based LDAP use the the HiveServer2 settings If you do not use HiveServer2 LDAP, then modify unravel.properties In Unravel Server 4.3.1.2, the property com.unraveldata.ldap.use_jndi true If you used com.unraveldata.ldap.search_bind_authentication If you set the bind_dn bind_pw Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties For MYDOMAIN QA.EXAMPLE.COM If SSL is in use: Use ldaps:\/\/ LDAP_HOST ldap:\/\/ LDAP_HOST Put a truststore unravel\/jre\/ unravel\/etc\/unravel.ext.sh You can append a port number, if needed; for example, ldap:\/\/ LDAP_HOST Knowing your precise DN is essential for some configuration examples below. If you are uncertain about what a normal DN is, use ldapsearch If you use AD, you can find the Windows domain using ldapsearch @ userPrincipalName To debug, use the property com.unraveldata.ldap.verbose null= Active Directory Domain Setup This is the simplest to implement and enables the widest access to Unravel Server. On the login page, if the user enters a login name with has no domain name appended --like thisUser @ MYDOMAIN @ MYDOMAIN The login name can appear in AD or be seen in an ldapsearch userPrincipalName sAMAccountName @ MYDOMAIN For MYDOMAIN com.unraveldata.login.mode=ldap \nhive.server2.authentication.ldap.url=ldap:\/\/ LDAP_HOST You can combine the optional property hive.server2.authentication hive.server2.authentication.ldap.userFilter Active Directory (AD) with user pattern Example 1: Active Directory with Base DN Defined This example works by composing a DN from the entered user account name in the web UI. For a user login foo uid=foo,ou=myou,dc=domain,dc=com guidKey uid sAMAccountName com.unraveldata.login.mode=ldap \nhive.server2.authentication.ldap.url=ldaps:\/\/ LDAP_HOST An optional hive.server2.authentication.ldap.userFilter Example 2: Active Directory with User Pattern Here we match users with one or more colon-separated patterns, combined with an optional inclusion filter ( userFilter name@MYDOMAIN %s Multiple userDNPatterns can be defined by using a colon ':' separator. The userFilter userDNPattern userDNPattern LDAP_HOST CN=%s,CN=Users,DC=domain DC=com =%s uid com.unraveldata.login.mode=ldap \nhive.server2.authentication.ldap.url=ldap:\/\/ LDAP_HOST Example 3: Active Directory with Group Pattern Here we match users with one or more patterns, and verify group membership in order to approve login. In this example, if the user enters name@MYDOMAIN %s Multiple groupDNPattern : groupDNPattern groupFilter :memberOf=CN= mygroup mygroup myou subdomain domain com Spaces (' ') are significant in the Unravel web login form. Use values relevant to your installation for LDAP_HOST %s myou subdomain com %s uid UNRAVEL_GROUP com.unraveldata.login.mode=ldap \nhive.server2.authentication.ldap.url=ldap:\/\/ LDAP_HOST For Open LDAP LDAP Example 1 Use values relevant to your installation for LDAP_HOST myunit example com uid An optional hive.server2.authentication hive.server2.authentication.ldap.userFilter Active Directory (AD) with user pattern com.unraveldata.login.mode=ldap \nhive.server2.authentication.ldap.url=ldap:\/\/ LDAP_HOST LDAP Example 2 Below is a typical DN to be uid=%s,ou=myunit,dc=example,dc=com %s cn uid You can specify multiple userDNPattern : hive.server2.authentication hive.server2.authentication.ldap.userFilter Active Directory (AD) with user pattern com.unraveldata.login.mode=ldap \nhive.server2.authentication.ldap.url=ldap:\/\/ LDAP_HOST Optional Master Bind Account When user accounts cannot do an LDAP search, you can add the properties below to specify a search account. This can be combined with any of the examples above. The password can be encrypted com.unraveldata.ldap.bind_dn=CN=unravel,ou=myunit,dc=example,dc=com \ncom.unraveldata.ldap.bind_pw= bigsecret Web UI Login Syntax In most cases, people want to enter a simple login account name on the Unravel login page, and the instructions here reflect that. However, if @ name windowsDomain The name field of the login page allows spaces to be entered. That way a name like john smith Restart unravel_ngui. # sudo \/etc\/init.d\/unravel_ngui restart Advanced Properties and Details These properties narrow the search for authorized users. For more detail, see the page User and Group Filtering LDAP in HiveServer2 The Unravel login process is described next. Property names are shortened for readability, but full names should be used. The SIMPLE LDAP authentication mechanism is used. Authentication Process If a bind_dn and bind_pw are set: Authentication starts out by binding with the given account. If the bind works, the verbose log contains Connected using bindDN inetOrgPerson sAMAccountName guidKey guidKey guid userPassword pw is a match inetOrgPerson userDNPattern guidKey If Windows domain is set: Bind as username + at sign + Windows domain, using the given password. The verbose log contains Connecting Connected using principal unravel_ngui.log userDNPattern guidKey Connected using DN Authorization Process These extra authorization steps are optional to further narrow who can access Unravel Server. The searches will be done using a bindDN account, if specified, otherwise the user account bind is used. If custom query is specified, the query is made, and results are checked for a match against the user name, if there is a match, login to Unravel is successful. If there are no results from the custom query, then login fails. Each search result will be matched with the simple name entered on the login page U. ser or group filters are ignored if a custom query is specified. If a user filter is specified, it is matched with the simple name entered on the login page. No additional LDAP search is done in this case. This allows access to Unravel to be controlled with an explicit list in Unravel. If a group pattern or filter is specified, it is checked. A query is made to find the groups to which a user belongs. The user membership list is scanned and if one of the groups is in the specified list of allowed groups, then this authorization step succeeds. The verbose log contains the resulting list and the match arguments in effect under Checking group Property Description Example Value or hive.server2.authentication.ldap.baseDN com.unraveldata.ldap.base LDAP base DN; use your rootDN value if a custom LDAP query is applied. Needed for Open LDAP. See also userDNPattern as alternative. DC=qa,DC=example,DC=com hive.server2.authentication.ldap.customLDAPQuery A full LDAP query that LDAP Atn provider uses to execute against LDAP Server. If this query returns a null resultset, the LDAP Provider fails the Authentication request, succeeds if the user is part of the resultset. If this property is set, filtering and group properties are ignored. (&(objectClass=group)(objectClass=top)(instanceType=4)(cn=Domain*)) (&(objectClass=person)(|(sAMAccountName=admin)(|(memberOf=CN=Domain Admins,CN=Users,DC=domain,DC=com) (memberOf=CN=Administrators,CN=Builtin,DC=domain,DC=com)))) or hive.server2.authentication.ldap.guidKey com.unraveldata.ldap.user_name_attr LDAP attribute name whose values are unique in this LDAP server. Default is uid uid or CN or sAMAccountName hive.server2.authentication.ldap.groupDNPattern Colon-separated list of patterns to use to find DNs for group entities in this directory. Use %s where the actual group name is to be substituted for. Each pattern should be fully qualified. Do not set ldap.Domain property to use this qualifier. CN=%s,CN=Groups,DC=subdomain,DC=domain,DC=com hive.server2.authentication.ldap.groupFilter Comma-separated list of LDAP Group names (short name not full DNs) HiveAdmins,HadoopAdmins,Administrators hive.server2.authentication.ldap.userDNPattern Colon-separated list of patterns to use to find DNs for users in this directory. Use %s CN=%s,CN=Users,DC=subdomain,DC=domain,DC=com hive.server2.authentication.ldap.userFilter Comma-separated list of LDAP usernames (just short names, not full DNs). , hiveuser impalauser hiveadmin hadoopadmin hive.server2.authentication.ldap.groupMembershipKey LDAP attribute name on the user entry that references a group that the user belongs to. Default is 'member'. , member uniqueMember memberUid hive.server2.authentication.ldap.groupClassKey LDAP attribute name on the group entry that is to be used in LDAP group searches. , group groupOfNames groupOfUniqueNames or hive.server2.authentication.ldap.url com.unraveldata.ldap.url The URL for the LDAP server. Can be multiple servers with a space separator. Standard port is used if unspecified. ldap:\/\/ LDAP_HOST ldaps:\/\/ LDAP_HOST ldap:\/\/ LDAP_HOST ldaps:\/\/ LDAP_HOST1 ldaps:\/\/ LDAP_HOST2 com.unraveldata.ldap.verbose Enables verbose logging. Search for Ldap \/usr\/local\/unravel\/logs\/unravel_ngui.log Can be true or false or not set; default is false. " }, 
{ "title" : "Enabling SAML Authentication for Unravel Web UI", 
"url" : "adv/adv-conf/security-configurations/adv-conf-security-enabling-saml-auth-for-unravel.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Enabling SAML Authentication for Unravel Web UI", 
"snippet" : "To use SAML you must configure both your Unravel Host and SAML server. Configure Unravel Host Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties. com.unraveldata.login.mode=saml com.unraveldata.login.saml.config=\/usr\/local\/unravel\/etc\/saml.json To use SAML with RBAC see Configu...", 
"body" : "To use SAML you must configure both your Unravel Host and SAML server. Configure Unravel Host Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties. com.unraveldata.login.mode=saml \ncom.unraveldata.login.saml.config=\/usr\/local\/unravel\/etc\/saml.json\n To use SAML with RBAC see Configure LDAP or SAML RBAC Properties Edit saml.config.json file Property Description Req Example Values entryPoint Identity provider entrypoint, Ping IdP address (SSO URL) Note: Identity provider entrypoint is required to be spec-compliant when the request is signed. Yes \"http:\/\/myHost:9080\/simplesaml\/saml2\/idp\/SSOService.php\" issuer Name of app that will connect to the saml server. Issuer string to supply to identity provider(Environment name). Should match the name configured in Idp. Yes \"unravel-myHost” cert IDP's public cert to validate auth response signature Note: You retrieve this from saml host. Yes Idp Cert String logoutUrl Base address to call with logout requests Default:entryPoint No \"http:\/\/myHost:9080\/simplesaml\/saml2\/idp\/SingleLogoutService.php\" logoutEnabled If true logs you out from every app. No false unravel_mapping Mapping saml auth response attributes to Unravel attributes. Yes { \"username\":\"userid\", \"groups\":\"ds_groups\" } privateCert Unravel private cert string to sign Auth requests. No Unravel cert string saml.json example {\nentryPoint\":\"http:\/\/myHost.unraveldata.com:9080\/simplesaml\/saml2\/idp\/SSOService.php\",\n\"issuer\":\"localhost\",\n \n\"logoutUrl\":\"http:\/\/myHost.unraveldata.com:9080\/simplesaml\/saml2\/idp\/SingleLogoutService.php\",\n \n\/\/ generate by saml host\n\"cert\":\"MIIDXTCCAkWgAwIBAgIJALmVVuDWu4NYMA0GCSqGSIb3DQEBCwUAMEUxCzAJBgNVBAYTAkFVMRMwEQYDVQQIDApTb21lLVN0YXRlMSEwHwYDVQQKDBhJbnRlcm5ldCBXaWRnaXRzIFB0eSBMdAcQf2CGAaVfwTTfSlzNLsF2lW\/ly7yapFzlYSJLGoVE+OHEu8g5SlNACUEfkXw+5Eghh+KzlIN7R6Q7r2ixWNFBC\/jWf7NKUfJyX8qIG5md1YUeT6GBW9Bm2\/1\/RiO24JTaYlfLdKK9TYb8sG5B+OLab2DImG99CJ25RkAcSobWNF5zD0O6lgOo3cEdB\/ksCq3hmtlC\/DlLZ\/D8CJ+7VuZnS1rR2naQ==\",\n \n\"privateCert\":\"-----BEGIN PRIVATE \/\/ generated by unravel node\n \nKEY-----\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDEt4Ma2k4DUkoW\\nG9QDHUBnY7S\/iS\/+u2BjPZqUG2JktzYZl30J05zA6i642i2VDn8eUIPHqt2Hw249\\nZ3nHKL4YnBVqa3yTfEkdMB\/6GSAkoCbnufaD3IsGcFJnlW5raDiT\/GZMy+1WnDfJ\\npB0\/.......vD8kRkcmEi9t3KLmKVy3SO15\/YHAhLxP9oTnTFGkPnIqZLRM0Y55UfwbRSZDlgH\/\\ny9GGmsV5IaIwhepuALJMdkHp\\n-----END PRIVATE KEY-----\\n\",\n \"unravel_mapping\":\n {\n \"username\":\"userid\",\n \"groups\":\"ds_groups\"\n } \n} For Ping, the IdP certificate can be obtained as follows: In the Server Configuration Certificate Management and Digital Signing & XML Decryption Keys & Certificates. Click Export Select Certificate Only Next Click Export Configure SAML Server Configure the following properties on the SAML server. Replace UNRAVEL_HOST Property Description Req PingFederate Specific configuration AssertionConsumerService \/ ACS Url http(s):\/\/ UNRAVEL_HOST Yes https:\/\/docs.pingidentity.com\/bundle\/p1_enterpriseEditAnApplication_cas\/page\/p1_t_EditASAMLApplication.html Entity Identifier unravel-Congo24 Yes Should be same as the issuer in saml.json Single Logout Endpoint http:\/\/ UNRAVEL_HOST Single Logout Response Endpoint http:\/\/ UNRAVEL_HOST No " }, 
{ "title" : "Enabling TLS to Unravel Web UI Directly", 
"url" : "adv/adv-conf/security-configurations/enabling-tls-to-unravel-web-ui-directly.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Enabling TLS to Unravel Web UI Directly", 
"snippet" : "Below we show how to directly enable TLS (SSL) to unravel_ngui Adding SSL and TLS to Unravel Web UI In this example, we stay on default port 3000 but change the protocol to HTTPS. We need SSL\/TLS certificate files accessible from unravel host. For more information, see Defining a Custom Web UI Port ...", 
"body" : "Below we show how to directly enable TLS (SSL) to unravel_ngui Adding SSL and TLS to Unravel Web UI In this example, we stay on default port 3000 but change the protocol to HTTPS. We need SSL\/TLS certificate files accessible from unravel host. For more information, see Defining a Custom Web UI Port On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties OPTION 1: Simple ssl config Update or add the following properties. For example, to enable ssl with minimal configuration #ENABLE\/DISABLE SSL com.unraveldata.ngui.ssl.enabled=true \n #PATH TO CERT FILE com.unraveldata.ngui.ssl.cert.file=\/etc\/certs\/wildcard_unravelhost_ssl_certificate \n #PATH TO KEY FILE com.unraveldata.ngui.ssl.key.file=\/etc\/certs \/wildcard_unravelhost_RSA_private.key \n #OPTIONAL - COMMA SEPARATED LIST OF CA FILES com.unraveldata.ngui.ssl.ca.files=\/etc\/certs\/IntermediateCA1.crt,\/etc\/certs\/IntermediateCA2.crt \n #OPTIONAL- PASSPHRASE IF NEEDED FOR KEY FILE com.unraveldata.ngui.ssl.passphrase=testp OPTION 2: Advanced ssl config: Update or add the following properties. For example, to enable SSL with advance configuration, update\/add these properties: #ENABLE\/DISABLE SSL com.unraveldata.ngui.ssl.enabled=true \n#PROVIDE SSL CONFIG THROUGH JS FILE FOR ADVANCE CONFIG\ncom.unraveldata.ngui.ssl.advance.config=\/usr\/local\/unrave\/etc\/advanced_unravel_ssl.js Content of advanced_unravel_ssl.js \/* advanced_unravel_ssl.js \n update below config variables \n SSL_KEY_FILE_PATH \n CA_CERT_FILE_PATH \n comment and uncomment the needed blocks \n *\/ \nconst fs = require('fs');\nconst constants = require('constants');\n\/* absolute path for ssl key file *\/\nconst SSL_KEY_FILE_PATH= '\/cert\/unravel_ssl.key'\n\/* absolute path for ssl cert file *\/\nconst SSL_CERT_FILE_PATH= '\/certunravel_ssl.crt'\n\/* absolute path for CA certs *\/\n\/* const CA_CERT_FILE_PATH=''*\/\nmodule.exports = {\nkey: fs.readFileSync(SSL_KEY_FILE_PATH),\npassphrase:'The password you gave when you created the key',\ncert: fs.readFileSync(SSL_CERT_FILE_PATH),\n\/\/ un comment below if using custom ca certs\n\/\/ ca : fs.readFileSync(CA_CERT_FILE_PATH),\n\/\/ uncomment below to enable disable TLS version.\n\/\/ secureOptions: constants.SSL_OP_NO_TLSv1 | constants.SSL_OP_NO_TLSv1_1,\n\/* LIST OF RECOMMENDED CIPHERS *\/\n\/* note OpenSSL-style format *\/\nciphers: ['TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384',\n'ECDHE_ECDSA_WITH_AES_128_GCM_SHA256',\n'ECDHE_ECDSA_WITH_AES_256_CBC_SHA384',\n'ECDHE_ECDSA_WITH_AES_256_CBC_SHA',\n'ECDHE_ECDSA_WITH_AES_128_CBC_SHA256',\n'ECDHE_ECDSA_WITH_AES_128_CBC_SHA',\n'ECDHE_RSA_WITH_AES_256_GCM_SHA384',\n'ECDHE_RSA_WITH_AES_128_GCM_SHA256',\n'ECDHE_RSA_WITH_AES_256_CBC_SHA384',\n'ECDHE_RSA_WITH_AES_128_CBC_SHA256',\n'ECDHE_RSA_WITH_AES_128_CBC_SHA'].join(':')\n} Set your advertised host in \/usr\/local\/unravel\/etc\/unravel.properties. This prefix will be used by Unravel server right after login or logout. com.unraveldata.advertised.url=https:\/\/unravel.example.com:3000 Restart Unravel web UI. # sudo service unravel_ngui restart " }, 
{ "title" : "Encrypting Passwords in Unravel Properties and Settings", 
"url" : "adv/adv-conf/security-configurations/encrypting-passwords-in-unravel-properties-and-settings.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Encrypting Passwords in Unravel Properties and Settings", 
"snippet" : "Unravel Server includes a command-line utility, pw_encrypt.sh Sample run of pw_encrypt.sh # sudo \/usr\/local\/unravel\/install_bin\/pw_encrypt.sh ... [Password:] The text you enter on the keyboard will not be displayed. After you press Enter Return ENC(gMJ5kx\/QioHJsum9rmqKROG0DRqbU51Z) This result, incl...", 
"body" : "Unravel Server includes a command-line utility, pw_encrypt.sh Sample run of pw_encrypt.sh # sudo \/usr\/local\/unravel\/install_bin\/pw_encrypt.sh\n...\n[Password:] The text you enter on the keyboard will not be displayed. After you press Enter Return ENC(gMJ5kx\/QioHJsum9rmqKROG0DRqbU51Z) This result, including the ENC() part, can be put into \/usr\/local\/unravel\/etc\/unravel.properties How it works The file \/usr\/local\/unravel\/etc\/entropy Passwords are redacted from diag or logs reports, but even if the encrypted form were accidentally transmitted or visible in an online meeting, because the entropy file is never included, the encrypted value would be impossible to decrypt. " }, 
{ "title" : "Kafka Security", 
"url" : "adv/adv-conf/security-configurations/kafka-security.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Kafka Security", 
"snippet" : "Table of Contents SSL+Kerberos for Kafka clients For Single Kafka clients For Multiple Kafka clients Kafka Authorizations References You can improve the Kafka cluster security by having kafkaauthenticate connections to brokers from client using either SSL or SASL. SSL+Kerberos for Kafka clients Prer...", 
"body" : " Table of Contents SSL+Kerberos for Kafka clients For Single Kafka clients For Multiple Kafka clients Kafka Authorizations References You can improve the Kafka cluster security by having kafkaauthenticate connections to brokers from client using either SSL or SASL. SSL+Kerberos for Kafka clients Prerequisite SSL+kerberos is supported by new Kafka consumers and producers. The configuration is same for consumer and producer. Replace items in red with values specific\/relevant to your environment. For Single Kafka clients Create a file named consumerConfig.properties. Add the following properties and copy\/move the file \/usr\/local\/unravel\/etc. You can locate youSLL + Kerbero configuration. ssl.protocol = TLSv1 sasl.mechanism = GSSAPI security.protocol = SASL_SSL sasl.kerberos.service.name = kafka ssl.truststore.location = \/usr\/java\/jdk1.7.0_67-cloudera\/jre\/lib\/security\/jssecacerts1 ssl.truststore.password = changeit ssl.truststore.type = JKS ssl.keystore.location = \/opt\/cloudera\/security\/jks\/server.keystore.jks ssl.keystore.password = password ssl.keystore.type = JKS ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1 sasl.jaas.config = com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ keyTab=\"\/etc\/keytabs\/kafka.keytab\" \\ principal=\"kafka\/edge-1.uddev.unraveldata.com@UDDEV.UNRAVELDATA.COM\" Kerberos configuration sasl.mechanism = GSSAPI security.protocol = SASL_PLAINTEXT sasl.kerberos.service.name = kafka sasl.jaas.config = com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ keyTab=\"\/etc\/keytabs\/kafka.keytab\" \\ principal=\"kafka\/edge-1.uddev.unraveldata.com@UDDEV.UNRAVELDATA.COM\"; Copy\/move consumerConfig.properties to \/usr\/local\/unravel\/etc. Edit \/usr\/local\/unravel\/unravel.properties. Search for com.unraveldata.ext.kafka.clusters. com.unraveldata.ext.kafka.clusters=ClusterName Add the following property using the ClusterName from above, com.unraveldata.ext.kafka.ClusterName.consumer.config=\/usr\/local\/unravel\/etc\/consumerConfig.properties Restart the kafka monitor daemon unravel_km. # service unravel_km restart For Multiple Kafka clients Each cluster must have a separate consumerConfig.properties Open \/usr\/local\/unravel\/unravel.properties. Search for com.unraveldata.ext.kafka.clusters. The property should be defined with a comma separated list. If there is only one cluster name see above com.unraveldata.ext.kafka.clusters=ClusterName1,ClusterName2,ClusterName3 Create a file named consumerConfigClusterName.properties for each cluster. ssl.protocol = TLSv1 sasl.mechanism = GSSAPI security.protocol = SASL_SSL sasl.kerberos.service.name = kafka ssl.truststore.location = \/usr\/java\/jdk1.7.0_67-cloudera\/jre\/lib\/security\/jssecacerts1 ssl.truststore.password = changeit ssl.truststore.type = JKS ssl.keystore.location = \/opt\/cloudera\/security\/jks\/server.keystore.jks ssl.keystore.password = password ssl.keystore.type = JKS ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1 sasl.jaas.config = com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ keyTab=\"\/etc\/keytabs\/kafka.keytab\" \\ principal=\"kafka\/edge-1.uddev.unraveldata.com@UDDEV.UNRAVELDATA.COM\" Copy\/move each file to \/usr\/local\/unravel\/etc. Edit \/usr\/local\/unravel\/unravel.properties. For each cluster add the following property. com.unraveldata.ext.kafka.ClusterName.consumer.config=\/usr\/local\/unravel\/etc\/consumerConfigClusterName.properties Restart the kafka monitor daemon unravel_km. # service unravel_km restart KafkaAuthorizations Unravel consumes message to topic __consumer_offsets UnravelOffsetConsumer SentryAuthorization The following privilege must be granted using sentry: HOST=*->CONSUMERGROUP=UnravelOffsetConsumer→action=read HOST=*->CONSUMERGROUP=UnravelOffsetConsumer→action=write HOST=*->CONSUMERGROUP=UnravelOffsetConsumer→action=describe HOST=*->TOPIC=__consumer_offsets→action=read HOST=*->TOPIC=__consumer_offsets→action=write HOST=*->TOPIC=__consumer_offsets->action=describe For further details see Using Kafka with Sentry Authorization Kafka with Ranger Authorization The following privilege must be granted using Ranger for the topic __consumer_offsets Publish Consume Describe For further details, see Security - Create a Kafka Polic References For further information see Apache Kafka documentation chapter # 7 Security. " }, 
{ "title" : "Restricting Direct Access to Unravel UI", 
"url" : "adv/adv-conf/security-configurations/restricting-direct-access-to-unravel-ui.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Restricting Direct Access to Unravel UI", 
"snippet" : "On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.ext.sh. # sudo vi \/usr\/local\/unravel\/etc\/unravel.ext.sh Add or modify NGUI_HOSTNAME. #SET NGUI_HOSTNAME export NGUI_HOSTNAME=127.0.0.1 Restart Unravel UI. # sudo service unravel_ngui restart...", 
"body" : " On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.ext.sh. # sudo vi \/usr\/local\/unravel\/etc\/unravel.ext.sh Add or modify NGUI_HOSTNAME. #SET NGUI_HOSTNAME \nexport NGUI_HOSTNAME=127.0.0.1 Restart Unravel UI. # sudo service unravel_ngui restart " }, 
{ "title" : "Using a Private Certificate Authority with Unravel", 
"url" : "adv/adv-conf/security-configurations/using-a-private-certificate-authority-with-unravel.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Security Configurations \/ Using a Private Certificate Authority with Unravel", 
"snippet" : "A private certificate authority (CA) is often used for signing certificates of non-public machines. Unravel server contains a bundled JRE that has well-known, public CAs. In order for Unravel to make REST requests to collect cluster metadata from HTTPS endpoints, it needs to know about your private ...", 
"body" : "A private certificate authority (CA) is often used for signing certificates of non-public machines. Unravel server contains a bundled JRE that has well-known, public CAs. In order for Unravel to make REST requests to collect cluster metadata from HTTPS endpoints, it needs to know about your private CA. Use one of the techniques below and restart all Unravel daemons with \"sudo \/etc\/init.d\/unravel_all.sh restart\" after making the change. HIGHLIGHTED \/path\/to\/jks_keystore Externally Managed JKS Keystore The bundled JRE will use an external keystore (jssecacerts) in preference over the built-in one (cacerts). Simply create a symlink as shown to your JKS keystore: # chmod 444 \/path\/to\/jks_keystore\n# ln -s {\/path\/to\/jks_keystore} \/usr\/local\/unravel\/jre\/lib\/security\/jssecacerts Note: Substitute \/path\/to\/jks_keystore Externally Managed JRE or JDK with curated cacerts An external JRE or JDK is often maintained for local use so that the cacerts \/usr\/local\/unravel\/etc\/unravel.ext.sh bin\/java \/usr\/java\/jdkl1.8 For example: export JAVA_HOME {\/usr\/java\/jdk1.8} Adding a CA Certificate to Bundled JRE You can add a CA certificate to the JRE that is bundled with Unravel server. First, copy cacerts jssecacerts # cd \/usr\/local\/unravel\/jre\/lib\/security\n# sudo cp -p cacerts jssecacerts List contents of the jssecacerts # sudo \/usr\/local\/unravel\/jre\/bin\/keytool -list -keystore jssecacerts Import\/insert a new certificate: Substitute your local values for {mycompanyca} {something.cer} # sudo \/usr\/local\/unravel\/jre\/bin\/keytool -keystore jssecacerts -importcert -alias {mycompanyca} -file {something.cer}\n# sudo \/usr\/local\/unravel\/jre\/bin\/keytool -list -keystore jssecacerts " }, 
{ "title" : "Autoscaling HDInsight Spark Cluster using Unravel API", 
"url" : "adv/adv-conf/adv-autoscaling-hdinsight-spark-cluster-with-un-api.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Autoscaling HDInsight Spark Cluster using Unravel API", 
"snippet" : "Prerequisites Install requests using pip. # pip install requests Install Azure CLI 1.0 (Azure CLI 2.0 does not support HDinsight cluster) Click here After install Azure CLI 1.0 Run the following command to login. # azure login Once you login to azure you should see existing HDinsight clusters using ...", 
"body" : "Prerequisites Install requests using pip. # pip install requests Install Azure CLI 1.0 (Azure CLI 2.0 does not support HDinsight cluster) Click here After install Azure CLI 1.0 Run the following command to login. # azure login Once you login to azure you should see existing HDinsight clusters using this command. # azure hdinsight cluster list Download the customizable script from https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/unravel-autoscaling\/unravel_HDInsight_autoscaling.py Open unravel_HDInsight_autoscaling.py Property Notes Example Value unravel_base_url http:\/\/localhost:3000\/ memory_threshold scale up\/down when memory_usage higher\/lower 80% 80 cpu_threshold scale up when cpu_usage higher\/lower 10% 10 min_nodes min worker nodes 4 max_nodes max worker nodes can scale up to 10 resource_group UNRAVEL01 cluster_name estspk2rh75` Run auto scaling script # python unravel_HDInsight_autoscaling.py Below is a screenshot ( Operations Dashboard " }, 
{ "title" : "Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"url" : "adv/adv-conf/adv-create-active-dir-kerberos-principal-keytabs.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Creating Active Directory Kerberos Principals and Keytabs for Unravel", 
"snippet" : "Define HOST Variable for Unravel Server as an FQDN (Replace UNRAVEL_HOST HOST= UNRAVEL_HOST Define the REALM Variable (Use upper case for all; replace EXAMPLEDOTCOM REALM= EXAMPLEDOTCOM Create the Active Directory (AD) Kerberos Principals and Keytabs Use the two variables you defined above to replac...", 
"body" : " Define HOST Variable for Unravel Server as an FQDN (Replace UNRAVEL_HOST HOST= UNRAVEL_HOST Define the REALM Variable (Use upper case for all; replace EXAMPLEDOTCOM REALM= EXAMPLEDOTCOM Create the Active Directory (AD) Kerberos Principals and Keytabs Use the two variables you defined above to replace the red text below. Verify that Unravel Server host is running ntpd For proper Kerberos operation with AD-KDC, DNS entries, including reverse DNS entries, must be in place. On AD server, logged in as AD Administrator, add 2 Managed Service Accounts unravel andhdfs: Open the Active Directory Users and Computers Confirm that the Managed Service Account REALM Right-click the Managed Service Account New->User Set names ( unravel hdfs Next Set a strong password to account (the password will not be used) and Check Password never expires check UN Password must be changed Check Password cannot be changed Right-click the created user, choose Properties Account In the Account Options Kerberos AES256-SHA1 On AD server, logged in as AD Administrator, create the Service Principal Names: Run these commands in a cmd or powershell console: setspn -A unravel\/HOSTunravel \nsetspn -A hdfs\/HOSThdfs On AD server, logged in as AD Administrator, generate keytab files that Unravel Server will use to authenticate with Kerberos using the ktpass utility in Active Directory: ktpass -princ unravel\/HOST@REALM-mapUser unravel -TargetREALM+rndPass -out unravel.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 \n ktpass -princ hdfs\/HOST@REALM-mapUser hdfs -TargetREALM+rndPass -out hdfs.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 Copy the two keytabs ( unravel.keytab hdfs.keytab \/etc\/keytabs\/ # sudo chmod 700 \/etc\/keytabs\/*\n# sudo chown unravel:unravel \/etc\/keytabs\/unravel.keytab\n# sudo chown hdfs:hdfs \/etc\/keytabs\/hdfs.keytab Assurances: hdfs.keytab " }, 
{ "title" : "Creating an AWS RDS CloudWatch Alarm for Free Storage Space", 
"url" : "adv/adv-conf/creating-an-aws-rds-cloudwatch-alarm-for-freestoragespace.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Creating an AWS RDS CloudWatch Alarm for Free Storage Space", 
"snippet" : "This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring. Go to AWS CloudWatch. Select on the left-hand corner tab for Alarms Click Create Alarm On the right-hand section under RDS Metrics Per-Database Metrics Under the column DBInstanceIdent...", 
"body" : "This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring. Go to AWS CloudWatch. Select on the left-hand corner tab for Alarms Click Create Alarm On the right-hand section under RDS Metrics Per-Database Metrics Under the column DBInstanceIdentifier Next In Alarm Threshold Name Description Add free storage of 20% left to alert contact under Whenever FreeStorageSpace 20 I have suggested adding 20% of free storage space left, however, you can tune this to be lower. Add 10 Under Actions Send notifications to This SNS topic should already be set up before you add it. Click Create Alarm The UI displays the alarm you just created in Alarms INSUFFICIENT DATA ALARM Alarms Click Create Alarm " }, 
{ "title" : "Enabling or Disabling Cluster Optimization Reports", 
"url" : "adv/adv-conf/enabling-or-disabling-cluster-optimization-reports.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Enabling or Disabling Cluster Optimization Reports", 
"snippet" : "This topic explains how to enable or disable Cluster Optimizationreports in Unravel UI. Required Settings On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default Security-Based Settings On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.pro...", 
"body" : "This topic explains how to enable or disable Cluster Optimizationreports in Unravel UI. Required Settings On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default Security-Based Settings On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default Settings for Sentry-Secured Clusters On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default Settings for Ranger-Secured Clusters On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default Settings for Kerberos-Secured Clusters On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default Optional Settings   If Unravel Server Has SSL Enabled   Disable Cluster Optimization Reports   " }, 
{ "title" : "Enabling or Disabling Small Files Reports and Files Reports", 
"url" : "adv/adv-conf/enabling-or-disabling-small-files-reports-and-files-reports.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Enabling or Disabling Small Files Reports and Files Reports", 
"snippet" : "This topic explains how to enable or disable Small Files Reports and Files Reports in Unravel UI. Small File Reports and File Reports features are enabled by default. To toggle the status, on the Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties and Property Description Set By User Unit...", 
"body" : "This topic explains how to enable or disable Small Files Reports and Files Reports in Unravel UI. Small File Reports and File Reports features are enabled by default. To toggle the status, on the Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties and Property Description Set By User Unit Default unravel.python.reporting.files.disable This flag disables the the the small files and files report feature. false: small files and files report are generated true: small files and files report features is disabled. Optional boolean false Required Settings On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default unravel.hive.server2.host Hive Server2 fully qualified hostname Optional string localhost unravel.hive.server2.port Hive Server2 port Optional integer 10000 Security-Based Settings Settings for Kerberos-Secured Clusters On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default unravel.hive.server2.authentication Authentication mechanism to connect to Hive Server2 Required string - unravel.hive.server2.kerberos.service.name Hive Server 2 Kerberos service name Required string - Settings for Sentry-Secured CDH Clusters (With or Without Kerberos) If your CDH cluster is secured with Sentry, Unravel's Small Files and Files reports (on the Data Insights tab) Grant dfsadmin or this alternative To grant this privilege, set\/update the HDFS configuration property dfs.cluster.administrators If you can't grant dfsadmin privilege to the Unravel user, follow the steps in the Troubleshooting Triggering an import of FSImage Remove the following line from \/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/scripts\/hive_queries_reporting\/hive_properties.hive ADD JAR {UDF_JAR_LOC}\/unravel-udf-0.1.jar This line is no longer required because UDFs jars are stored locally on the HiveServer2 node in Sentry-secured CDH clusters. Allow the Unravel user to submit Hive queries to a YARN queue. If you can't allow this on your default YARN queue, you can grant this permission on a different YARN queue: Create a different YARN queue for the Unravel user. Give the Unravel user permission to submit Hive queries to the new YARN queue. In unravel.properties unravel.python.reporting.files.hive_mr_queue Create a new Sentry role, unravel_role beeline create role unravel_role Map the unravel unravel_role grant role unravel_role to group unravel Set HDFS access privileges for the Unravel user: The Unravel user needs to copy FSImage to \/tmp\/fsimage grant all on uri 'hdfs:\/\/\/tmp\/fsimage' to role unravel_role; Grant the Unravel user the following privileges on the Hive tables under the default Create\/drop\/truncate\/alter Hive tables Run\/select\/insert queries on Hive tables Alternatively, you can Use a different database for the Unravel user, such as unravel_db Give the Unravel user the above permissions on that database: grant all on database unravel_db to role unravel_role; In unravel.properties unravel.python.reporting.files.hive_database For example, unravel.python.reporting.files.hive_database=unravel_db Add a JAR and create temporary UDFs: As the Unravel user, copy the Unravel JAR, \/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/jars\/small_files\/unravel-udf-0.1.jar The HiveServer2 aux JAR path is specified by the Hive Auxillary Jars directory (in a Hive Service wide configuration) OR by hive.reloadable.aux.jars.path Hive user and group should own this JAR. For example, if the HiveServer2 aux JAR path is \/tmp\/hive_jars, this directory and the copied jar must be owned by hive admin user ( hive chown -R hive:hive \/tmp\/hive_jars Grant the Unravel user access to this JAR: grant all on uri 'file:\/\/\/tmp\/hive_jars\/' to role unravel_role; In Cloudera Manager, restart HiveServer2. Use the show grant role unravel_role +--------------------------------------------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n\n| database | table | partition | column | principal_name | principal_type | privilege | grant_option | grant_time | grantor |\n\n+--------------------------------------------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n\n| file:\/\/\/tmp\/hive_jars\/unravel-udf-0.1.jar | | | | unravel_role | ROLE | * | false | 1550829915318000 | -- |\n\n| unravel_db | | | | unravel_role | ROLE | * | false | 1550829820331000 | -- |\n\n| hdfs:\/\/\/tmp\/fsimage | | | | unravel_role | ROLE | * | false | 1550830532328000 | -- |\n\n+--------------------------------------------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n\n Settings for Ranger-Secured HDP Clusters (With or Without Kerberos) If your HDP cluster is secured with Ranger, Unravel's Small Files and Files reports (on the Data Insights tab) Grant dfsadmin or this alternative For example: Allow the Unravel user to connect to HiveServer2. Allow the Unravel user to CREATE, TRUNCATE, ALTER, DROP, INSERT, and SELECT Hive tables. For example: Allow the Unravel user to change or switch the Hive database. For example: Allow the Unravel user to submit Hive queries to a particular YARN queue. For example: Allow the Unravel user to use concurrent hive queries: Set hive.txn.manager=DbTxnManager hive.support.concurrency=true Allow the Unravel user to do the following actions dynamically: Set the following parameters: hive.auto.convert.join\nhive.support.concurrency\nhive.support.sql11.reserved.keywords\nhive.txn.manager\nhive.variable.substitute\nmapred.job.queue.name\nmapreduce.map.java.opts\nmapreduce.map.memory.mb Add JAR and create temporary functions from UDFs in this JAR. Settings for SSL-Enabled Systems Currently not supported. " }, 
{ "title" : "Enabling or Disabling Cloud Reports and Forecasting Reports", 
"url" : "adv/adv-conf/enabling-or-disabling-cloud-reports-and-forecasting-reports.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Enabling or Disabling Cloud Reports and Forecasting Reports", 
"snippet" : "Cloud Reports and Forecasting Reports share the same configuration; therefore, changing any property affects both reports. There is no enable\/disable flag for these reports; to enable them you must configure the properties below. Un-setting (setting to nil, removing, or commenting out) the propertie...", 
"body" : " Cloud Reports and Forecasting Reports share the same configuration; therefore, changing any property affects both reports. There is no enable\/disable flag for these reports; to enable them you must configure the properties below. Un-setting (setting to nil, removing, or commenting out) the properties disables the report. On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default com.unraveldata.cluster.type Possible values are HDP, CDH, MAPR Required String - com.unraveldata.cluster.name Cluster to connect to if multiple options exist. Required in multi-cluster environments only. Will first attempt to match on the cluster ID, and then fall-back to matching on the display name. For Ambari, the cluster ID and the display name are equivalent, which is the \"cluster_name\" attribute from the \"\/clusters\" endpoint. E.g,. http:\/\/HOST:8080\/api\/v1\/clusters\/ For Cloudera Manager, the cluster id is the \"name\" attribute from the \"\/clusters\" endpoint. E.g., http:\/\/HOST:7180\/api\/v17\/clusters\/ Required String - The following properties are defined by Cluster tool ManagerName Be sure to only set the properties for your cluster manager tool, which is either \"ambari\" or \"cloudera\" Property Description Set by User Unit Default com.unraveldata. ManagerName URL of Cluster Manager, e.g., http:\/\/$clouderaserver:7180, https:\/\/$ambariserver:8443 If the Cloudera URL does not contain a port you must define port below. Required URL - string - com.unraveldata. ManagerName user name for the manger Required string - com.unraveldata. ManagerName password Required string - com.unraveldata.cloudera.manager.port This is required only Required number - com.unraveldata.cloudera.manager.api_version Optional and only valid for Cloudera Manager in order to override the API version number to use, such as \"17\" Optional number - Security-Based Settings On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default com.unraveldata. ManagerName If SSL is enabled, then make sure to change the URL to use HTTPS and that the port is correct. Cloudera Manager uses SSL port 7183 Ambari uses SSL port 8443 Required URL - Settings for Sentry-Secured Clusters None Settings for Ranger-Secured Clusters None Settings for Kerberos-Secured Clusters Currently not Supported " }, 
{ "title" : "Enabling or Disabling Queue Analysis Reports", 
"url" : "adv/adv-conf/enabling-or-disabling-queue-analysis-reports.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Enabling or Disabling Queue Analysis Reports", 
"snippet" : "This topic explains how to enable or disable Queue Analysisreports in Unravel UI. The Queue Analysis Reports feature is enabled by default. To toggle the status, on the Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default com.unraveldata.report...", 
"body" : "This topic explains how to enable or disable Queue Analysisreports in Unravel UI. The Queue Analysis Reports feature is enabled by default. To toggle the status, on the Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default com.unraveldata.report.queue.metrics.sensor.enabled Enables or disables queue metric sensor. bool true Security-Based Settings None. Settings for Sentry-Secured Clusters None. Settings for Ranger-Secured Clusters None. Settings for Kerberos-Secured Clusters None. If Unravel Server Has SSL Enabled None. Optional Settings On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default com.unraveldata.report.queue.poll.interval.msec How often queue metric sensor polls polls YARN Resource Manager. ms 60000 com.unraveldata.report.queue.http.timeout.msec* YARN Resource Manager HTTP connection timeout. ms 10000 com.unraveldata.report.queue.http.retries* YARN Resource Manager HTTP connection retries. count 3 com.unraveldata.report.queue.http.retry.period.msec* YARN Resource Manager HTTP connection retry wait period. ms 0 unravel.python.queueanalysis.metrics.scale UI rendered graph metrics scale factor. number 1000 unravel.python.queueanalysis.daterange.span UI report date picker range. days 30 * - obsolete since 4.5.0.4. " }, 
{ "title" : "Enabling or Disabling Sessions", 
"url" : "adv/adv-conf/enabling-or-disabling-sessions.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Enabling or Disabling Sessions", 
"snippet" : "This topic explains how to enable or disable Sessionsin Unravel UI. Sessions are enabled by default. To toggle the status, on the Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default com.unraveldata.session.enabled Switch to enable\/disable sess...", 
"body" : "This topic explains how to enable or disable Sessionsin Unravel UI. Sessions are enabled by default. To toggle the status, on the Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties Property Description Set By User Unit Default com.unraveldata.session.enabled Switch to enable\/disable session functionality from Unravel false: disables Sessions true: disabled No Boolean true Security-Based Settings N\/A Settings for Sentry-Secured Clusters N\/A Settings for Ranger-Secured Clusters N\/A Settings for Kerberos-Secured Clusters N\/A If Unravel Server Has SSL Enabled N\/A Optional Settings Property Description Set By User Unit Default com.unraveldata.session.max.autotune.runs Maximum runs session is allowed. Users can't exceed this value Yes Integer 8 " }, 
{ "title" : "Triggering an import of FSImage", 
"url" : "adv/adv-conf/triggering-an-import-of-fsimage.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configurations \/ Triggering an import of FSImage", 
"snippet" : "The etl_fsimage unravel_hdfs_fsimage_master_orc etl_fsimage UTC However, there may be times when you want to import FSImage immediately, such as after Unravel Server is installed or upgraded. In this case, you have to start etl_fsimage curl -v http:\/\/localhost:5000\/small-files-etl This script ensure...", 
"body" : "The etl_fsimage unravel_hdfs_fsimage_master_orc etl_fsimage UTC However, there may be times when you want to import FSImage immediately, such as after Unravel Server is installed or upgraded. In this case, you have to start etl_fsimage curl -v http:\/\/localhost:5000\/small-files-etl This script ensures that the latest FSImage is incorporated in Unravel's Small Files\/File Reports. The etl_fsimage etl_fsimage FSImage Size etl_fsimage 19 GB 24 hours 9 GB 14 hours 4 GB 7 hours Troubleshooting If etl_fsimage [2018-09-10 23:11:57,357: WARNING\/ForkPoolWorker-1]* stderr: sudo: hdfs: command not found* In this case, do the following: Fetch the FSImage as a user with dfadmin privileges using the commands rm -rf unravel_node_fsimage_dir\/* hdfs dfsadmin -fetchImage unravel_node_fsimage_dir These commands delete all existing FSImage files and then copy the latest FSImage into the directory you specify ( unravel_node_fsimage_dir The directory unravel_node_fsimage_dir must \/srv\/unravel\/tmp\/reports\/fsimage and it should be readable by unravel user. Best practice is to run these commands in a cron job that completes before Unravel's etl_fsimage task is triggered every day at 00:00 UTC. Configure Unravel OnDemand to access FSImage from unravel_node_fsimage_dir unravel.properties unravel.python.reporting.files.skip_fetch_fsimage=true unravel.python.reporting.files.external_fsimage_dir=unravel_node_fsimage_dir Note: unravel_node_fsimage_dir. Restart the Unravel OnDemand daemon. rm -rf unravel_node_fsimage_dir\/* hdfs dfsadmin -fetchImage unravel_node_fsimage_dir Unravel OnDemand assumes the FSImage filename starts with fsimage not .txt " }, 
{ "title" : "Configure JVM Sensor", 
"url" : "adv/adv-configure-jvm-sensor.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configure JVM Sensor", 
"snippet" : "CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR) HDP Enable JVM Sensor Cluster-Wide for MapReduce2 (MR)...", 
"body" : " CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR) HDP Enable JVM Sensor Cluster-Wide for MapReduce2 (MR) " }, 
{ "title" : "CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR)", 
"url" : "adv/adv-configure-jvm-sensor/adv-conf-jvm-sensor-cdh-clusterwide-mr.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configure JVM Sensor \/ CDH Enable JVM Sensor Cluster-Wide for MapReduce (MR)", 
"snippet" : "Before following these instructions, follow the steps in Step 2: Install Unravel Sensor and Configure Impala In Cloudera Manager (CM) go to YARN service. Select the Configuration Search for Application Master Java Opts Base Make sure that \"-\" is a minus sign. You need to modify the value of UNRAVEL_...", 
"body" : "Before following these instructions, follow the steps in Step 2: Install Unravel Sensor and Configure Impala In Cloudera Manager (CM) go to YARN service. Select the Configuration Search for Application Master Java Opts Base Make sure that \"-\" is a minus sign. You need to modify the value of UNRAVEL_HOST_IP -javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043 Search for MapReduce Client Advanced Configuration Snippet (Safety Valve) mapred-site.xml Enter following xml four block properties snippet to Gateway Default Group View as XML <property>\n<name>mapreduce.task.profile<\/name>\n<value>true<\/value>\n<\/property> \n<property>\n<name>mapreduce.task.profile.maps<\/name>\n<value>0-5<\/value>\n<\/property> \n<property>\n<name>mapreduce.task.profile.reduces<\/name>\n<value>0-5<\/value>\n<\/property> \n\/\/ this is one line \n<property><name>mapreduce.task.profile.params<\/name><value>-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport={UNRAVEL_HOST_IP}:4043<\/value><\/property> Save the changes. Deploy the client configuration by clicking the deploy glyph ( Actions Cloudera Manager will specify a restart which is not necessary to effect these changes. (Click Restart Stale Services Use the Unravel UI to monitor the situation. When you view the MapReduce APM Resource Usage " }, 
{ "title" : "HDP Enable JVM Sensor Cluster-Wide for MapReduce2 (MR)", 
"url" : "adv/adv-configure-jvm-sensor/hdp-enable-jvm-sensor-cluster-wide-for-mapreduce2--mr-.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Configure JVM Sensor \/ HDP Enable JVM Sensor Cluster-Wide for MapReduce2 (MR)", 
"snippet" : "must be a fully qualified DNS or an IP address. UNRAVEL_HOST_IP Upon completion you must In AWU, on the left-hand side, click MapReduce2 Configs Advanced Advanced mapred-site Search for MR AppMaster Java Heap Size current.yarn.app.mapreduce.am.command-opts -javaagent:\/usr\/local\/unravel-agent\/jars\/bt...", 
"body" : " must be a fully qualified DNS or an IP address. UNRAVEL_HOST_IP Upon completion you must In AWU, on the left-hand side, click MapReduce2 Configs Advanced Advanced mapred-site Search for MR AppMaster Java Heap Size current.yarn.app.mapreduce.am.command-opts -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP On the top notification banner, click Save In AWU, on the left-hand side, click MapReduce2 Configs Advanced Custom mapred-site Inside Custom mapred-site Add Property On the top notification banner, click Save You can manually edit \/etc\/hadoop\/conf\/mapred-site.xml -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP Propagate the Unravel resource metrics sensor JAR onto all the servers in the cluster. If you have already run the unravel_hdp_setup.sh Open an SSH session to the Unravel gateway host server and use guided steps to unzip the Unravel MR JARs. With root or sudo access, change directory to \/usr\/local\/unravel-agent # cd \/usr\/local\/unravel-agent\n# curl http:\/\/localhost:3000\/hh\/unravel-agent-pack-bin.zip -o unravel-agent-pack-bin.zip\n# unzip -d jars unravel-agent-pack-bin.zip Ensure you have already installed unzip curl Create a .tar file of the \/usr\/local\/unravel-agent # cd \/usr\/local\/\n# tar -cvf unravel-agent.tar .\/unravel-agent Copy the unravel-agent.tar untar \/usr\/local untar unravel-agent Restart of all affected HDFS, MAPREDUCE2, YARN and HIVE services in Ambari UI. " }, 
{ "title" : "Connectivity", 
"url" : "adv/adv-connectivity.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Connectivity", 
"snippet" : "Connecting to\/Configuration of a Kafka Stream Hive Metastore Access...", 
"body" : " Connecting to\/Configuration of a Kafka Stream Hive Metastore Access " }, 
{ "title" : "Connecting to\/Configuration of a Kafka Stream", 
"url" : "adv/adv-connectivity/adv--connectivity-connect-config-kafka-stream.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Connectivity \/ Connecting to\/Configuration of a Kafka Stream", 
"snippet" : "To connect Unravel Server to a Kafka stream (topic), you must change the configuration of the Kafka cluster and add some properties to unravel.properties Export an available port for JMX_PORT. You need to export one port for each Kafka server. export JMX_PORT= port_num The default JMX port for kafka...", 
"body" : "To connect Unravel Server to a Kafka stream (topic), you must change the configuration of the Kafka cluster and add some properties to unravel.properties Export an available port for JMX_PORT. You need to export one port for each Kafka server. export JMX_PORT= port_num The default JMX port for kafka in CDH is 9393. In HDP you would export this parameter under Advanced kafka-env kafka-env template Enable remote access for JMX monitoring by appending the following lines to KAFKA_JMX_OPTS in kafka_run_class.sh: -Dcom.sun.management.jmxremote.rmi.port=$JMX_PORT\n-Djava.rmi.server.hostname=127.0.0.1\n-Djava.net.preferIPv4Stack=true Not required for HDP. Verify the configuration changes on the Kafka cluster. Restart the Kafka broker. Configure Unravel Server to monitor the Kafka cluster. The unravel daemon, unravel_km, Kafka Monitoring Properties In \/usr\/local\/unravel\/etc\/unravel.properties RED com.unraveldata.ext.kafka.clusters \ncom.unraveldata.ext.kafka. ClusterID ClusterID ClusterID JMX_ServerID ClusterID JMX_ServerID Example: com.unraveldata.ext.kafka.clusters=c1,c2 \ncom.unraveldata.ext.kafka.c1.bootstrap_servers=localhost:9092,localhost:9093 \ncom.unraveldata.ext.kafka.c2.bootstrap_servers=localhost:9192,localhost:9193 \ncom.unraveldata.ext.kafka.c1.jmx_servers=kafka-test1,kafka-test2 \ncom.unraveldata.ext.kafka.c2.jmx_servers=kafka-test1,kafka-test2 \ncom.unraveldata.ext.kafka.c1.jmx.kafka-test1.host=localhost \ncom.unraveldata.ext.kafka.c1.jmx.kafka-test2.host=localhost \ncom.unraveldata.ext.kafka.c2.jmx.kafka-test1.host=localhost \ncom.unraveldata.ext.kafka.c2.jmx.kafka-test2.host=localhost \ncom.unraveldata.ext.kafka.c1.jmx.kafka-test1.port=5005 \ncom.unraveldata.ext.kafka.c1.jmx.kafka-test2.port=5010 \ncom.unraveldata.ext.kafka.c2.jmx.kafka-test1.port=5105 \ncom.unraveldata.ext.kafka.c2.jmx.kafka-test2.port=5110 " }, 
{ "title" : "Hive Metastore Access", 
"url" : "adv/adv-connectivity/adv-connectivity-hive-access-connect-metastore.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Connectivity \/ Hive Metastore Access", 
"snippet" : "Enabling Hive Metastore Access in Unravel requires the following steps: Step 1: Gather Hive Metastore details Step 2: Configuring Unravel to access Hive Metastore...", 
"body" : "Enabling Hive Metastore Access in Unravel requires the following steps: Step 1: Gather Hive Metastore details Step 2: Configuring Unravel to access Hive Metastore " }, 
{ "title" : "Connecting to a Hive Metastore", 
"url" : "adv/adv-connectivity/adv-connectivity-hive-access-connect-metastore/adv-connectivity-hive-access-connect-to-metastore-.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Connectivity \/ Hive Metastore Access \/ Connecting to a Hive Metastore", 
"snippet" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data For CDH+CM To enable instrumentation for the Hive metastore you need to obtain Hive metastore details from Cloudera Manager. Obtain the Hive metastore details from th...", 
"body" : "Connecting to a Hive metastore enables Unravel Server to collect instrumentation from it, which results in the population of the Data For CDH+CM To enable instrumentation for the Hive metastore you need to obtain Hive metastore details from Cloudera Manager. Obtain the Hive metastore details from the Cloudera Manager by using a CDH REST API. From CDH version 5.5 onward, use the REST API http:\/\/CMGR_HOSTNAME_IP:7180\/api\/v12\/cm\/deployment Look at the response body, a JSON-like text format as in the image below. Search the response body for metastore Edit \/usr\/local\/unravel\/etc\/unravel.properties See Hive Metastore Configuration for information. FIXLINK. Locate and edit the following properties. If necessary, add the properties. Substitute your local values. javax.jdo.option.ConnectionURL=hive_metastore_database_host ;javax.jdo.option.ConnectionDriverName=org.mariadb.jdbc.Driver \njavax.jdo.option.ConnectionPassword=hive_metastore_database_password ; \njavax.jdo.option.ConnectionUserName=hive_metastore_database_user ; Restart Unravel Server. # sudo \/etc\/init.d\/unravel_all.sh restart After restart, confirm that Hive queries appear in Unravel UI in Applications Applications For HDP and MapR Please contact your cluster administrator. " }, 
{ "title" : "Hive Metastore Configuration", 
"url" : "adv/adv-connectivity/adv-connectivity-hive-access-connect-metastore/hive-metastore-configuration.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Connectivity \/ Hive Metastore Access \/ Hive Metastore Configuration", 
"snippet" : "In order for the Reports | Data Insights must Unravel no longer bundles the MySql Driver JARs. If you are using MySql as the Hive Metastore DB, you have two options: Use the mariadb Download the MySql JDBC connector JAR from: https:\/\/dev.mysql.com\/downloads\/connector\/j\/ \/usr\/local\/unravel\/dlib\/mybat...", 
"body" : "In order for the Reports | Data Insights must Unravel no longer bundles the MySql Driver JARs. If you are using MySql as the Hive Metastore DB, you have two options: Use the mariadb Download the MySql JDBC connector JAR from: https:\/\/dev.mysql.com\/downloads\/connector\/j\/ \/usr\/local\/unravel\/dlib\/mybatis\/ All parameters are defined in \/usr\/local\/unravel\/etc\/unravel.properties Hive Metastore Access You must configure the following properties for the Data Insights tab to populate its information correctly. Property Definition Example Default javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store containing the metadata. postgresql org.postgresql.Driver - mariadb org.mariadb.jdbc.Driver MySql org.mysql.jdbc.Driver javax.jdo.option.ConnectionPassword Password used to access the data store. OIhwSFl - javax.jdo.option.ConnectionUserName Username used to access the data store. hive - javax.jdo.option.ConnectionURL JDBC connection string for the data store containing the metadata of the form: jdbc: DB_Driver HOST : PORT postgresql jdbc:postgresql:\/\/congo21.unraveldata.com:7432\/hive - JDBC Configurations You may optionally configure the following properties to manage the Hive Metastore JDBC connection pooling. Unravel uses the c3p0 library to manage the pooling. Property Definition Default com.unraveldata.metastore.db.c3p0.acquireRetryAttempts Controls how many times c3p0 tries to obtain an connection from the database before giving up. 30 com.unraveldata.metastore.db.c3p0.acquireRetryDelay Controls how much waiting time is between each retry attempts in milliseconds. 1000 com.unraveldata.metastore.db.c3p0.breakafteracquirefailure Allows you to mark data source as broken and permanently be closed if a connection cannot be obtained from database. The default value is false False com.unraveldata.metastore.db.c3p0.maxconnectionage The maximum number of seconds any connections were forced to be released from the pool. If the default value (0) is usedthe connections will never be released. 0 com.unraveldata.metastore.db.c3p0.maxidletimeexcessconnections The number of seconds that connections are permitted to remain idle in the pool before being released. If the default value (0) is usedthe connections will never be released. 0 com.unraveldata.metastore.db.c3p0.maxpoolsize The maximum connections in the connection pool. 5 Reference [1] c3p0 project page: http:\/\/www.mchange.com\/projects\/c3p0 " }, 
{ "title" : "How to Write Jolokia JMX MBean", 
"url" : "adv/how-to-write-jolokia-jmx-mbean.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ How to Write Jolokia JMX MBean", 
"snippet" : "To introduce a new Jolokia JMX MBean you must: Write the interface annotated with @MXBean While not required, it is good practice for the name to have the suffix MXBean Write the class to implement the interface. The class must be annotated using @JsonMBean @Singleton @AutoService JolokiaMBean.class...", 
"body" : "To introduce a new Jolokia JMX MBean you must: Write the interface annotated with @MXBean While not required, it is good practice for the name to have the suffix MXBean Write the class to implement the interface. The class must be annotated using @JsonMBean @Singleton @AutoService JolokiaMBean.class MBean can be injected by google Guice and used by Unravel daemon (or by other java classes). MBean must be thread safe The Unravel daemon (typically for write access) The Jolokia agent (typically for read access) The Jolokia MBean is singleton The Jolokia MBean interface has one mandatory method: which must return an unique MBean ObjectName getName() Jolokia MBean can contain also operations. Operations are methods that can be called remotely on a MBean. They may: Trigger some action on an unravel daemon Have any number of parameters Return any supported type Jolokia supports following MBean attribute types: Primitive types and their object equivalents List, Set, and Map types POJO type composed of types mentioned above and which can be nested Objects of other types have to be converted to supported types, for example, to string Use the helper class com.unraveldata.jmx.Converter " }, 
{ "title" : "Example", 
"url" : "adv/how-to-write-jolokia-jmx-mbean.html#UUID-95179eee-0e3f-fc1a-9f38-47f5239772a5_id_HowtoWriteJolokiaJMXMBean-Example", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ How to Write Jolokia JMX MBean \/ Example", 
"snippet" : "This example shows how to create and use the MBean, DbStatusMXBean. Interface DbStatusMXBean @MXBean public interface DbStatusMXBean extends JolokiaMBean { boolean isConnectionOk(); } Class DbStatusMBean @JsonMBean @Singleton @AutoService(JolokiaMBean.class) public class DbStatusMBean implements DbS...", 
"body" : "This example shows how to create and use the MBean, DbStatusMXBean. Interface DbStatusMXBean @MXBean\npublic interface DbStatusMXBean extends JolokiaMBean {\n\n boolean isConnectionOk();\n} Class DbStatusMBean @JsonMBean\n@Singleton\n@AutoService(JolokiaMBean.class)\npublic class DbStatusMBean implements DbStatusMXBean {\n private static final String JMX_NAME = \"com.unraveldata:type=Monitoring,group=Database,name=DbStatus\";\n\n private boolean isConnectionOk;\n private MBeanStatus mBeanStatus;\n\n public synchronized void setConnectionOk(boolean connectionOk) {\n this.isConnectionOk = connectionOk;\n this.mBeanStatus = new MBeanStatus(LocalDateTime.now());\n }\n\n @Override\n public synchronized boolean isConnectionOk() {\n return isConnectionOk;\n }\n\n @Override\n public synchronized MBeanStatus getMBeanStatus() {\n return mBeanStatus;\n }\n\n @Override\n public String getName() {\n return JMX_NAME;\n }\n} Usage DbStatusMonitor @Singleton\npublic class DbStatusMonitor {\n\n private final DbStatusMBean mBean;\n\n @Inject\n public DbStatusMonitor(DbStatusMBean mBean) {\n this.mBean = mBean;\n }\n\n void execute(Connection sqlConnection) {\n boolean isConnectionOk = testConnection();\n mBean.setConnectionOk(isConnectionOk);\n }\n} " }, 
{ "title" : "Deploying Unravel over SELinux", 
"url" : "adv/how-to-write-jolokia-jmx-mbean/adv-conf-deploying-unravel-over-selinux.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ How to Write Jolokia JMX MBean \/ Deploying Unravel over SELinux", 
"snippet" : "Security Enhanced Linux (SELinux) allows you to set access control through SELinux policies. SELinux Modes : In this mode the SELinux polices and rules are strictly enforced and applied over the subjects and object. All production systems have SELinux enabled in enforcing mode. The policies are enfo...", 
"body" : "Security Enhanced Linux (SELinux) allows you to set access control through SELinux policies. SELinux Modes : In this mode the SELinux polices and rules are strictly enforced and applied over the subjects and object. All production systems have SELinux enabled in enforcing mode. The policies are enforced whenever any violations or errors are detected and the violations\/errors are logged. Enforcing : The policies and rules of SELinux are applied over the subjects and objects but are not enforced. All violations and errors based on the SELinux policy are ignored and logged into the log files. If the SELinux policy prevents a specific service from accessing a specific folder, this mode allows access but logs a denial message. This mode provides enough debugging information to fine tune the SELinux Policy so it runs smoothly in enforcing mode. Permissive : No policies are enforced. Disabled SELinux Policy Unravel currently only supports the targeted Prerequisites Enable SELinux on Unravel Node running Linux. Open \/etc\/sysconfig\/selinux Set the SELinux mode. This is SELinux's default; whenever the system reboots it starts SELinux in this mode. See Switching Modesfor how to change the mode while running. SELINUX=enforcing Use the default policy, targeted SELINUXTYPE=targeted Reboot the system to effect the changes. # getenforce\nenforcing Verify the mode setting after reboot. Installing Unravel Core RPMs on a Node with SELinux Install Unravel in permissive mode or enforcing mode. You can install Unravel in either mode. However, installing Unravel in enforcing mode is highly discouraged since SELinux issues a warning regarding uncertainty of functionality. Installing in Permissive Mode (Recommended) Set mode to permissive # setenforce 0\n# getenforce\npermissive Install Unravel using rpm # sudo rpm -Uvv unravel-<version>.x86_64.rpm 2> \/tmp\/rpm.txt\n# sudo \/usr\/local\/unravel\/install_bin\/await_fixups.sh\n SELinux may generate similar alerts during the installation process depending on the environment. But this should not hinder with the installation process. # sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. Installing in Enforcing Mode (Highly Discouraged) When Unravel is installed in enforcing Execute getenforce enforcing Step b # getenforce\nenforcing Install Unravel using rpm # sudo rpm -Uvv unravel-<version>.x86_64.rpm 2> \/tmp\/rpm.txt\n# sudo \/usr\/local\/unravel\/install_bin\/await_fixups.sh\n The rpm installation sets SELINUX permissive -----RPM installation log\n+ setenforce Permissive\n+ echo\n+ tee_echo '[CREATE_B1: SECURITY: WARNING] Setting selinux to be temporarily Permissive; after a reboot it might revert to Enforced and Unravel functionality might be an issue.'\n+ tee -a \/tmp\/rpm_upgrade.log\n++ date '+%Y-%m-%d %H:%M:%S'\n + echo '[2019-01-28 06:33:17] [CREATE_B1: SECURITY: WARNING] Setting selinux to be temporarily Permissive; after a reboot it might revert to Enforced and Unravel functionality might be an issue.' \n+ echo\n+ FILE_CACHE_HEADROOM_MB=2000\n----- # getenforce\npermissive SELinux generates two alerts like the ones below. Similiar alerts are generated throughout the installation process. # sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. Switch to user. There should be no alerts at this stage. Set SELINUX enforcing # setenforce 1\n# getenforce\nenforcing Run the script switch_to_user.sh X Y switch_to_user # sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh X Y Start Unravel services after RPM installation. Run the following command to make sure all services start up successfully. # sudo \/etc\/init.d\/unravel_all.sh start SELinux generates two alerts. Similar alerts are generated throughout the installation process. # sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. Verify SELINUX is set to enforcing. # getenforce\nenforcing If getenforce permissive # sudo \/etc\/init.d\/unravel_all.sh stop\n# setenforce 0\n# sudo \/etc\/init.d\/unravel_all.sh start Configure Unravel Server and install sensors. Substitute your fully qualified domain name or your host's IP for UNRAVEL_HOST # python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_cdh_setup.py --spark-version 2.3.0 --unravel-server UNRAVEL_HOST Checking for Any Alerts, Denials, or Policy Violations Run these commands to check for any violations which might have after an installation or an operation\/job submission to see if any violations have occurred. To view any Unravel specific alerts: # sealert -a \/var\/log\/audit\/audit.log | grep unravel To view any system specific alerts: # sealert -a \/var\/log\/audit\/audit.log Installing and Using SELinux Tools # yum install setroubleshoot setools\n# yum install policycoreutils policycoreutils-python selinux-policy selinux-policy-targeted libselinux-utils setroubleshoot setools setools-console These tools help you get more information about the policy and analyze the avc Use seinfo # seinfo\nStatistics for policy file: \/sys\/fs\/selinux\/policy\nPolicy Version & Type: v.28 (binary, mls)\n\nClasses: 94 Permissions: 262 \nSensitivities 1 Categories: 1024\nTypes: 4747 Attributes: 251\nUsers: 8 Roles: 14\nBooleans: 307 Cond. Expr.: 56\nAllow: 101746 Neverallow: 0\nAuditallow: 155 Dontaudit: 8846\nType_trans: 17759 Type_change: 74\nType_member: 35 Role allow: 39\nRole_trans: 416 Range_trans: 5697\nConstraints: 109 Validatetrans: 0\nInitial SIDs: 27 Fs_use: 29\nGenfscon: 105 Portcon: 602\nNetifcon: 0 Nodecon: 0\nPermissives: 6 Polcap: 2 Use semodule # semodule -DB Use sealert All alerts generated by SELinux # sealert -a \/var\/log\/audit\/audit.log Unravel specific alerts # sealert -a \/var\/log\/audit\/audit.log | grep unravel For debugging (in other words, if you're testing in enforcement mode), run the following commands: Log all trivial violations logged by SELinux. # semodule -DB Set the audit log file to 0 so you get to know of access violations happening during the testing of enforcement mode. # > \/var\/log\/audit\/audit.log Working with Modes Checking Which Mode SELinux is Running Retrieve the current SELinux mode. Output is permissive nforcing # getenforce Switching Modes You can switch modes on the fly using the setenforce When Unravel is restarted SELinux returns to the default mode set in \/etc\/sysconfig\/selinux To set permissive mode # setenforce 0 To set enforcement mode # setenforce 1 Installing MySQL in Enforcing Mode Instructions for installing and configuring MySQL are available here If the datadir \/srv\/unravel\/db_data MySQL installation page Configure and Start MySQL Server An alert is generated in Step 5 . when starting mysqld datadir=\/srv\/unravel\/db_data : If you believe that mysqld should be allowed read access on the plugin.frm file by default Alert Configure Unravel to Connect My SQL Server An alert is generated in Step 1while creating the database. : If you believe that mysqld should be allowed create access on the ibdata1 file by default. Alert An alert is generated in Step 3when creating the schema for Unravel. : If you believe that mysqld should be allowed remove_name access on the Alert edge-4.lower-test Sample policy module my-mysqld 1.0;\nrequire {\ntype mysqld_safe_t;\ntype var_t;\ntype mysqld_t;\nclass process siginh;\nclass dir { add_name create remove_name write };\nclass file { create getattr lock open read rename unlink write };\n}\n#============= mysqld_safe_t ==============\n#!!!! This avc is allowed in the current policy\nallow mysqld_safe_t mysqld_t:process siginh;\n#============= mysqld_t ==============\n#!!!! This avc is allowed in the current policy\nallow mysqld_t var_t:dir { add_name create remove_name write };\nallow mysqld_t var_t:file rename;\n#!!!! This avc is allowed in the current policy\nallow mysqld_t var_t:file { create getattr lock open read unlink write }; " }, 
{ "title" : "Monitoring Workflows", 
"url" : "adv/monitoring-workflows.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Monitoring Workflows", 
"snippet" : "Monitoring Oozie Workflows Monitoring Airflow Workflows...", 
"body" : " Monitoring Oozie Workflows Monitoring Airflow Workflows " }, 
{ "title" : "Monitoring Oozie Workflows", 
"url" : "adv/monitoring-workflows/monitoring-oozie-workflows.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Monitoring Workflows \/ Monitoring Oozie Workflows", 
"snippet" : "Enabling Oozie Open \/usr\/local\/unravel\/etc\/unravel.properties oozie.server.url Oozie_host oozie.server.url Oozie_host Restart unravel_os3 # \/etc\/init.d\/unravel_os3 restart Monitoring Oozie To start monitoring your Oozie workflows go to Applications | Workflows....", 
"body" : "Enabling Oozie Open \/usr\/local\/unravel\/etc\/unravel.properties oozie.server.url Oozie_host oozie.server.url Oozie_host Restart unravel_os3 # \/etc\/init.d\/unravel_os3 restart Monitoring Oozie To start monitoring your Oozie workflows go to Applications | Workflows. " }, 
{ "title" : "Monitoring Airflow Workflows", 
"url" : "adv/monitoring-workflows/monitoring-airflow-workflows.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Monitoring Workflows \/ Monitoring Airflow Workflows", 
"snippet" : "This article describes how to set up Unravel Server to monitor Airflow workflows so you can see them in Unravel Web UI. Table of Contents Airflow Web UI Access Changing the Monitoring Range Enabling AirFlow Configuration Properties Note Before you start, ensure the Unravel Server host and the server...", 
"body" : "This article describes how to set up Unravel Server to monitor Airflow workflows so you can see them in Unravel Web UI. Table of Contents Airflow Web UI Access Changing the Monitoring Range Enabling AirFlow Configuration Properties Note Before you start, ensure the Unravel Server host and the server that runs Airflow web service are in the same cluster. All the following steps are on the Unravel Server host that runsthe unravel_jcs2 HIGHLIGHTED Airflow Web UIAccess Open \/usr\/local\/unravel\/etc\/unravel.properties and update the following properties. If you can't find them, add them. Http For Airflow Web UIAccess Set the following three (3) properties. Replace AIRFLOW_WEB_URL http:\/\/ com.unraveldata.airflow.protocol=http com.unraveldata.airflow.server.url= AIRFLOW_WEB_URL com.unraveldata.airflow.available=true Https For Airflow Web UIAccess Set the following four (4) properties. Replace AIRFLOW_WEB_URL https:\/\/ Replace AIRFLOW_WEB_UI_username AIRFLOW_WEB_UI_password com.unraveldata.airflow.server.url= AIRFLOW_WEB_URL com.unraveldata.airflow.available=true com.unraveldata.airflow.login.name=AIRFLOW_WEB_UI_username com.unraveldata.airflow.login.password= AIRFLOW_WEB_UI_password Restartthe unravel_jcs2daemon. # sudo \/etc\/init.d\/unravel_jcs2 restart Changing the Monitoring Range By default, Unravel Server ingests all the workflows that started within the last five (5) days. You change the date range to the last X Open \/usr\/local\/unravel\/etc\/unravel.properties and update the following property. If you can't find it, add it.Note there’s a “-” (minus sign) in the value. airflow.look.back.num.days=- X Restartthe unravel_jcs2daemon. # sudo \/etc\/init.d\/unravel_jcs2 restart Enabling AirFlow Below is a sample script, spark-test.py spark-test.py from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators import PythonOperator\nfrom datetime import datetime, timedelta\nimport subprocess\n\n\ndefault_args = {\n 'owner': 'airflow',\n 'depends_on_past': False,\n 'start_date': datetime(2015, 6, 1),\n 'email': ['airflow@airflow.com'],\n 'email_on_failure': False,\n 'email_on_retry': False,\n 'retries': 1,\n 'retry_delay': timedelta(minutes=5),\n # 'queue': 'bash_queue',\n # 'pool': 'backfill',\n # 'priority_weight': 10,\n # 'end_date': datetime(2016, 1, 1),\n}\n In Airflow, workflows are represented by directed acyclic graphs (DAGs). For example, dag = DAG('spark-test', default_args=default_args)\n Add Hooks for Unravel Instrumentation. This script below, example-hdp-client.sh spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark.unravel.server.hostport We recommend setting these parameters on a per-application only spark-defaults.conf This script can be invoked to submit an Airflow Spark application via spark-submit PATH_TO_SPARK_EXAMPLE_JAR=\/usr\/hdp\/2.3.6.0-3796\/spark\/lib\/spark-examples-*.jar UNRAVEL_SERVER_IP_PORT=10.20.30.40:4043 SPARK_EVENT_LOG_DIR=hdfs:\/\/ip-10-0-0-21.ec2.internal:8020\/user\/ec2-user\/eventlog example-hdp-client.sh hdfs dfs -rmr pair.parquet\nspark-submit \\\n--class org.apache.spark.examples.sql.RDDRelation \\\n--master yarn-cluster \\\n--conf \"spark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true\" \\\n--conf \"spark.executor.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1\" \\\n--conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n--conf \"spark.eventLog.dir=$SPARK_EVENT_LOG_DIR\" \\\n--conf \"spark.eventLog.enabled=true\" \\\n$PATH_TO_SPARK_EXAMPLE_JAR Submit the Workflow Operators (also called tasks) determine execution order (dependencies). In the example below, t1 t2 BashOperator PythonOperator example-hdp-client.sh Note: The pathname of example-hdp-client.sh ~\/airflow\/dags t1 = BashOperator(\ntask_id='example-hdp-client',\nbash_command=\"example-scripts\/example-hdp-client.sh\",\nretries=3,\ndag=dag)\n\n\ndef spark_callback(**kwargs):\nsp\n = subprocess.Popen(['\/bin\/bash', \n'airflow\/dags\/example-scripts\/example-hdp-client.sh'], \nstdout=subprocess.PIPE, stderr=subprocess.STDOUT)\nprint sp.stdout.read()\n\n\nt2 = PythonOperator(\ntask_id='example-python-call',\nprovide_context=True,\npython_callable=spark_callback,\nretries=1,\ndag=dag)\n\n\nt2.set_upstream(t1)\n You can test the operators first. For example, in Airflow: airflow test spark-test example-python-call Configuration Properties See here Note Due to this Airflow bug in v1.10.0: https:\/\/issues.apache.org\/jira\/browse\/AIRFLOW-2799 " }, 
{ "title" : "Roles and Role Based Access Control", 
"url" : "adv/adv-rbac-roles-rbac.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Roles and Role Based Access Control", 
"snippet" : "Unravel supports three roles. Admin Read-Only Admin End-User...", 
"body" : "Unravel supports three roles. Admin Read-Only Admin End-User " }, 
{ "title" : "Role Based Access Control (RBAC)", 
"url" : "adv/adv-rbac-roles-rbac.html#UUID-703b7757-54af-b296-3e5a-078126a6f880_id_RolesandRoleBasedAccessControl-RoleBasedAccessControlRBAC", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Roles and Role Based Access Control \/ Role Based Access Control (RBAC)", 
"snippet" : "RBAC allows Admin to restrict an end-users view to certain pages and apps. Admins and Read-only Admins views and abilities are not affected If you are not familiar with the concept of tagging, please What is Tagging? RBAC Roles Configure RBAC RBAC UI Configure LDAP or SAML RBAC Properties Creating R...", 
"body" : "RBAC allows Admin to restrict an end-users view to certain pages and apps. Admins and Read-only Admins views and abilities are not affected If you are not familiar with the concept of tagging, please What is Tagging? RBAC Roles Configure RBAC RBAC UI Configure LDAP or SAML RBAC Properties Creating Read-only Admins Manage Page " }, 
{ "title" : "RBAC Roles", 
"url" : "adv/adv-rbac-roles-rbac/adv-rbac-roles.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Roles and Role Based Access Control \/ RBAC Roles", 
"snippet" : "RBAC allows admins to restrict the pages a specific end-user can view and those pages are populated. Application tagging is intertwined with RBAC. While it is possible to use RBAC without defining applications tags, its usefulness is limited. See What is Tagging? The End-user's Access is Restricted ...", 
"body" : "RBAC allows admins to restrict the pages a specific end-user can view and those pages are populated. Application tagging is intertwined with RBAC. While it is possible to use RBAC without defining applications tags, its usefulness is limited. See What is Tagging? The End-user's Access is Restricted Based Upon Three Factors Tags for the End-user You can create tags for Applications - See Tagging Applications Workflows - See Tagging Workflows End-users are then associated with the tags via LDAP or SAML. See com.unraveldata.login.mode When RBAC is turned on an end-user view is filtered based upon their tags. For instance, if a user only has the defined tag dept:marketing dept:marketing. Unravel Default Tag is always used to filter the end-user's view. It is set to com.unraveldata.rbac.default Username Mode determines which pages the end-user can access. It is set to com.unraveldata.ngui.user.mode extended In extended Application | Applications Operations | Usage Details | Infrastructure and Operations | Usage Details | Impala Usage, Reports | Operational Insights | Chargeback In restricted Application | Applications What the End-user Sees When RBAC is Turned On The available pages (as defined by mode) display applications contained in (filtered end-user tags) ∪ com.unraveldata.rbac.default If com.unraveldata.rbac.default and How to Exempt an End-User from RBAC To exempt a end-user from RBAC, you can make the end-user an read-only admin. " }, 
{ "title" : "Configure RBAC General Properties", 
"url" : "adv/adv-rbac-roles-rbac/adv-rbac-configure.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Roles and Role Based Access Control \/ Configure RBAC General Properties", 
"snippet" : "Open \/usr\/local\/unravel\/etc\/unravel.properties \/\/ set login mode to open, ldap or saml. com.unraveldata.login.mode=mode com.unraveldata.rbac.enabled=true com.unraveldata.rbac.default=Username com.unraveldata.ngui.user.mode=extended com.unraveldata.rbac.tagcmd=path_of_tag_file If you are upgrading fr...", 
"body" : " Open \/usr\/local\/unravel\/etc\/unravel.properties \/\/ set login mode to open, ldap or saml.\ncom.unraveldata.login.mode=mode \ncom.unraveldata.rbac.enabled=true \ncom.unraveldata.rbac.default=Username \ncom.unraveldata.ngui.user.mode=extended \ncom.unraveldata.rbac.tagcmd=path_of_tag_file If you are upgrading from 4.3 you must replace\/redefine the following properties. Below we are assuming login.mode ldap 4.3 property Replacement com.unraveldata.rbac.mode=ldap com.unraveldata.login.mode=ldap com.unraveldata.rbac.prefix=dept- com.unraveldata.rbac.ldap.tag.dept.regex.find=dept-(.*) com.unraveldata.rbac.tag=dept com.unraveldata.rbac.ldap.tags=dept com.unraveldata.rbac.user.operations.enabled=true com.unraveldata.ngui.user.mode=(extended|restricted) You can exempt specific end-users from RBAC effects by adding them to the readonly admin group. Modify the following property, based upon the com.unraveldata.login.mode Open com.unraveldata.login.admins.readonly=user1,user2,user3 LDAP com.unraveldata.login.admins.ldap.readonly=user1,user2,user3 SAML com.unraveldata.login.admins.saml.readonly=user1,user2,user3 " }, 
{ "title" : "Configure LDAP or SAML RBAC Properties", 
"url" : "adv/adv-rbac-roles-rbac/adv-rbac-configure-ldap-saml.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Roles and Role Based Access Control \/ Configure LDAP or SAML RBAC Properties", 
"snippet" : "RBAC uses tags, please see What is Tagging Configure the following properties for either SAML and LDAP based upon the value of com.unraveldata.login.mode com.unraveldata.login.admins.readonly.[saml|ldap].groups here com.unraveldata.login.admins.saml.readonly=user1,user2,user3 \/\/ Required com.unravel...", 
"body" : "RBAC uses tags, please see What is Tagging Configure the following properties for either SAML and LDAP based upon the value of com.unraveldata.login.mode com.unraveldata.login.admins.readonly.[saml|ldap].groups here com.unraveldata.login.admins.saml.readonly=user1,user2,user3 \/\/ Required com.unraveldata.login.admins.ldap.groups=admin1,admin2,admin3 \ncom.unraveldata.rbac.ldap.tags.find=proj,dept \ncom.unraveldata.rbac.ldap.proj.regex.find=proj-(.*)\ncom.unraveldata.rbac.ldap.dept.regex.find=dept-(.*) \/\/ Optional com.unraveldata.login.admins.readonly.ldap.groups=RO-admin4,RO-admin5,RO=admin6 SAML \/\/ Required com.unraveldata.login.admins.saml.groups=admin1,admin2,admin3 \ncom.unraveldata.rbac.saml.tags.find=proj,dept \ncom.unraveldata.rbac.saml.proj.regex.find=proj-(.*)\ncom.unraveldata.rbac.saml.dept.regex.find=dept-(.*) \/\/ Optional com.unraveldata.login.admins.readonly.saml.groups=RO-admin4,RO-admin5,RO=admin6 Example When a user logs on, their LDAP or SAML group User LDAP Groups Tags Key Value user1 [\"dept-hr,\"dept-sale\",\"dept-finance\"] {\"dept\":{\"hr\",\"sale\",\"finance\"} dept hr, sales, finance user2 [\"proj-group1\",\"proj-group2\", \"proj-group3\"] {\"proj\":{\"-\"group1\",\"group2\", \"group3\"} proj group1, group2, group3 user3 [\"proj-group1\",\"proj-group2\", \"proj-group3\", \"dept-hr,\"dept-sale\",\"dept-finance\"] {\"proj\":{\"-\"group1\",\"group2\", \"group3\"} \\ proj group01, group02, group03 user4 [\"div-div1\",\"div-div2\", \"div-div3\"] n\/a n\/a n\/a and User 1 User 2 com.unraveldata.rbac.LDAP.tags.find LDAP groups has two tags matching User 3 com.unraveldata.rbac.LDAP.tags.find proj LDAP groups has one tag, User 4 div, " }, 
{ "title" : "Creating Read-only Admins", 
"url" : "adv/adv-rbac-roles-rbac/adv-conf-custom-creating-read-only-admins.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Roles and Role Based Access Control \/ Creating Read-only Admins", 
"snippet" : "A read-only admin has access to all the UI's page including the Manage page Open \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.login.admins.readonly = com.unraveldata.login.admins.readonly user1 user2 user3 If you are using LDAP or SAML, you must configure the read-only admins...", 
"body" : "A read-only admin has access to all the UI's page including the Manage page Open \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.login.admins.readonly = com.unraveldata.login.admins.readonly user1 user2 user3 If you are using LDAP or SAML, you must configure the read-only admins using com.unraveldata.login.admins. MODE MODE com.unraveldata.login.mode Configure LDAP or SAML RBAC Properties .MODE. com.unraveldata.login.admins groups admin1 admin2 admin3 user1 user2 user3 " }, 
{ "title" : "Example RBAC Configurations", 
"url" : "adv/adv-rbac-roles-rbac/adv-rbac-example-config.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Roles and Role Based Access Control \/ Example RBAC Configurations", 
"snippet" : "Admins and read-only admins are always exempt for RBAC restrictions. To use RBAC, set these properties: com.unraveldata.rbac.enabled=true com.unraveldata.ngui.user.mode=[extended | restricted] are irrelevant if mode is LDAP or SAML. com.unraveldata.login.admins[.readyonly] In all the examples below ...", 
"body" : "Admins and read-only admins are always exempt for RBAC restrictions. To use RBAC, set these properties: com.unraveldata.rbac.enabled=true com.unraveldata.ngui.user.mode=[extended | restricted] are irrelevant if mode is LDAP or SAML. com.unraveldata.login.admins[.readyonly] In all the examples below we are using LDAP mode. Set Admin Access To set admin and not read-only admin access, set these properties: com.unraveldata.login.admins=L772417,K228680\n\/\/ comment out or remove\n#com.unraveldata.login.admins.readonly= For LDAP or SAML set these properties: com.unraveldata.login.mode=LDAP\ncom.unraveldata.login.admins.ldap.groups=LDAP_Users,,,,\n\/\/ comment out or remove\n#com.unraveldata.login.admins.readonly.ldap.groups=LDAP_Users,,,, Set Only Read-Only Admin Access To set only read-only admin access, set these properties: com.unraveldata.login.admins.readonly=RO-L772417,RO-K228680\n \/\/comment out or remove #com.unraveldata.login.admins=L772417,K22868 For LDAP or SAML set these properties: com.unraveldata.login.mode=LDAP \ncom.unraveldata.login.admins.readonly.ldap.groups= LDAP_Users LDAP_Users Set Admin and Read-Only Admin Access To set admin and read-only admin access, set these properties: com.unraveldata.login.admins=L772417,K228680\n com.unraveldata.login.admins.readonly=RO-L772417,RO-K228680 For LDAP or SAML set these properties: com.unraveldata.login.mode=LDAP \ncom.unraveldata.login.admins.readonly.ldap.groups= LDAP_Users LDAP_Users Exempt Select End-Users from RBAC To exempt select end-users from RBAC – add them to the read-only admin property: com.unraveldata.login.admins.readonly=RO-L772417,RO-K228680 For LDAP or SAML set these property: com.unraveldata.login.admins.ldap.groups= LDAP_Users " }, 
{ "title" : "RBAC UI", 
"url" : "adv/adv-rbac-roles-rbac/adv-rbac-ui.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Roles and Role Based Access Control \/ RBAC UI", 
"snippet" : "If com.unraveldata.login.mode The tags used for RBAC end-users must also be loaded as Application Workflows com.unraveldata.rbac.default RBAC Configuration Go to Manage | Role Manager to access the Role Manager. The RBAC default is set via com.unraveldata.rbac.enabled . If you are not using LDAP\/SAM...", 
"body" : " If com.unraveldata.login.mode The tags used for RBAC end-users must also be loaded as Application Workflows com.unraveldata.rbac.default RBAC Configuration Go to Manage | Role Manager to access the Role Manager. The RBAC default is set via com.unraveldata.rbac.enabled . If you are not using LDAP\/SAML login mode, you can add filters for specific end-users. Any end-user roles you have previously set are displayed. If the Unravel daemon was restarted after you added end-user roles the entries are lost. You can add end-users one at at time via Add New Role. csv Adding Roles You limit end-user access through tags. In the example below only two tags are available, project tenant department csv Clicking on Add New Role Adding one or more roles via a role file Click on Select role file csv csv : first row is a header row defining the columns tags user, tagKey[,tagKey]* : is a valid tag key, i.e., department, tenant. tagKey : one or more rows defining user and tag values user, tagValue[,tagValue]* : is empty, a valid tag value for tagValue tagKey tagString : is a series of tagString tagValue : means zero or more * Note: The file must tagKey tagValue user After you add your last tagValue .csv s must be ordered as defined in the header row. tagValue No special characters or spaces are allowed in file. The . csv user,project,tenant user72,\"group1,group2\",mm user25,,\"3n,3m\" userNew,groupNew user33,\"group3,group2\",\"3m,mm\" Editing or Deleting Roles To edit a role, click the edit glyph ( To delete a role, click the delete glyph. Effect of Role Access Control End-user's Access with RBAC turned off The user has access to all the Unravel UI features and all applications. End-user's Access with RBAC turned on. The user only has access to their applications or those matching their tags. " }, 
{ "title" : "Manage Page", 
"url" : "adv/adv-rbac-roles-rbac/adv-rbac-manage-page.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Roles and Role Based Access Control \/ Manage Page", 
"snippet" : "This page is only available to Admins and read-only Admins....", 
"body" : "This page is only available to Admins and read-only Admins. " }, 
{ "title" : "Running Verification Scripts and Benchmarks", 
"url" : "adv/running-verification-scripts-and-benchmarks.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks", 
"snippet" : "Table of Contents Why Run Verification Tests or Benchmarks? Running Verification Tests (“Smoke Tests”) CDH HDP MapR Running Benchmarks Spark Available Benchmark Packages Executing the benchmarks Benchmarks for 1.6.x Benchmarks 2.0.x This topic explains how to run verification tests and benchmarks af...", 
"body" : " Table of Contents Why Run Verification Tests or Benchmarks? Running Verification Tests (“Smoke Tests”) CDH HDP MapR Running Benchmarks Spark Available Benchmark Packages Executing the benchmarks Benchmarks for 1.6.x Benchmarks 2.0.x This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server. " }, 
{ "title" : "Why Run Verification Tests or Benchmarks?", 
"url" : "adv/running-verification-scripts-and-benchmarks.html#UUID-15e465c2-38c3-fb51-db7d-ccbd945be886_id_RunningVerificationScriptsandBenchmarks-WhyRunVerificationTestsorBenchmarks", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Why Run Verification Tests or Benchmarks?", 
"snippet" : "Verification tests highlight the value of Unravel’s application performance management\/analysis. Benchmarks verify that Unravel features are working correctly....", 
"body" : " Verification tests highlight the value of Unravel’s application performance management\/analysis. Benchmarks verify that Unravel features are working correctly. " }, 
{ "title" : "Running Verification Tests (“Smoke Tests”)", 
"url" : "adv/running-verification-scripts-and-benchmarks.html#UUID-15e465c2-38c3-fb51-db7d-ccbd945be886_id_RunningVerificationScriptsandBenchmarks-RunningVerificationTestsSmokeTests", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Running Verification Tests (“Smoke Tests”)", 
"snippet" : "Currently, we provide smoke tests for Spark jobs only. Please follow the instructions in the section that matches your deployment. When content is noted red with brackets, i.e. { UNRAVEL_HOST_IP_ADDRESS} CDH On your Unravel Server host, run the spark_test_via_parcel.sh Installing the Unravel Parcel ...", 
"body" : "Currently, we provide smoke tests for Spark jobs only. Please follow the instructions in the section that matches your deployment. When content is noted red with brackets, i.e. { UNRAVEL_HOST_IP_ADDRESS} CDH On your Unravel Server host, run the spark_test_via_parcel.sh Installing the Unravel Parcel on CDH+CM UNRAVEL_HOST_IP_ADDRESS} # \/usr\/local\/unravel\/install_bin\/spark_test_via_parcel.sh --unravel-server {UNRAVEL_HOST_IP_ADDRESS} Note: You can run this script before configuring the \" Gateway Automatic Deployment of Spark Instrumentation After you configure the \" Gateway Automatic Deployment of Spark Instrumentation # \/opt\/cloudera\/parcels\/CDH\/lib\/spark\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--deploy-mode client \\\n--master yarn \\\n\/opt\/cloudera\/parcels\/CDH\/lib\/spark\/examples\/lib\/spark-examples-*.jar 1000 HDP After you install Unravel Sensor for Spark # \/usr\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/usr\/hdp\/current\/spark-client\/lib\/spark-examples*.jar 10 MapR After you install Unravel Sensor for Spark # \/opt\/mapr\/spark\/spark-1.6.1\/bin\/spark-submit \\\n--class org.apache.spark.examples.SparkPi --master yarn-client \\\n--num-executors 1 --driver-memory 512m --executor-memory 512m \\\n--executor-cores 1 \/opt\/mapr\/spark\/spark-1.6.1\/lib\/spark-examples*.jar 10 " }, 
{ "title" : "Running Benchmarks", 
"url" : "adv/running-verification-scripts-and-benchmarks.html#UUID-15e465c2-38c3-fb51-db7d-ccbd945be886_id_RunningVerificationScriptsandBenchmarks-RunningBenchmarks", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Running Verification Scripts and Benchmarks \/ Running Benchmarks", 
"snippet" : "We provide sample Spark apps that you can download from preview.unraveldata.com Spark Available Benchmark Packages Package Name Location Benchmarks 1.6.x https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz Benchmarks 2.0.x https:\/\/preview.unraveldata.com\/img\/demo-benchmarks-for-spark-2.0.tgz T...", 
"body" : "We provide sample Spark apps that you can download from preview.unraveldata.com Spark Available Benchmark Packages Package Name Location Benchmarks 1.6.x https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz Benchmarks 2.0.x https:\/\/preview.unraveldata.com\/img\/demo-benchmarks-for-spark-2.0.tgz The .tgz Executing the benchmarks Go to the directory where you want to download and unpack the benchmark package. Download the file, where { LOCATION FNAME # curl {LOCATION} -o {FNAME} Once downloaded, run md5sum FNAME # md5sum {FNAME} Confirm that the output of md5sum ff8e56b4d5abfb0fb9f9e4a624eeb771 Md5sum for spark-benchmarks1.tgz 71198901cedeadd7f8ebcf1bb1fd9779 Md5sum for demo-benchmarks-for-spark-2.0.tgz Uncompress the .tgz # tar -zxvf {FNAME} After unpacking , cd demo_dir # cd {demo_dir}\n# ls\nbenchmarks\/ data\/ The benchmarks folder includes the jar of the Spark examples, the source files, and the scripts used to execute the examples. # ls benchmarks\nREADME recommendations.png src\/\nlib\/ scripts\/ tpch-query-instances\/ lib: scripts: two scripts .\/example{#} .sh .\/example{#} -after.sh src: tpch-query-instances: cd # cd data\n# ls data\nDATA.BIG.2G\/ tpch10g\/ Upload the datasets (requiring 12 GB size) # hdfs dfs -put tpch10g\/ \/tmp\/\n# hdfs dfs -put DATA.BIG.2G\/ \/tmp\/ Execute the first benchmark script, where {#} is the number of the script you wish to execute. # .\/example{#}.sh After the run, Unravel recommendations are shown in the UI, on the application page. Once the example script is issued, the application metadata is displayed. Use the app id Recommendations are deployment specific so you need to edit the Spark properties in the example{#}-after.sh scripts as suggested in the Recommendations tab of the Unravel UI. The categories of recommendations and insights are: actionable recommendations (examples 1, 2 , and 5)* Spark SQL (example 3) error view and insights for failed applications (example4) and recommendations for caching (example 5) *if running Benchmarks 2.0.x, example 6 is an actionable recommendation. Example Spark Recommendations Execute the edited \"-after\" script, that includes the Spark configuration properties as suggested in the Recommendations tab of the Unravel UI. # .\/example{#}-after.sh After running the sample, check whether the re-execution of the script improved the performance or resource efficiency of the application. You can also check the Program Execution Example 5 In order to run this script you must enable insights for caching, which are disabled by default as it consumes additional heap from the memory allocation of the Spark worker daemon. You should enable insights for caching only if you expect that caching will improve performance of your Spark application. Add the following property to \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.spark.events.enableCaching=true Restart the spark worker daemon. # sudo \/etc\/init.d\/unravel_sw_1 restart Repeat step 9 - 12. Once you have completed running example5.sh and example5-after.sh reset the caching insight option to false. com.unraveldata.spark.events.enableCaching=false Restart the spark worker daemon. # sudo \/etc\/init.d\/unravel_sw_1 restart Benchmarks for 1.6.x Example Description Demonstrates example1 A Scala-based application which generates its input and applies multiple transformations to the generated data. How Unravel helps select the number of partitions and container sizes for best performance, i.e., increasing the number of partitions and reducing per-container memory resources. example2 A Scala-based application which generates its input and applies multiple transformations to the generated data. How Unravel helps select the container sizes for best performance, i.e., reducing per-container memory resources. example3 A Scala program containing a SparkSQL How Unravel helps select the number of executors for best performance when dynamic allocation is disabled, i.e., increasing the number of executors. example4 A Scala-based application. This application generates its input and applies multiple transformations to the generated data. How Unravel helps to root-cause a failed example5 A Scala-based application. The application runs on an input of 2GB and applies multiple join co-group Pre-requirement com.unraveldata.spark.events.enableCaching=true unravel.properties This property is disabled only Unravel’s insights for caching persist() In this example, dynamic allocation is disabled. Benchmarks 2.0.x Example Description Demonstrates example1 see example1 in Benchmarks for Spark 1.6.x example2 A Scala-based application which generates its input and applies multiple transformations to the generated data including the coalesce How Unravel helps select the number of partitions and container sizes for best performance of a Spark application, i.e., increasing the number of partitions. example3 example4 example5 see example3 - example5 in Benchmarks for Spark 1.6.x example6 A Scala-based Spark application which generates its input and applies multiple transformations to the generated data. How Unravel helps select the container sizes for best performance of a Spark application, i.e., reducing the memory requirements per executor. " }, 
{ "title" : "Tagging", 
"url" : "adv/adv-tagging.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Tagging", 
"snippet" : "See What is Tagging? Application and workflow tags allow you to: Filter the applications displayed, Group applications together (workflow), Selected a subset of applications based upon tags for chargeback reports Limit users UI access and applications they can see via Role Based Access Control What ...", 
"body" : "See What is Tagging? Application and workflow tags allow you to: Filter the applications displayed, Group applications together (workflow), Selected a subset of applications based upon tags for chargeback reports Limit users UI access and applications they can see via Role Based Access Control What is Tagging? Tagging Workflows Tagging a Hive on Tez Query Tagging Applications " }, 
{ "title" : "What is Tagging?", 
"url" : "adv/adv-tagging/adv-tags-what-is-tagging.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Tagging \/ What is Tagging?", 
"snippet" : "Tagging is a method to apply more context to your applications so you can view and organize them with more specificity based upon your requirements and needs. There are two types of tags, application and workflow. tags provide ways to filter applications. Application tags logically group a set of ap...", 
"body" : "Tagging is a method to apply more context to your applications so you can view and organize them with more specificity based upon your requirements and needs. There are two types of tags, application and workflow. tags provide ways to filter applications. Application tags logically group a set of applications. Workflow Application Tagging By the time your application reaches the cluster much of its context Available Metadata by Default All applications have the following metadata. (See here App type App Id App Name Username Cluster (empty for server-less applications) Queue (empty for server-less applications) Configuration (accessible for some applications) As an admin you need to be able to organize\/view the applications running on the cluster with more granularity than this limited set allows. You will want to “slice and dice\" applications a myriad of ways for a variety of reasons. Unravel can not “magically” deduce which applications belong to what tenants, projects, teams, etc.; to gain such granularity you must provide Unravel with this information. Tagging an app is how you tell Unravel about its context. What is a tag At the basic level a tag is simply a < key value> A simple way to conceptualize tagging is to think of a spreadsheet. The key value App Type App ID App Name User Cluster Queue Tenant Project MR j_134 distc john def root.UM-proj1.print marketing proj1 Spark s_345 rate.spark.sim jane def root.UM-proj2.print sales proj2 Spark s_456 over.spark.sim jane def root.UM-proj3.print sales proj3 See below for an explanation of how these tags were generated. Effective Use of Tags You can use tags to, among other things, to Generate chargeback reports based upon specific criteria, e.g., project, dept, team, etc. Decide which apps a specific user can see, e.g., the marketing head can see all marketing apps while user2 can only see specific marketing project apps. Group applications together (see Tagging Workflows In order to effectively use tags you need to understand your requirements for displaying or grouping your apps. You might want to consider, among other things Do you need chargebacks reports for each tenant in a multi-tenant cluster? Apps must be tagged with the tenant it belongs to. Do applications need to be billed back to departments and teams? Apps need department and team tags. Do you want to allow some users to see all projects and others just a subset (see Role Based Access Control A specific tag can be used for different purposes. You can use the project tag to generate chargeback reports and to filter views specific users. Assigning Tags You generate a tagging dictionary But like Unravel, the only information you have about the application running on the cluster is its metadata. So how can you develop a script to tag specific applications; how can you determine and generate your < key value Methods to generate\/create tags for an app Naming Conventions By creating naming conventions for app, queues, and cluster names you can embed information to use for your tags, e.g., placing all apps belonging to project-1 root.UM-proj1.prin Using the Metadata You can use the app’s metadata to create the tag values, some examples directly, e.g., <team, username parse it to extract information, e.g., <project, {extracted from queue name concatenate metadata with other metadata or strings, e.g., <dept, { username app name External Mapping Information A tagging script can access files which contain further mapping information. e.g., maps projects to tenants. You can find example tagging scripts at https:\/\/github.com\/unraveldata-org\/tagging Example The above table is fairly simple example to help you understand tagging concepts. In your environment you will likely use a more complicated schema. Below we explain how the tags were developed Determining the Tags The cluster is multi-tenant; we created the tags [tenant: marketing, sales]. <tenant, marketing> <tenant, sales> There are three projects; we created the tags [project: proj1, proj2, proj3]. <project, proj1> <project, proj2> <project, proj3> Finally, we have a file which maps the projects to the tenants proj1 to marketing proj2 and proj3 to sales Assigning the Tags Then we told Unravel about the tags and how to assign them, i.e., developed the tagging dictionary. First, an app’s queue is parsed to extract the project it belongs to. The project name is encoded between \"UM-\" and \".\". Once extracted, the name was used as the project value value key The script results in the three applications being assigned tenant and project tags. Workflow Tagging Workflow tags are much simpler than application tags. You use preexisting Unravel tags to create workflows, specifically unravel.workflow.name unravel.workflow.utctimestamp Tagging Workflows " }, 
{ "title" : "Tagging Workflows", 
"url" : "adv/adv-tagging/adv-tagging-workflows.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Tagging \/ Tagging Workflows", 
"snippet" : "About Unravel Workflow Tags You can add two Unravel tags (key-value pairs) to mark queries and jobs that belong to a particular workflow: : a string that represents the name of the workflow. Recommended format is unravel.workflow.name TenantName-ProjectName-WorkflowName : a timestamp in unravel.work...", 
"body" : " About Unravel Workflow Tags You can add two Unravel tags (key-value pairs) to mark queries and jobs that belong to a particular workflow: : a string that represents the name of the workflow. Recommended format is unravel.workflow.name TenantName-ProjectName-WorkflowName : a timestamp in unravel.workflow.utctimestamp yyyyMMddThhmmssZ $(date -u '+%Y%m%dT%H%M%SZ') Do not put quotes (\"\") or blank spaces in\/around the tag keys or values. For example: [Incorrect syntax] SET unravel.workflow.name=\"ETL-Workflow; [Correct syntax] SET unravel.workflow.name=ETL-Workflow; Different runs of the same the same unravel.workflow.name values for different unravel.workflow.utctimestamp Different workflows have different values for unravel.workflow.name Text in brackets ( { } ), unless otherwise noted, indicate where you must substitute your particular values for the text including the brackets. Hive Query Example This is a Hive query that was marked as part of the Financial-Tenant-ETL-Workflow SET unravel.workflow.name=Financial-Tenant-ETL-Workflow; SET unravel.workflow.utctimestamp=20160201T000000Z; SELECT foo FROM table WHERE … Your Hive Query text goes here Easy Recipes for Tagging Workflows Export the workflow name and UTC timestamp from your top-level script that schedules each run of the workflow. Here, we use bash date export WORKFLOW_NAME=Financial-Tenant-ETL-Workflow export UTC_TIME_STAMP=$(date -u '+%Y%m%dT%H%M%SZ') Follow the instructions for your job type. Hive on MR query Hive on Tez query Sqoop job Direct MapReduce job Spark job Pig job Impala Job Examples by Job Type Hive on MR Query Using SET Commands in Hive # hive -f hive\/simple_wf.hql In hive\/simple_wf.hql: SET unravel.workflow.name=${env:WORKFLOW_NAME}; SET unravel.workflow.utctimestamp=${env:UTC_TIME_STAMP}; select count(1) from lineitem; Sqoop Job Using –D Command Line Parameters # sqoop export \\\n -D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \\\n --connect jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod --table settings -m 1 \\\n --export-dir \/tmp\/sqoop_test --username unravel --verbose --password foobar\n Sqoop has bugs related to quotes: https:\/\/issues.apache.org\/jira\/browse\/SQOOP-3061 Direct MapReduce Job Using –D Command Line Parameters Substitute your file name for \/tmp\/data\/small \/tmp\/outsmoke # hadoop jar libs\/ooziemr-1.0.jar com.unraveldata.mr.apps.Driver \\\n-D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \\\n-p \/wordcount.properties -input {\/tmp\/data\/small} -output {\/tmp\/outsmoke} Spark Job Using --conf Command Line Parameters For Spark jobs, you must prefix the Unravel tags with \" spark. unravel.workflow.name spark.unravel.workflow.name # spark-submit \\\n --conf \"spark.unravel.workflow.name=$WORKFLOW_NAME\" \n --conf \"spark.unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \n --conf \"spark.eventLog.enabled=true\" \\\n --class org.apache.spark.examples.SparkPi \\\n --master yarn-cluster \\\n --deploy-mode cluster Pig Job Using –param and SET Commands # pig \\\n-param WORKFLOW_NAME=$WORKFLOW_NAME -param UTC_TIME_STAMP=$UTC_TIME_STAMP \\\n-x mapreduce -f pig\/simple.pig In pig\/simple.pig SET unravel.workflow.name $WORKFLOW_NAME; SET unravel.workflow.utctimestamp $UTC_TIME_STAMP; lines = LOAD '\/tmp\/data\/small' using PigStorage('|') AS (line:chararray); words = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) as word; grouped = GROUP words BY word; wordcount = FOREACH grouped GENERATE group, COUNT(words); DUMP wordcount; Impala Job Using SET Commands # impala-shell -i <impald_host:port> \\\n -f simpleImpala.sql \\\n --var=workflowname='ourImpalaWorkflow' \\\n --var=utctimestamp=$(date -u '+%Y%m%dT%H%M%SZ') In ..\/simpleImpala.sql SET DEBUG_ACTION=\":::: unravel.workflow.name Finding Workflows in Unravel Web UI Once your tagged workflows have been run, go log into Unravel Web UI and select Applications | Workflows " }, 
{ "title" : "Tagging a Hive on Tez Query", 
"url" : "adv/adv-tagging/adv-tagging-workflows/adv-tagging-workflows-hive-on-tez.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Tagging \/ Tagging Workflows \/ Tagging a Hive on Tez Query", 
"snippet" : "For general information see Tagging Workflows The following properties must be set in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.tagging.script.enabled=true com.unraveldata.app.tagging.script.path={\/usr\/local\/unravel\/etc\/tag_app.py} com.unraveldata.app.tagging.script.method.name={get_...", 
"body" : "For general information see Tagging Workflows The following properties must be set in \/usr\/local\/unravel\/etc\/unravel.properties com.unraveldata.tagging.script.enabled=true\ncom.unraveldata.app.tagging.script.path={\/usr\/local\/unravel\/etc\/tag_app.py}\ncom.unraveldata.app.tagging.script.method.name={get_tags} You can create tagged workflows for tez applications in four (4) ways. Use --hiveconf via hive command. Enter the following the hive command line. hive --hiveconf unravel.workflow.name=my_tez_workflow --hiveconf unravel.workflow.utctimestamp=20180801T000001Z -f tez.sql\n Sample tez.sql. set hive.execution.engine=tez;\nselect count(*) from my_test_table; Use the global python script for application tagging. Assuming the global script is \/tmp\/tag_app.py, Use--hiveconf via beeline command. Enter the following command in the beeline command line. > beeline -n hive -u 'jdbc:hive2:\/\/congo6.unraveldata.com:10000' --hiveconf unravel.workflow.name=my_tez_workflow --hiveconf unravel.workflow.utctimestamp=20180801T000001Z -f tez.sql\n Use the tez.sql script, then run beeline. You must define these the two (2) workflow tags in tez.sql: set hive.execution.engine=tez;\nset unravel.workflow.name=my_tez_workflow;\nset unravel.workflow.utctimestamp=20180801T000001Z;\nselect count(*) from my_test_table; Enter the following command in the beeline command line. > beeline -n hive -u 'jdbc:hive2:\/\/congo6.unraveldata.com:10000'-f tez.sql\n " }, 
{ "title" : "Tagging Applications", 
"url" : "adv/adv-tagging/adv-tagging-applications.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Tagging \/ Tagging Applications", 
"snippet" : "You can define tags for groups of applications using a python script. Unravel retrieves the script from the property com.unraveldata.app.tagging.script.path workflow tags You can think of the script of creating a database comprised of a list tag_keys tag_values <tag_key, tag_value> For example, You ...", 
"body" : "You can define tags for groups of applications using a python script. Unravel retrieves the script from the property com.unraveldata.app.tagging.script.path workflow tags You can think of the script of creating a database comprised of a list tag_keys tag_values <tag_key, tag_value> For example, You have three departments: finance, hr, and marketing. You would create the tag_key department give it three tag_values finance hr marketing You would then associate applications with one of more of <tag_key, tag_value> . one hive query might be associated with dept:marketing dept:market dept:finance See What is Tagging? Your Python script must be idempotent, i.e., it must produce the same result over multiple invocations with different input (metadata) for the same application. Application tags are immutable and once created they cannot be changed. To Use Python Script See Writing a Python Script example script Set the following properties in \/usr\/local\/unravel\/etc\/unravel.properties \/\/ turns tagging on com.unraveldata.tagging.script.enabled=true \/\/ set to the fully qualified path + script name, e.g., \/usr\/local\/scripts\/myTest.py com.unraveldata.app.tagging.script.path=python_script \/\/ identifies the method within the script for Unravel to use. com.unraveldata.app.tagging.script.method.name=method_name Restart the following daemons. You must # \/etc\/init.d\/unravel_all.sh stop-etl\n# \/etc\/init.d\/unravel_all.sh start Running Scripts The tags computed in the Python script feed into Unravel core ETL pipeline. The Python script is invoked in the ingestion pipeline and is set up to access application metadata to create tags on the fly. The first time an application is invoked and running it is not listed when applications are filtered by tags. Debug and print statements are logged multiple times as the script is invoked multiple times over a run. References See https:\/\/github.com\/unraveldata-org\/tagging for more examples of tagging scripts. " }, 
{ "title" : "Writing a Python Script", 
"url" : "adv/adv-tagging/adv-tagging-applications.html#UUID-f4df4d7a-cda3-2055-d9e5-eaff424e42c5_N1552802203691", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Tagging \/ Tagging Applications \/ Writing a Python Script", 
"snippet" : "You can add print\/debugging statements to the script, but they are logged each time the script is run. Consequently, there are numerous\/duplicated entries as the script is invoked multiple times during an application's run. You can also specify workflow tags Format In the Python script, you set a ta...", 
"body" : "You can add print\/debugging statements to the script, but they are logged each time the script is run. Consequently, there are numerous\/duplicated entries as the script is invoked multiple times during an application's run. You can also specify workflow tags Format In the Python script, you set a tag_key tag_value Your tag_value tag[\"auth\"]=\"admin\" tag[\"scope\"]=app_obj.getAppQueue() tags[\"dept\"]=app_obj.getAppName() + \"_\" + app_obj.getQueue() Field Where generated Description Method app_id Hadoop Application ID app_obj.getAppId() app_type Unravel Application type: mr, spark, hive, impala, or tez app_obj.getAppType() app_name Hadoop Application name app_obj.getAppName() username Hadoop Application's owner app_obj.getUsername() queue Hadoop The queue the app is running in. app_obj.getQueue() cluster_id Hadoop The cluster the the app is running on. Note : default app_obj.getClusterId() For Hive on MapReduce applications the following property is available. (This method is currently unavailable for Spark or Tez.) getAppConf(\" parameter\" Any field which exists within the MR configuration object. e.g., app_obj.getAppConf (“hive.query.id”) app_obj.getAppConf(\" parameter_name " }, 
{ "title" : "Example Python Script", 
"url" : "adv/adv-tagging/adv-tagging-applications.html#UUID-f4df4d7a-cda3-2055-d9e5-eaff424e42c5_N1552802253349", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Tagging \/ Tagging Applications \/ Example Python Script", 
"snippet" : "The below script creates seven (7) tag_keys hive_query_id dept team auth scope , and unravel.workflow.name unravel.workflow.utctimestamp tagged workflows The tagging properties are set to the script file and method name. com.unraveldata.app.tagging.script.path=\/usr\/scripts\/Tagging.py com.unraveldata...", 
"body" : "The below script creates seven (7) tag_keys hive_query_id dept team auth scope , and unravel.workflow.name unravel.workflow.utctimestamp tagged workflows The tagging properties are set to the script file and method name. com.unraveldata.app.tagging.script.path=\/usr\/scripts\/Tagging.py com.unraveldata.app.tagging.script.method.name=get_tags # filename: \/usr\/scripts\/Tagging.py\n\nfrom datetime import datetime\n\n# get_tags is the method so com.unraveldata.app.tagging.script.method.name=get_tags \ndef get_tags(obj):\n\n tags = {}\n\n# MR apps get the hive_query_id tag\n if app_obj.getAppType() == \"mr\":\n tags[\"hive_query_id\"] = app_obj.getAppConf(\"hive.query.id\")\n\n# every app gets a dept and team tag\n tags[\"dept\"] = app_obj.getAppName() + \"_\" + app_obj.getQueue())\n tags[\"team\"] = app_obj.getUsername()\n\n# Only apps with username=admin get this tag\n if app_obj.getUsername() == \"admin\": \n tags[\"auth\"] = \"admin\"\n\n# Every app gets a scope tag based upon queue they are in\n if app_obj.getQueue() == \"engr\":\n # All apps in the \"engr\" queue get this tag\n tags[\"scope\"] = \"engineering-application\"\n elif app_obj.getQueue() == \"qa\":\n # All apps in the \"qa\" queue get this tag\n tags[\"scope\"] = \"qa-application\"\n else:\n # All apps not in the\"engr\" or \"qa\" queues get this tag\n tags[\"scope\"] = \"daily-application\"\n\n# creates the workflow tags, these are Unravel tags and you should contact support@unraveldata.com before using them\n tags[\"unravel.workflow.name\"] = \"Workflow-\" + tags[\"team\"] \n tags[\"unravel.workflow.utctimestamp\"] = app_obj.getAppType() + \"-\" + str(datetime.utcnow())\n\n\n return tags " }, 
{ "title" : "Unravel APIs", 
"url" : "adv/adv-unravel-apis.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel APIs", 
"snippet" : "REST API Use Case - Auto Actions and Pagerduty...", 
"body" : " REST API Use Case - Auto Actions and Pagerduty " }, 
{ "title" : "REST API", 
"url" : "adv/adv-unravel-apis/rest-api-rest-api.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel APIs \/ REST API", 
"snippet" : "The REST API's provides you the ability to query and collect data from your cluster. They work with both HTTP and HTTPS. The information available is analogous to what you can access through the Unravel UI. In order to use the REST APIs you must first log into the Unravel UI and then log in via CLI ...", 
"body" : "The REST API's provides you the ability to query and collect data from your cluster. They work with both HTTP and HTTPS. The information available is analogous to what you can access through the Unravel UI. In order to use the REST APIs you must first log into the Unravel UI and then log in via CLI with a curl command using the same credentials (see Sign In role All requests\/responses are in JSON Format. Applications Like the Applications here here Operations Provides detailed data on various ongoing activities\/resource usage of your cluster. These include memory and vcore usage by cluster, queue, application, and user. You can also obtain information about job status and recent alerts. See here Operations Reports Returns key cluster KPIs and the small files report. See here Reports | Data Insight Generates chargeback reports (by user, queue, app), and Cluster workload reports. See here Reports | Operational Insight Monitoring Provides lightweight daemon which allows you to monitor various Unravel components via the REST API, e.g., Zookeeper, Kafka, ElasticSearch, DB, disk usage. See here Available REST Resources This API supports a Representational State Transfer (REST) model for accessing a set of resources through a fixed set of operations. API Http Method Description Sign In POST \/signIn apps POST GET GET GET GET GET GET GET \/apps\/search \/apps\/events\/inefficient_apps\/ \/apps\/status\/running\/ \/apps\/status\/finished\/ \/apps\/resources\/cpu\/allocated \/apps\/resources\/memory\/allocated \/yarn_rm\/kill_app \/yarn_rm\/move_app Applications GET GET GET GET GET \/common\/app\/{app_id}\/recommendation \/common\/app\/{app_id}\/summary \/common\/app\/{app_id}\/status \/common\/app\/{app_id}\/errors \/common\/app\/{app_id}\/logs Data Insights GET GET \/reports\/data\/kpis \/reports\/data\/small_file_report_details auto actions GET GET GET \/autoactions \/autoactions\/recent_violations \/autoactions\/metrics jobs GET \/jobs\/count operations GET GET GET GET GET GET GET \/clusters\/resources\/cpu\/allocated \/clusters\/resources\/cpu\/total \/clusters\/resources\/memory\/allocated \/clusters\/resources\/memory\/total \/clusters\/nodes \/clusters\/resources\/tagged\/cpu \/clusters\/resources\/tagged\/memory Reports GET GET GET GET GET \/search\/cb\/appt \/search\/cb\/appt\/user \/search\/cb\/appt\/queue \/reports\/operational\/clusterstats \/reports\/operational\/clusterworkload Workflow GET GET \/workflows \/workflows\/missing_sla HTTP\/HTTPS methods The API resources listed below follow standard Create-Read-Update-Delete (CRUD) semantics; the HTTP request path defines the Unravel Server and the method the action to perform. Method Operation POST Create entries GET Read entries PUT Update or edit entries DELETE Delete entries Error Codes Upon failure one of the following errors are returned Error Code Description 400 Invalid request parameters; Malformed requests 401 Authentication failure 403 Authorization failure 404 Object not found 500 Internal API error 503 Response temporarily unavailable; the caller should retry later " }, 
{ "title" : "Sign In", 
"url" : "adv/adv-unravel-apis/rest-api-rest-api/rest-api-sign-in.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Sign In", 
"snippet" : "To use the REST APis you must sign in and authorize use both the Unravel UI and in the CLI Authorize in the Unravel UI Log into the UI and open the API page. Click on the POST \/signIn Try it out Fill username password Execute. The command listed in the Curl text box above, is the same command you en...", 
"body" : "To use the REST APis you must sign in and authorize use both the Unravel UI and in the CLI Authorize in the Unravel UI Log into the UI and open the API page. Click on the POST \/signIn Try it out Fill username password Execute. The command listed in the Curl text box above, is the same command you enter in the CLI. Click Authorize Post \/signIn Authorize Command Line \n ssh {ClusterName} # ssh@root {ClusterName} Once you have successfully logged in, enter the curl command from step 2 above. You should see output as shown below. # curl -X POST \"http:\/\/UNRAVEL_HOST:3000\/api\/v1\/signIn\" -H \"accept: application\/json\" -H \"content-type: application\/x-www-form-urlencoded\" -d \"username=user1&password=Password1\" \n Response example {\n \"message\": \"ok\",\n \"token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoiYWRtaW4iLCJ1c2VybmFtZSI6ImFkbWluIiwidGFncyI6IiIsImlhdCI6MTUzNzc3NDMwOSwiZXhwIjoxNTM3NzgxNTA5fQ.iD6NXDRj1UqRYr58H4xYlNRcdrWFcU9l3p8NmbpN30k\",\n \"role\": \"admin\",\n \"readOnly\": false,\n \"tags\": \"\",\n \"id\": \"admin\",\n \"username\": \"admin\"\n} The token is needed for each REST API curl command executed. Each time you log in a new token is generated to be used in commands for that session. " }, 
{ "title" : "Apps APIs", 
"url" : "adv/adv-unravel-apis/rest-api-rest-api/adv-rest-apis-apps.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Apps APIs", 
"snippet" : "You must specify the UNRAVEL_HOST Port Returns all apps filtered by app type, status, username, queue, and tags \/app\/search curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[ appTypes appStatus End Start UserName queueName tagList UNRAVEL_HOST Port Parameters : mr | hive | s...", 
"body" : "You must specify the UNRAVEL_HOST Port Returns all apps filtered by app type, status, username, queue, and tags \/app\/search curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[ appTypes appStatus End Start UserName queueName tagList UNRAVEL_HOST Port Parameters : mr | hive | spark | pig | cascading | impala | tez appTypes : S(ucess) | F(ailed) | K(illed) | R(unning) | W | P(ending) | U(nkown) appStatus : valid users for the Unravel_host users : valid queues for the Cluster queues : \"key\":[\"value,value\"], e.g., \"dept\":[finance, mktg] taglist For Hive and MR you must specify at least one Status type. Code: 200\n{\n\"metadata\": {},\n\"results\": [\n{ }\n]\n} curl -X POST \"http:\/\/myserver.com:3000\/api\/v1\/apps\/search\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" Returns all Apps filtered by app type, status, username, queue, and tags \/apps\/events\/inefficient_apps\/ All Failed, Killed and Unknown Hive and Tez Apps Apps All apps with the failed, killed or unknown status regardless of user, queue, or tags. curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[\"hive\",\"tez\"],\"appStatus\":[\"\"F\",\"K\",\"U\"],\"end_time\":\"2018-10-09T05:18:42.000Z\",\"start_time\":\"1539043200\"}' http:\/\/172.36.1.124:3000\/api\/v1\/apps\/search App User All apps owned by a user regardless of status, queue, or tags. curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[\"mr\",\"hive\",\"spark\",\"pig\",\"cascading\",\"impala\",\"tez\"],\"appStatus\":[\"S\",\"F\",\"K\",\"R\",\"W\",\"P\",\"U\"],\"end_time\":\"1539129600\",\"start_time\":\"1539043200\",\"users\":[\"root\"]}' http:\/\/172.36.1.124:3000\/api\/v1\/apps\/search Sample Output {\n \"metadata\": {\n \"duration\": {\n \"max\": 21228577,\n \"min\": 0\n },\n \"resource\": {\n \"max\": 1,\n \"min\": 0\n },\n \"events\": {\n \"max\": 4,\n \"min\": 0\n },\n \"appTypes\": {\n \"mr\": 115,\n \"hive\": 67\n },\n \"appStatus\": {\n \"S\": 168,\n \"F\": 9,\n \"K\": 4,\n \"R\": 1\n },\n \"users\": {\n \"root\": 106,\n \"hdfs\": 68,\n \"user11\": 8\n },\n \"queues\": {\n \"root.users.root\": 83,\n \"root.users.hdfs\": 62,\n \"root.users.user11\": 8,\n \"root.abcdefghijklmnopqrstuvwxyz445583938375ytgjebfjrfhfhdfgfkhfkhfuffuhfhfhfhfhfhfhfhfjhfjhfsjhfhfsjkhfjhfdjhfj\": 6,\n \"root.abcdefghijklmnopqrstuvwxyz445583938375ytgjebfjrfhfhdfgfkhfkhfuffuhfhfhfhfhfhfhfhfjhfjh\": 4,\n \"root.abcdefghijklmnopqrstuvwxyz\": 1,\n \"root.pooja.pooja\": 1\n },\n \"clusters\": {\n \"default\": 157\n },\n \"totalRecords\": 182\n },\n \"results\": [\n {\n \"appId\": \"hdfs_20180830044747_7dd1d797-c4f9-4978-b9da-ca477c93d6b8-u_sxv4\",\n \"appType\": \"Hive\",\n \"appt\": \"hive\",\n \"gotoId\": \"hdfs_20180830044747_7dd1d797-c4f9-4978-b9da-ca477c93d6b8-u_sxv4\",\n \"gotoLevel\": \"hive\",\n \"id\": \"job_1534930112573_1201\",\n \"nick\": \"1201\",\n \"name\": \"select\\n\\ts_acctbal,\\n\\ts_name,\\n\\tn_name,\\n\\t...100(Stage-5)\",\n \"queue\": \"root.users.hdfs\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"hdfs\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"AA2\"\n ],\n \"aa2Badge\": true,\n \"inBadge\": false,\n \"clusterId\": \"default\",\n \"clusterTag\": \"-\",\n \"start_time\": \"08\\\/30\\\/18 11:48:29\",\n \"start_time_long\": \"2018-08-30T11:48:29.315Z\",\n \"duration_long\": 88000,\n \"predDuration_long\": 0,\n \"io_long\": 327736439,\n \"read_long\": 293122770,\n \"write_long\": 34613669,\n \"resource\": 1,\n \"service\": 1,\n \"events\": 1,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 1,\n \"mrJobIds\": [\n \n ],\n \"appIds\": [\n \n ],\n \"sm\": 1,\n \"sr\": 5,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 1,\n \"totalReduceTasks\": 5,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": 67916,\n \"totalReduceSlotDuration\": 38679,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": null,\n \"outputTables\": null,\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"elastic\": true,\n \"kind\": \"MR\",\n \"kindLong\": \"MR\",\n \"name_long\": \"select\\n\\ts_acctbal,\\n\\ts_name,\\n\\tn_name,\\n\\t...100(Stage-5)\",\n \"goToApp\": {\n \"appId\": \"hdfs_20180830044747_7dd1d797-c4f9-4978-b9da-ca477c93d6b8-u_sxv4\",\n \"appType\": \"Hive\"\n },\n \"kind_url\": \"jobs\",\n \"kind_parent_url\": \"hive_queries\"\n }\n ]\n} Returns list of events Inefficient apps \/apps\/events\/inefficient_apps\/ Parameters: : starting date of date range. Format YYYY-MM-DD from : ending date of date range. Format YYYY-MM-DD to Type of applications. 0=> MR, 1 => HIVE , 2 => SPARK, 16 =>IMPALA entity_type: Schema: '200':\nschema:\ntype: array\nitems:\nstring curl -H \"Content-Type: application\/json\" -X POST -d '{\"from\":0,\"appTypes\":[\"mr\",\"hive\",\"spark\",\"pig\",\"cascading\",\"impala\",\"tez\"],\"appStatus\":[\"F\"],\"end_time\":\"1539129600\",\"start_time\":\"1539043200\"}' http:\/\/myserver.com:3000\/api\/v1\/apps\/search Returns list of running apps \/apps\/status\/running\/ No Parameters Schema: '200':\n{\n\"date\":\n\"appsRunning\":\n\"appsPendiing\"::}\n} curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/apps\/status\/running\/\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoiYWRtaW4iLCJ1c2VybmFtZSI6ImFkbWluIiwidGFncyI6IiIsImlhdCI6MTU1MTk0Nzc2OSwiZXhwIjoxNTUxOTU0OTY5fQ.pZMwc7diQG2zyQcqmAgtS5FaASMs9zTZIJybzXMU68M\" Schema: {\n\"date\":1551947819180,\n\"appsRunning\":3,\n\"appsPending\":0}\n} Returns list of finished apps \/apps\/status\/finished\/ No Parameters Schema: '200':\n[\n{\n\"date\": 1551938400000,\n\"SUCCEEDED\": 86\n}\n] curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/apps\/status\/running\/\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoiYWRtaW4iLCJ1c2VybmFtZSI6ImFkbWluIiwidGFncyI6IiIsImlhdCI6MTU1MTk0Nzc2OSwiZXhwIjoxNTUxOTU0OTY5fQ.pZMwc7diQG2zyQcqmAgtS5FaASMs9zTZIJybzXMU68M\" Sample Output [\n{\n\"date\": 1551938400000,\n\"SUCCEEDED\": 86\n},\n{\n\"date\": 1551942000000,\n\"SUCCEEDED\": 20\n},\n{\n\"date\": 1551945600000,\n\"SUCCEEDED\": 34\n}\n] Returns a time series for Allocated CPU by app type \/apps\/resources\/cpu\/allocated Parameters: : starting date of date range. Format YYYY-MM-DD from : ending date of date range. Format YYYY-MM-DD to Schema: '200':\n[\n{\n\"date\": 1551938400000,\n\"SUCCEEDED\": 86\n}\n] curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/apps\/status\/running\/\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoiYWRtaW4iLCJ1c2VybmFtZSI6ImFkbWluIiwidGFncyI6IiIsImlhdCI6MTU1MTk0Nzc2OSwiZXhwIjoxNTUxOTU0OTY5fQ.pZMwc7diQG2zyQcqmAgtS5FaASMs9zTZIJybzXMU68M\" Sample Output [\n{\n\"date\": 1551938400000,\n\"SUCCEEDED\": 86\n},\n{\n\"date\": 1551942000000,\n\"SUCCEEDED\": 20\n},\n{\n\"date\": 1551945600000,\n\"SUCCEEDED\": 34\n}\n] Returns a time series for allocated memory by app type \/apps\/resources\/memory\/allocated Parameters: from to Schema: '200':\nschema:\ntype: array\nitems:\nstring curl -X GET \"http:\/\/myserver.co:3000\/api\/v1\/apps\/resources\/memory\/allocated?from=2019-03-01&to=2019-03-06\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" {\n \"1551787200000\":\"2\"\n}} \/yarn_rm\/kill_app - Returns the status code only Parameters: cluserid appid Schema: '200': curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/yarn_rm\/kill_app?clusterid=ignite1&appid=application_1550764666755_0668\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" \/yarn_rm\/move_app - Returns the status code only Required Parameters: cluserid appid queue: Schema: '200': curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/yarn_rm\/move_app?clusterid=ignite1&appid=application_1550764666755_0668&queue=root.users.root\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" " }, 
{ "title" : "Auto Actions APIs", 
"url" : "adv/adv-unravel-apis/rest-api-rest-api/rest-api-auto-actions-apis.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Auto Actions APIs", 
"snippet" : "All commands require: Unravel_Host: Port: Output schema: schema: type: array items: type: string Returns list of active and inactive auto actions \/autoactions No parameters curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/autoactions\" -H \"accept: application\/json\" \"Authorization: JWT TOKEN\" [{\"enabled\":...", 
"body" : "All commands require: Unravel_Host: Port: Output schema: schema:\ntype: array\nitems:\ntype: string Returns list of active and inactive auto actions \/autoactions No parameters curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/autoactions\" -H \"accept: application\/json\" \"Authorization: JWT TOKEN\" [{\"enabled\":false,\"policy_name\":\"AutoAction2\",\"policy_id\":10,\"instance_id\":\"5346557784540652874\",\n\"name_by_user\":\"Long running YARN application Automation\",\"description_by_user\":\"\",\"created_by\":\"admin\",\"last_edited_by\":\"admin\",\"created_at\":1551125124967,\"updated_at\":1551861204577,\"rules\":[{\"SAME\":[{\"scope\":\"apps\",\"metric\":\"elapsedTime\",\"compare\":\">\",\"state\":\"*\",\"type\":\"mapreduce\",\"value\":10000}]}],\"actions\":[{\"action\":\"send_email\",\"to\":[\"ravi@unraveldata.com\"],\"to_owner\":false},{\"action\":\"kill_app\"}],\"cluster_mode\":0,\"cluster_list\":[],\"cluster_transform\":\"\",\"queue_mode\":0,\"queue_list\":[],\"queue_transform\":\"\",\"user_mode\":2,\"user_list\":[\"user7\"],\"user_transform\":\"\",\"app_mode\":2,\"app_list\":[\"QuasiMonteCarlo\"],\"app_transform\":\"\",\"sustain_mode\":0,\"sustain_time\":0,\"time_mode\":1,\"start_hour\":13,\"start_min\":59,\"end_hour\":13,\"end_min\":59,\"dt_zone\":\"America\/Los_Angeles\",\"broker_mode\":0,\"broker_list\":[],\"broker_transform\":\"\",\"topic_mode\":0,\"topic_list\":[],\"topic_transform\":\"\"},{\"enabled\":false,\"policy_name\":\"AutoAction2\",\"policy_id\":10,\"instance_id\":\"7944134729015777055\",\n\"name_by_user\":\"Long running Hive query Automation\",\"description_by_user\":\"\",\"created_by\":\"admin\",\"last_edited_by\":\"admin\",\"created_at\":1551125155192,\"updated_at\":1551787684684,\"rules\":[{\"SAME\":[{\"scope\":\"apps\",\"metric\":\"duration\",\"compare\":\">=\",\"state\":\"*\",\"type\":\"hive\",\"value\":100}]}],\"actions\":[{\"action\":\"http_post\",\"urls\":[\"https:\/\/unraveldata.slack.com\/messages\/CA2RX1M35\/\"]}],\"cluster_mode\":0,\"cluster_list\":[],\"cluster_transform\":\"\",\"queue_mode\":0,\"queue_list\":[],\"queue_transform\":\"\",\"user_mode\":0,\"user_list\":[],\"user_transform\":\"\",\"app_mode\":0,\"app_list\":[],\"app_transform\":\"\",\"sustain_mode\":0,\"sustain_time\":0,\"time_mode\":0,\"start_hour\":12,\"start_min\":5,\"end_hour\":12,\"end_min\":5,\"dt_zone\":\"America\/Los_Angeles\",\"broker_mode\":0,\"broker_list\":[],\"broker_transform\":\"\",\"topic_mode\":0,\"topic_list\":[],\"topic_transform\":\"\"},{\"enabled\":false,\"policy_name\":\"AutoAction2\",\"policy_id\":10,\"instance_id\":\"2556543518905386312\",\n\"name_by_user\":\"Resource contention in cluster allocated memory Automation\",\"description_by_user\":\"\",\"created_by\":\"admin\",\"last_edited_by\":\"admin\",\"created_at\":1551125186605,\"updated_at\":1551787687869,\"rules\":[{\"SAME\":[{\"scope\":\"clusters\",\"metric\":\"allocatedMB\",\"compare\":\">\",\"value\":1024},{\"scope\":\"clusters\",\"metric\":\"appCount\",\"compare\":\">\",\"state\":\"*\",\"value\":2}]}],\"actions\":[{\"action\":\"send_email\",\"to\":[\"ravi@unraveldata.com\"],\"to_owner\":false}],\"cluster_mode\":1,\"cluster_list\":[],\"cluster_transform\":\"\",\"queue_mode\":0,\"queue_list\":[],\"queue_transform\":\"\",\"user_mode\":2,\"user_list\":[\"user1\"],\"user_transform\":\"\",\"app_mode\":0,\"app_list\":[],\"app_transform\":\"\",\"sustain_mode\":0,\"sustain_time\":0,\"time_mode\":0,\"start_hour\":12,\"start_min\":6,\"end_hour\":12,\"end_min\":6,\"dt_zone\":\"America\/Los_Angeles\",\"broker_mode\":0,\"broker_list\":[],\"broker_transform\":\"\",\"topic_mode\":0,\"topic_list\":[],\"topic_transform\":\"\"},{\"enabled\":false,\"policy_name\":\"AutoAction2\",\"policy_id\":10,\"instance_id\":\"8761136669870544897\",\n\"name_by_user\":\"Resource contention in cluster allocated vcores Automation\",\"description_by_user\":\"\",\"created_by\":\"admin\",\"last_edited_by\":\"admin\",\"created_at\":1551125217281,\"updated_at\":1551787691083,\"rules\":[{\"SAME\":[{\"scope\":\"clusters\",\"metric\":\"allocatedVCores\",\"compare\":\">=\",\"value\":2},{\"scope\":\"clusters\",\"metric\":\"appCount\",\"compare\":\">=\",\"state\":\"*\",\"value\":2}]}],\"actions\":[{\"action\":\"send_email\",\"to\":[\"ravi@unraveldata.com\"],\"to_owner\":false}],\"cluster_mode\":1,\"cluster_list\":[],\"cluster_transform\":\"\",\"queue_mode\":0,\"queue_list\":[],\"queue_transform\":\"\",\"user_mode\":2,\"user_list\":[\"user2\"],\"user_transform\":\"\",\"app_mode\":0,\"app_list\":[],\"app_transform\":\"\",\"sustain_mode\":0,\"sustain_time\":0,\"time_mode\":0,\"start_hour\":12,\"start_min\":6,\"end_hour\":12,\"end_min\":6,\"dt_zone\":\"America\/Los_Angeles\",\"broker_mode\":0,\"broker_list\":[],\"broker_transform\":\"\",\"topic_mode\":0,\"topic_list\":[],\"topic_transform\":\"\"}] Returns list of violations \/autoactions\/violations Parameters: from YYYY-MM-DD to YYYY-MM-DD limit: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/autoactions\/recent_violations?from=2019-03-01&to=2019-03-06&limit=10\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" Returns list of autoaction metrics \/autoactions\/metrics No parameters curl -X GET \"http:\/\/devcdh513k.unraveldata.com:3000\/api\/v1\/autoactions\/metrics\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" { \"appCount\",\"elapsedTime\",\"allocatedMB\",\"allocatedVCores\",\"runningContainers\",\"memorySeconds\",\"vcoreSeconds\",\"jobCount\",\"elapsedAppTime\",\"mapsTotal\",\"mapsCompleted\",\"reducesTotal\",\"reducesCompleted\",\"mapsPending\",\"mapsRunning\",\"reducesPending\",\"reducesRunning\",\"newReduceAttempts\",\"runningReduceAttempts\",\"failedReduceAttempts\",\"killedReduceAttempts\",\"successfulReduceAttempts\",\"newMapAttempts\",\"runningMapAttempts\",\"failedMapAttempts\",\"killedMapAttempts\",\"successfulMapAttempts\",\"badId\",\"connection\",\"ioError\",\"wrongLength\",\"wrongMap\",\"wrongReduce\",\"fileBytesRead\",\"fileBytesWritten\",\"fileReadOps\",\"fileLargeReadOps\",\"fileWriteOps\",\"hdfsBytesRead\",\"hdfsBytesWritten\",\"hdfsReadOps\",\"hdfsLargeReadOps\",\"hdfsWriteOps\",\"mapInputRecords\",\"mapOutputRecords\",\"mapOutputBytes\",\"mapOutputMaterializedBytes\",\"splitRawBytes\",\"combineInputRecords\",\"combineOutputRecords\",\"reduceInputGroups\",\"reduceShuffleBytes\",\"reduceInputRecords\",\"reduceOutputRecords\",\"spilledRecords\",\"shuffledMaps\",\"failedShuffle\",\"mergedMapOutputs\",\"gcTimeMillis\",\"cpuMilliseconds\",\"physicalMemoryBytes\",\"virtualMemoryBytes\",\"committedHeapBytes\",\"totalLaunchedMaps\",\"totalLaunchedReduces\",\"dataLocalMaps\",\"slotsMillisMaps\",\"slotsMillisReduces\",\"millisMaps\",\"millisReduces\",\"vcoresMillisMaps\",\"vcoresMillisReduces\",\"mbMillisMaps\",\"mbMillisReduces\",\"bytesRead\",\"bytesWritten\",\"duration\",\"totalDfsBytesRead\",\"totalDfsBytesWritten\",\"inputRecords\",\"outputRecords\",\"outputToInputRecordsRatio\",\"totalJoinInputRowCount\",\"totalJoinOutputRowCount\",\"inputPartitions\",\"outputPartitions\",\"joinInputRowCount\",\"joinOutputRowCount\",\"joinOutputToInputRowRatio\"\n}] " }, 
{ "title" : "Applications APIs", 
"url" : "adv/adv-unravel-apis/rest-api-rest-api/rest-api-applciations.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Applications APIs", 
"snippet" : "All commands require: Unravel server Unravel_Host: port # Port: assigned application id app_id: : recommendation | summary | status | errors | logs query string Unravel's Recommendations for the Application \/common\/app\/{app_id}\/recommendation curl -X GET \"http:\/\/playground.unraveldata.com:3000\/api\/v...", 
"body" : "All commands require: Unravel server Unravel_Host: port # Port: assigned application id app_id: : recommendation | summary | status | errors | logs query string Unravel's Recommendations for the Application \/common\/app\/{app_id}\/recommendation curl -X GET \"http:\/\/playground.unraveldata.com:3000\/api\/v1\/common\/app\/job_1543784013107_1631\/recommendation\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoiYWRtaW4iLCJ1c2VybmFtZSI6ImFkbWluIiwidGFncyI6IiIsImlhdCI6MTU1MTQwNzI5MiwiZXhwIjoxNTUxNDE0NDkyfQ.Kf8bIhSoWFVY2uaR0WHrPw_nq6k0iVH9piSnqbO6vLg\" Sample Output Summary [\n{\n\"parameter\": \"mapreduce.map.memory.mb\",\n\"current_value\": \"7596\",\n\"recommended_value\": \"3896\"\n},\n{\n\"parameter\": \"mapreduce.map.java.opts\",\n\"current_value\": \"-Xmx8192m\",\n\"recommended_value\": \"-Xmx3117m\"\n}\n] Application's Summary \/common\/app\/{app_id}\/summary curl -X GET \"http:\/\/playground.unraveldatalab.com:3000\/api\/v1\/common\/app\/parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\/summary\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoicGFyd2luZGVyIiwidXNlcm5hbWUiOiJwYXJ3aW5kZXIiLCJ0YWdzIjoiIiwiaWF0IjoxNTUxNDA1NTM3LCJleHAiOjE1NTE0MTI3Mzd9.vvcvLm6QTn9CBIrqtIx4JoOqyt66Wv5HDsfitPoAzLQ\" Sample Output Summary {\n \"@class\": \"com.unraveldata.annotation.HiveQueryAnnotation\",\n \"vcoreSeconds\": 0,\n \"memorySeconds\": 0,\n \"cents\": 0,\n \"version\": 1,\n \"source\": \"post-db\",\n \"kind\": \"hive\",\n \"id\": \"parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\",\n \"nick\": \"Hive Query\",\n \"status\": \"S\",\n \"user\": \"parwinder\",\n \"mrJobIds\": [\n \"job_1550559654567_8334\"\n ],\n \"duration\": 82120,\n \"startTime\": 1551405626487,\n \"numMRJobs\": 0,\n \"totalMRJobs\": 1,\n \"totalMapTasks\": 0,\n \"sm\": 0,\n \"km\": 0,\n \"kmu\": 0,\n \"fm\": 0,\n \"fmu\": 0,\n \"totalReduceTasks\": 0,\n \"sr\": 0,\n \"kr\": 0,\n \"kru\": 0,\n \"fr\": 0,\n \"fru\": 0,\n \"totalMapSlotDuration\": 0,\n \"totalReduceSlotDuration\": 0,\n \"totalDfsBytesRead\": 0,\n \"totalDfsBytesWritten\": 0,\n \"queryString\": \"\\nINSERT OVERWRITE DIRECTORY '\\\/user\\\/benchmark-user\\\/benchmarks\\\/oozie\\\/workflows\\\/road_accident_db\\\/output\\\/'\\nROW FORMAT DELIMITED\\nFIELDS TERMINATED BY '\\\\t'\\nSTORED AS TEXTFILE\\nselect reflect(\\\"java.lang.Thread\\\", \\\"sleep\\\", bigint(60000))\",\n \"type\": \"DML\",\n \"numEvents\": 0\n} Application's Status \/common\/app\/{app_id}\/status curl -X GET \"http:\/\/playground.unraveldatalab.com:3000\/api\/v1\/common\/app\/parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\/status\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoicGFyd2luZGVyIiwidXNlcm5hbWUiOiJwYXJ3aW5kZXIiLCJ0YWdzIjoiIiwiaWF0IjoxNTUxNDA1NTM3LCJleHAiOjE1NTE0MTI3Mzd9.vvcvLm6QTn9CBIrqtIx4JoOqyt66Wv5HDsfitPoAzLQ\" Sample Output {\n \"status\": \"Success\",\n \"message\": \"The app status of parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF is Success\"\n} Application's Errors \/common\/app\/{app_id}\/errors curl -X GET \"http:\/\/playground.unraveldatalab.com:3000\/api\/v1\/common\/app\/parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\/errors\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoicGFyd2luZGVyIiwidXNlcm5hbWUiOiJwYXJ3aW5kZXIiLCJ0YWdzIjoiIiwiaWF0IjoxNTUxNDA1NTM3LCJleHAiOjE1NTE0MTI3Mzd9.vvcvLm6QTn9CBIrqtIx4JoOqyt66Wv5HDsfitPoAzLQ\" Sample Output Noerrorsfound Application's Logs \/common\/app\/{app_id}\/logs curl -X GET \"http:\/\/playground.unraveldatalab.com:3000\/api\/v1\/common\/app\/parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\/logs\" -H \"accept: application\/json\" -H \"Authorization: JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJyZWFkT25seSI6ZmFsc2UsImlkIjoicGFyd2luZGVyIiwidXNlcm5hbWUiOiJwYXJ3aW5kZXIiLCJ0YWdzIjoiIiwiaWF0IjoxNTUxNDA1NTM3LCJleHAiOjE1NTE0MTI3Mzd9.vvcvLm6QTn9CBIrqtIx4JoOqyt66Wv5HDsfitPoAzLQ\" Sample Output Nologviewfoundforapphive_20190126160707_1d3f4e51-55eb-4cc7-8a8b-bca05d598920\n " }, 
{ "title" : "Data APIs", 
"url" : "adv/adv-unravel-apis/rest-api-rest-api/adv-rest-api-data.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Data APIs", 
"snippet" : "The query types fall in the following categories: KPIs Small files All output timestamps are in EPOCH. Variable parameters within the command are indicated by brackets {parameter}. Be sure to substitute your local host for UNRAVEL_HOST. KPIs curl -X GET \"http:\/\/ UNRAVEL_HOST Required parameters are:...", 
"body" : "The query types fall in the following categories: KPIs Small files All output timestamps are in EPOCH. Variable parameters within the command are indicated by brackets {parameter}. Be sure to substitute your local host for UNRAVEL_HOST. KPIs curl -X GET \"http:\/\/ UNRAVEL_HOST Required parameters are: : number of days numDays Schema [\n {\n \"st\": start time EPOCH_timestamp,\n \"et\": end time EPOCH_timestamp,\n \"nlaTb\": # of tables accessed,\n \"nlaPr\": # of partitions accessed,\n \"nlaQr\": # of queries accessing the table,\n \"nlaRi\": Total Read I\/O due to accessing the Tables,\n \"nlcTb\": # of tables created,\n \"nlcPr\": # of partitions created,\n \"nlcTz\": size of tables created,\n \"nlcPz\": size of partitions created,\n \"ntoTb\": total number of tables in the system,\n \"ntoPr\": total number of partitions in the system,\n \"nhtTb\": ,\n \"nwaTb\": ,\n \"ncoTb\": ,\n \"nhtPr\": ,\n \"nwaPr\": ,\n \"ncoPr\": ,\n \"rp\": ,\n \"rs\": ,\n \"fs\": ,\n \"users\": [\n \/\/ comma separated list of users\n ]\n }\n] curl -X GET \"http:\/\/localhost:3000\/api\/v1\/reports\/data\/kpis?numDays=1\" -H \"accept: application\/json\" -H \"Authorization: JWT token\" Sample Output [\n {\n \"st\": 1536795705,\n \"et\": 1536882105,\n \"nlaTb\": 15,\n \"nlaPr\": 0,\n \"nlaQr\": 57,\n \"nlaRi\": 101489411821,\n \"nlcTb\": 7,\n \"nlcPr\": 0,\n \"nlcTz\": 304768890,\n \"nlcPz\": 0,\n \"ntoTb\": 378,\n \"ntoPr\": 30061,\n \"nhtTb\": 19,\n \"nwaTb\": 0,\n \"ncoTb\": 359,\n \"nhtPr\": 1823,\n \"nwaPr\": 0,\n \"ncoPr\": 28238,\n \"rp\": 30061,\n \"rs\": 92544293666,\n \"fs\": 92544293666,\n \"users\": [\n \"root\",\n \"hdfs\"\n ]\n }\n] SMALL FILES curl -X GET \"http:\/\/ UNRAVEL_HOST Required parameters are: : small file report name\/id entity_id Schema {\n \"date\": date created EPOCH_timestamp,\n \"isSuccess\": report generation sucess\/failure,\n\n\/\/ report parameters\n \"avg_size_threshold\": small file size in bytes,\n \"num_files_threshold\": minimum number of small files,\n \"top_n_small_files\": # of directories to show,\n \"report_id\": report name\",\n \"root\": [\n \/\/ array[top_n_small files] directory\n {\n \"MaxFilesize\": maximum size file in directory,\n \"MinFilesize\": minimum size file in directory,\n \"NumFiles\": # of small files in the directory,\n \"DirPath\": directory path,\n \"TotalFilesize\": sum of file size in directory,\n \"AvgFilesize\": average file size\n }\n ]\n} curl -X GET \"http:\/\/localhost:3000\/api\/v1\/reports\/data\/small_file_report_details?entity_id=small_files_1536927781_5674\" -H \"accept: application\/json\" -H \"Authorization: JWT token\" Sample Output {\n \"date\": 1536899695,\n \"isSuccess\": true,\n \"avg_size_threshold\": 100000,\n \"num_files_threshold\": 100,\n \"top_n_small_files\": 5,\n \"report_id\": \"small_files_1536927781_5674\",\n \"root\": [\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"21523359\",\n \"DirPath\": \"\\\/97ovg\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n },\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"11302185\",\n \"DirPath\": \"\\\/98esm\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n },\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"7174452\",\n \"DirPath\": \"\\\/99acx\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n },\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"7174452\",\n \"DirPath\": \"\\\/97ovg\\\/97zqu\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n },\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"7174452\",\n \"DirPath\": \"\\\/97ovg\\\/99kgg\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n }\n ]\n} " }, 
{ "title" : "Operational APIs", 
"url" : "adv/adv-unravel-apis/rest-api-rest-api/operational-apis.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Operational APIs", 
"snippet" : "All queries are made with the http request GET in a curl command and using JSON for the query format. The available queries fall in the following categories: Cluster User Queue Jobs All commands require the following: Unravel_Host: Port: Start: End: token: sign in All timestamps are in EPOCH. You mu...", 
"body" : "All queries are made with the http request GET in a curl command and using JSON for the query format. The available queries fall in the following categories: \n Cluster \n User \n Queue \n Jobs All commands require the following: \n Unravel_Host: \n Port: \n Start: \n End: \n token: sign in All timestamps are in EPOCH. You must substitute your local or relevant information for fields indicated by RED CLUSTER Cluster Vcore\/Memory Usage curl -X GET \"http:\/\/ UNRAVEL_HOST Port resourceType Query End Interval Start token Required Parameters: \n Resoure Type cpu: vcores memory: returns memory in bytes \n Query Type allocated: allocated total: used \n Polling Interval Schema \n \n \n {\n EPOCH_timestamp : count\n} \n \n Allocated Vcore Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/cpu\/allocated?to=1538666160&interval=30m&from=1536273311\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\"\n Sample Output \n \n \n {\n \"1535360400000\": \"12\", \n \"1535364000000\": \"3.125\", \n \"1535367600000\": \"5.0303030303\"\n} \n Total Memory Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/memory\/total?to=1536273841&interval=1h&from=1536187441\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" Sample Output \n \n \n {\n \"1535364000000\": 47460,\n \"1535367600000\": 45087,\n \"1535371200000\": 37968\n} Cluster Nodes by health Return the count by node status\/health. curl -X GET \"http:\/\/ UNRAVEL_HOST Port End \n Interval \n Start token Additional required parameters: \n Polling Interval Schema \n Note \n \n \n { \n \"date\" : [ \n \/\/ array of polling EPOCH_timestamp\n EPOCH_timestamp\n ],\n \"total\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n },\n \"active\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n },\n \"lost\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }, \n \"unhealthy\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }, \n \"decommissioned\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }, \n \"rebooted\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }\n}\n \n Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/nodes?to=1536339111&interval=1h&from=1535734311\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\"\n Sample Output \n \n \n {\n \"date\": [\n 1535688000000,\n 1535691600000,\n 1535695200000,\n 1535698800000\n ],\n \"total\": {\n \"1535688000000\": 3,\n \"1535691600000\": 3,\n \"1535695200000\": 3,\n \"1535698800000\": 3\n },\n \"active\": {\n \"1535688000000\": 1,\n \"1535691600000\": 1,\n \"1535695200000\": 1,\n \"1535698800000\": 1\n },\n \"lost\": {\n \"1535688000000\": 0,\n \"1535691600000\": 0,\n \"1535695200000\": 0,\n \"1535698800000\": 0\n },\n \"unhealthy\": {\n \"1535688000000\": 1,\n \"1535691600000\": 1,\n \"1535695200000\": 1,\n \"1535698800000\": 1\n },\n \"decommissioned\": {\n \"1535688000000\": 1,\n \"1535691600000\": 1,\n \"1535695200000\": 1,\n \"1535698800000\": 1\n },\n \"rebooted\": {\n \"1535688000000\": 0,\n \"1535691600000\": 0,\n \"1535695200000\": 0,\n \"1535698800000\": 0\n }\n} APPLICATION VCore\/Memory Usage curl -X GET \"http:\/\/ UNRAVEL_HOST Port Query End \n AppType \n Interval \n Start token Required parameters: \n Query Type cpu: returns vcores as count memory: returns memory in bytes \n Application Type \n Polling Interval Schema \n \n \n {\n EPOCH_timestamp : count\n} \n Mapreduce Vcore Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/cpu?to=1536275883&groupBy=%7B%22type%22:%22applicationType%22,%22value%22:%5B%22MAPREDUCE%22%5D%7D&interval=1h&from=1536189483\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\"\n \n Spark Memory Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/memory?to=1536275883&groupBy=%7B%22type%22:%22applicationType%22,%22value%22:%5B%22SPARK%22%5D%7D&interval=1h&from=1536189483\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" User VCore\/Memory Usage curl -X GET \"http:\/\/ UNRAVEL_HOST Port Query End \n UserName \n Interval \n Start token Required parameters: \n Query Type cpu: returns vcores as count memory: returns memory in bytes \n User \n Polling Interval Schema \n \n \n {\n EPOCH_timestamp : result\n} \n \n Vcore Usage Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/cpu?to=1536276842&groupBy=%7B%22type%22:%22user%22,%22value%22:%5B%22root%22%5D%7D&interval=1h&from=1536190442\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" \n Memory Usage Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/memory?to=1536276842&groupBy=%7B%22type%22:%22user%22,%22value%22:%5B%22root%22%5D%7D&interval=1h&from=1536190442\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" QUEUE VCore\/Memory Usage curl -X GET \"http:\/\/ UNRAVEL_HOST Port Query End \n QueueName \n Interval \n Start token Additional required parameters: \n Query Type cpu: returns vcores as count memory: returns memory in bytes \n Queue \n Polling Interval Schema \n \n \n {\n EPOCH_timestamp : result\n} \n Vcore Usage Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/cpu?to=1536277362&groupBy=%7B%22type%22:%22queue%22,%22value%22:%5B%22root.users.root%22%5D%7D&interval=1h&from=1536190962\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" \n Memory Usage Example # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/tagged\/memory?to=1536277362&groupBy=%7B%22type%22:%22queue%22,%22value%22:%5B%22root.users.root%22%5D%7D&interval=1h&from=1536190962\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" JOBS Returns average number of jobs by Groupby type. curl -X GET \"http:\/\/ UNRAVEL_HOST Port End \n GroupBy \n Interval \n Start token Required parameters \n Groupby: \n Polling Interval \n State Example Schema - State \n \n \n {\n \"date\": [\n EPOCH_timestamp\n ],\n \"RUNNING\": {\n EPOCH_timestamp: average\n },\n \"ACCEPTED\": {\n EPOCH_timestamp: average\n }\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/jobs\/count?to=1536275822&groupBy=state&interval=1h&from=1535671022\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" Sample Output \n \n \n {\n \"date\": [\n 1535626800000\n ],\n \"RUNNING\": {\n \"1535626800000\": \"1.3333333333\"\n },\n \"ACCEPTED\": {\n \"1535695200000\": \"1\"\n }\n} \n Application Type Schema - Application \n \n \n {\n { \n \"date\" : [ \n \/\/ array of polling EPOCH_timestamp\n EPOCH_timestamp\n ],\n\n APP_TYPE : { \n \/\/ APP_TYPE mr | hive | spark | pig | cascading | impala | tez \n \/\/ for each polling interval where the app type is running\n EPOCH_timestamp: count\n }\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/jobs\/count?to=1536280789&groupBy=applicationType&interval=1h&from=1535675989\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" Sample Output - Application \n \n \n {\n \"date\": [\n 1535644800000,\n 1535695200000,\n 1535698800000,\n 1535702400000,\n 1535706000000,\n 1535709600000,\n 1535731200000,\n 1535745600000,\n 1535814000000,\n 1535817600000,\n 1535904000000,\n 1535961600000,\n 1535990400000,\n 1536030000000,\n 1536037200000,\n 1536040800000\n ],\n \"MAPREDUCE\": {\n \"1535644800000\": \"1\",\n \"1535695200000\": \"1\",\n \"1535698800000\": \"1\",\n \"1535702400000\": \"1\",\n \"1535706000000\": \"1\",\n \"1535709600000\": \"1\",\n \"1535731200000\": \"1\",\n \"1535745600000\": \"1\",\n \"1535814000000\": \"1\",\n \"1535817600000\": \"1\",\n \"1535904000000\": \"1\",\n \"1535961600000\": \"1.5\",\n \"1535990400000\": \"1\",\n \"1536030000000\": \"1\",\n \"1536037200000\": \"1\",\n \"1536040800000\": \"1.1538461539\"\n },\n \"SPARK\": {\n \"1535698800000\": \"1\"\n }\n} \n User Example Schema - User \n \n \n {\n \"date\": [\n EPOCH_timestamp\n ],\n userName: {\n EPOCH_timestamp: average\n }\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/jobs\/count?to=1536281730&groupBy=user&interval=1h&from=1535676930\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" Sample Output - User \n \n \n {\n \"date\": [\n 1535695200000,\n 1535698800000,\n 1535702400000,\n 1535706000000,\n 1535709600000,\n 1535731200000,\n 1535745600000,\n 1535814000000,\n 1535817600000,\n 1535904000000\n ],\n \"root\": {\n \"1535695200000\": \"1\",\n \"1535709600000\": \"1\",\n \"1535731200000\": \"1\",\n \"1535745600000\": \"1\",\n \"1535814000000\": \"1\",\n \"1535817600000\": \"1\",\n \"1535904000000\": \"1\"\n },\n \"hdfs\": {\n \"1535698800000\": \"1\",\n \"1535702400000\": \"1\",\n \"1535706000000\": \"1\"\n }\n} \n Queue Example Schema - Queue \n \n \n {\n \"date\": [\n EPOCH_timestamp\n ],\n queue Name: {\n EPOCH_timestamp: average\n }\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/jobs\/count?to=1536339111&groupBy=queue&interval=1h&from=1535734311\" -H \"accept: application\/json\" -H \"Authorization: JWT 12345..68798\" Sample - Queue \n \n \n {\n \"date\": [\n 1535695200000,\n 1535698800000,\n 1535702400000,\n 1535706000000,\n 1535709600000,\n 1535731200000,\n 1535745600000,\n 1535814000000,\n 1535817600000,\n 1535904000000,\n 1535961600000,\n 1535990400000,\n 1536030000000,\n 1536037200000,\n 1536040800000,\n 1536044400000,\n 1536048000000,\n 1536051600000,\n 1536055200000,\n 1536058800000,\n 1536076800000\n ],\n \"root.users.root\": {\n \"1535695200000\": \"1\",\n \"1535709600000\": \"1\",\n \"1535731200000\": \"1\",\n \"1535745600000\": \"1\",\n \"1535814000000\": \"1\",\n \"1535817600000\": \"1\",\n \"1535904000000\": \"1\",\n \"1535961600000\": \"1\",\n \"1535990400000\": \"1\",\n \"1536030000000\": \"1\",\n \"1536037200000\": \"1\",\n \"1536040800000\": \"1\",\n \"1536076800000\": \"1\"\n },\n \"root.users.hdfs\": {\n \"1535698800000\": \"1\",\n \"1535702400000\": \"1\",\n \"1535706000000\": \"1\",\n \"1535961600000\": \"2\",\n \"1536040800000\": \"1.1304347826\",\n \"1536044400000\": \"1\",\n \"1536055200000\": \"1.3333333333\",\n \"1536058800000\": \"1\"\n },\n \"root.users.user11\": {\n \"1535698800000\": \"1\"\n },\n \"root.abcdefghijklmnopqrstuvwxyz\": {\n \"1536037200000\": \"1\",\n \"1536040800000\": \"1\",\n \"1536044400000\": \"1\"\n }\n} " }, 
{ "title" : "Reports APIs", 
"url" : "adv/adv-unravel-apis/rest-api-rest-api/rest-api-reports-apis.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Reports APIs", 
"snippet" : "The query types fall in the following categories: Variable parameters within the command are indicated in RED All commands require: Unravel_Host: Port: app_id: Returns charge back by application type Gives the count of all applications in all queues for all users across all clusters. curl -X GET \"ht...", 
"body" : "The query types fall in the following categories: Variable parameters within the command are indicated in RED All commands require: Unravel_Host: Port: app_id: Returns charge back by application type Gives the count of all applications in all queues for all users across all clusters. curl -X GET \"http:\/\/ UNRAVEL_HOST Start End token Required parameters: Parameters gte lte Schena {\n \"cb\": [\n \/\/ array organizing by application type\n {\n \"ms\": memory usage seconds,\n \"count\": application count,\n \"v1\": \"application type (mr | spark)\",\n \"vs\": vcore usage in second\n }\n ]\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/search\/cb\/appt?from=1536670860<to=1536757260\" -H \"accept: application\/json\" -H \"Authorization: JWT token\"\n Sample Output {\n \"cb\": [\n {\n \"ms\": 2324825154,\n \"count\": 4332,\n \"v1\": \"mr\",\n \"vs\": 1512734\n },\n {\n \"ms\": 298989641,\n \"count\": 75,\n \"v1\": \"spark\",\n \"vs\": 120404\n }\n ]\n} Chargeback by user \/search\/cb\/appt\/user Required parameters: gte lte Schema {\n \"cb\": [\n \/\/ array by application type\n {\n \"count\": application count,\n \"v1\": application type (mr, | spark), ,\n \"cb\": [\n \/\/array of users or queues (depending on command)\n {\n \"ms\": memory usage in milliseconds,,\n \"count\": application count,,\n \"v2\": userName | queueName,,\n \"vs\": vcore usage in seconds\n }\n ]\n }\n ]\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/search\/cb\/appt\/user?from=1536676860<to=1536763260\" -H \"accept: application\/json\" -H \"Authorization: JWT \"token\"\" Sample Output {\n \"cb\": [\n {\n \"count\": 27,\n \"v1\": \"mr\",\n \"cb\": [\n {\n \"ms\": 1974235,\n \"count\": 27,\n \"v2\": \"root.users.root\",\n \"vs\": 798\n }\n ]\n }\n ]\n} Chargeback by queue \/search\/cb\/appt\/queue Required parameters: gte lte Schema {\n \"cb\": [\n \/\/ array by application type\n {\n \"count\": application count,\n \"v1\": application type (mr, | spark), ,\n \"cb\": [\n \/\/array of users or queues (depending on command)\n {\n \"ms\": memory usage in milliseconds,,\n \"count\": application count,,\n \"v2\": userName | queueName,,\n \"vs\": vcore usage in seconds\n }\n ]\n }\n ]\n} # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/search\/cb\/appt\/user?from=1536676860<to=1536763260\" -H \"accept: application\/json\" -H \"Authorization: JWT \"token\"\" Sample Output {\n \"cb\": [\n {\n \"count\": 27,\n \"v1\": \"mr\",\n \"cb\": [\n {\n \"ms\": 1974235,\n \"count\": 27,\n \"v2\": \"root.users.root\",\n \"vs\": 798\n }\n ]\n }\n ]\n} Chargeback app type Return report by application type for a specific queue. curl -X GET \"http:\/\/ UNRAVEL_HOST AppType End Start QueueName Required parameters: AppType: Queue Name Start: End: Schema {\n \"cb\": [\n \/\/ array by application type\n {\n \"ms\": memory usage in milliseconds,\n \"count\": application count,\n \"v2\": userName | queueName,\n \"vs\": vcore usage in seconds\n }\n ]\n}\n Example App Type = mr # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/search\/cb\/user?appt=mr>e=1536677580<e=1536763980&queue=root.users.root\" -H \"accept: application\/json\" -H \"Authorization: JWT token\"\n Sample {\n \"cb\": [\n {\n \"ms\": 2324825154,\n \"count\": 4332,\n \"v1\": \"mr\",\n \"vs\": 1512734\n },\n {\n \"ms\": 298989641,\n \"count\": 75,\n \"v1\": \"spark\",\n \"vs\": 120404\n }\n ]\n} Clusters Cluster Summary curl -X GET \"http:\/\/ UNRAVEL_HOST Required parameters Mode: Start: End: Output for mode user or queue {\n \"userStats\": [\n\/\/ array of users | queues currently on cluster\n {\n \"root\": {\n \"running\": {\n \"min\": minimum applications running,,\n \"max\": minimum applications running,\n \"mean\": average appication number running,\n \"stddev\": standard deviation\n },\n \"memory\": { \/\/ see running above },\n \"pending\": { \/\/ see running above },\n \"vcores\": { \/\/ see running above }\n }\n }\n ]\n} Example mode = user # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/reports\/operational\/clusterstats?et=1536744589000&mode=user&st=1536658189000\" -H \"accept: application\/json\" -H \"Authorization: JWT \"token\"\"\n Sample output mode=user {\n \"userStats\": [\n {\n \"root\": {\n \"running\": {\n \"min\": 1,\n \"max\": 1,\n \"mean\": 1,\n \"stddev\": 0\n },\n \"memory\": {\n \"min\": 1024,\n \"max\": 16384,\n \"mean\": 7040,\n \"stddev\": 7099.0974074174\n },\n \"pending\": {\n \"min\": 0,\n \"max\": 0,\n \"mean\": 0,\n \"stddev\": 0\n },\n \"vcores\": {\n \"min\": 1,\n \"max\": 3,\n \"mean\": 2,\n \"stddev\": 0.81649658092773\n }\n }\n }\n ]\n} Example mode = queue # curl -X GET \"http:\/\/localhost:3000\/api\/v1\/reports\/operational\/clusterstats?et=1536744589000&mode=queue&st=1536658189000\" -H \"accept: application\/json\" -H \"Authorization: JWT \"token\"\"\n Sample output mode=queue {\n \"queueStats\": [\n {\n \"root.users.root\": {\n \"running\": {\n \"min\": 1,\n \"max\": 1,\n \"mean\": 1,\n \"stddev\": 0\n },\n \"memory\": {\n \"min\": 1024,\n \"max\": 16384,\n \"mean\": 7040,\n \"stddev\": 7099.0974074174\n },\n \"pending\": {\n \"min\": 0,\n \"max\": 0,\n \"mean\": 0,\n \"stddev\": 0\n },\n \"vcores\": {\n \"min\": 1,\n \"max\": 3,\n \"mean\": 2,\n \"stddev\": 0.81649658092773\n }\n }\n }\n ]\n} Output for mode=app { [ appID : application id , type : memorySeconds \/vcoreSeconds vcore : vcore value memory : memory value ] } Cluster Workload curl -X GET \"http:\/\/ UNRAVEL_HOST Start End token Required parameters: Start: End: token: sign in reportby Schema \/\/ work load by month one of more months with the application count for month\n\/\/ minimum of one month\n { timestamp: appcount[,timestamp: appcount] }\n\n\/\/ work load by hour array of 25 hours\n [\n { timestamp: appcount }, ... { timestamp: appcount }\n ]\n\n\/\/ work load by day - array for each days contained with the time period (Mon - Sun)\n\/\/ minimum of one day\n [\n { timestamp: appcount }\n ]\n\n\/\/ work load by hour\/day:array for each day (Mon - Sun) by hour\n\/\/ minimum of 24 hours for one day\n [\n { timestamp: appcount }\n ]\n\n curl -X GET \"http:\/\/localhost:3000\/api\/v1\/reports\/operational\/clusterworkload?gte=1536777000Z<=1536863400Z&reportBy=month\" -H \"accept: application\/json\" -H \"Authorization: JWT token\"\n Sample output mode {\"1536777000\":96,\"1536863400\":4} [ {\"1536813000000\":0},{\"1536816600000\":0},{\"1536820200000\":0},{\"1536823800000\":10},{\"1536827400000\":7},{\"1536831000000\":13},{\"1536834600000\":10},{\"1536838200000\":31},{\"1536841800000\":0},{\"1536845400000\":0},{\"1536849000000\":4},{\"1536852600000\":2},{\"1536856200000\":0},{\"1536859800000\":0},{\"1536863400000\":0},{\"1536867000000\":0},{\"1536870600000\":0},{\"1536874200000\":0},{\"1536877800000\":0},{\"1536881400000\":0},{\"1536885000000\":0},{\"1536888600000\":0},{\"1536892200000\":0},{\"1536895800000\":6},{\"1536899400000\":2} [ {\"1536777000000\":96},{\"1536863400000\":7} ] [{\"1536813000000\":0},{\"1536816600000\":0},{\"1536820200000\":0},{\"1536823800000\":10},{\"1536827400000\":7},{\"1536831000000\":13},{\"1536834600000\":10},{\"1536838200000\":31},{\"1536841800000\":0},{\"1536845400000\":0},{\"1536849000000\":4},{\"1536852600000\":2},{\"1536856200000\":0},{\"1536859800000\":0},{\"1536863400000\":0},{\"1536867000000\":0},{\"1536870600000\":0},{\"1536874200000\":0},{\"1536877800000\":0},{\"1536881400000\":0},{\"1536885000000\":0},{\"1536888600000\":0},{\"1536892200000\":0},{\"1536895800000\":6},{\"1536899400000\":2}] " }, 
{ "title" : "Workflow APIs", 
"url" : "adv/adv-unravel-apis/rest-api-rest-api/rest-api-workflow-apis.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel APIs \/ REST API \/ Workflow APIs", 
"snippet" : "endpoint All commands require: Unravel_Host: Port: app_id: Returns list of Workflows \/workflows No Parameters Response Schema Code: 200 type: array items: type: string curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/workflows\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" Sample Output wi...", 
"body" : " endpoint All commands require: Unravel_Host: Port: app_id: Returns list of Workflows \/workflows No Parameters Response Schema Code: 200\ntype: array\nitems:\n type: string curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/workflows\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" Sample Output with only One Missing SLA: { \"Benchmark: Road_Accident_2005-2016\": { \"key\": \"Benchmark: Road_Accident_2005-2016\", \"doc_count\": 565, \"duration_stats\": { \"count\": 565, \"min\": 21000, \"max\": 772000, \"avg\": 174877.8761061947, \"sum\": 98806000, \"sum_of_squares\": 17972790000000, \"variance\": 1227976236.197041, \"std_deviation\": 35042.49186626204, \"std_deviation_bounds\": { \"upper\": 244962.85983871878, \"lower\": 104792.8923736706 } } } } Returns list of Workflows which are missing SLA \/workflows\/missing_sla Parameters from to Response Schema Code: 200\ntype: array\nitems:\n type: string curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/workflows\/missing_sla?from=2019-02-01&to=2019-03-06\" -H \"accept: application\/json\" -H \"Authorization: JWT TOKEN\" Sample Output with no Missing SLAs. [] " }, 
{ "title" : "Use Case - Auto Actions and Pagerduty", 
"url" : "adv/adv-unravel-apis/use-case---auto-actions-and-pagerduty.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel APIs \/ Use Case - Auto Actions and Pagerduty", 
"snippet" : "The Autoaction's template provides the ability to send alerts via email to one or more recipients and\/or create a HTTP endpoint allowing integration with a 3rd party tool, e.g., Slack. These two options are specified when creating\/editing an auto action. Once entered nothing further needs to be done...", 
"body" : "The Autoaction's template provides the ability to send alerts via email to one or more recipients and\/or create a HTTP endpoint allowing integration with a 3rd party tool, e.g., Slack. These two options are specified when creating\/editing an auto action. Once entered nothing further needs to be done for notifications to be sent. Unravel has developed a third option allowing you to use Pagerduty to send notifications to one or more users through Unravel's Autoactions API. Currently, this action is initiated outside of Unravel's Server. For the integration you need to complete 2 setups: Set up a service at Pagerduty and specify who should be notified and how (email, etc.), and Run a python script on your local machine, specifying the Unravel server and Pagerdutyinformation. The python script \n must pagerduty Using pagerduty for notifications Set up a pagerduty service. \n 1 Configuration Services https:\/\/www.pagerduty.com \n 2 Add New Service \n 3 Use our API directly Events API v2 \n 4 Add Service \n 5 I ntegration Key Integrations Run the Unravel API python script on your local computer. \n 1 https:\/\/github.com\/Unravel-Andy\/unravel-api-demo-v1 # git clone https:\/\/github.com\/Unravel-Andy\/unravel-api-demo-v1 \n 2 cd unravel-api-demo-v1 # cd unravel-api-demo-v1\n# ls\nREADME.md api-test.py \n 3 # python .\/api-test.py \n 4 Unravel Autoactions API Address The Autoactions API address has the form of http[s]:\/\/UNRAVEL_HOST_IP\/api\/v1\/autocactions UNRAVEL_HOST_IP https:\/\/playground.unraveldata.com. Enter the pagerduty integration key Pagerduty API key \n 5 Start To scroll within list, click within the Autoactions Name \n 6. Navigate to the Auto Actions page ( Manage Auto Actions Creating Auto Actions \n 7. In our example there were 2 active autoactions, \n Kill job hogging the cluster \n Rogue App AA #1. \n Kill job hogging the cluster \n 8. \n \n Kill job hogging the cluster \n Updated Unravel UI \n Updated Unravel API \n Kill job hogging the cluster \n 9 \n Sample sms message \n Sample email The python script \n must pagerduty " }, 
{ "title" : "Unravel Monitoring Service", 
"url" : "adv/unravel-monitoring-service.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Monitoring Service", 
"snippet" : "Monitoring service is lightweight daemon which allows you to monitor various Unravel components, e.g., Zookeeper, Kafka, ElasticSearch, DB, disk usage. Monitoring service consists of: JMX MBeans How to Write Jolokia JMX MBean Monitors Monitors REST API Configuration Disk Monitoring JMX Client Monito...", 
"body" : "Monitoring service is lightweight daemon which allows you to monitor various Unravel components, e.g., Zookeeper, Kafka, ElasticSearch, DB, disk usage. Monitoring service consists of: JMX MBeans How to Write Jolokia JMX MBean Monitors Monitors REST API Configuration Disk Monitoring JMX Client Monitors REST API " }, 
{ "title" : "Configuration", 
"url" : "adv/unravel-monitoring-service/monitoring-conf.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Monitoring Service \/ Configuration", 
"snippet" : "To configure Email: see Email Alerts. Disk monitoring: see Disk Monitoring JavaScript Rules Property\/Description Set By User Unit Default com.unraveldata.monitoring.js_rules.eval.interval Frequency, in seconds, to evaluate monitoring rules based on javascript. 0: disables evaluation. sec 60 com.unra...", 
"body" : "To configure Email: see Email Alerts. Disk monitoring: see Disk Monitoring JavaScript Rules Property\/Description Set By User Unit Default com.unraveldata.monitoring.js_rules.eval.interval Frequency, in seconds, to evaluate monitoring rules based on javascript. 0: disables evaluation. sec 60 com.unraveldata.monitoring.js_rules.cool.off.period Alert action cool-off period. sec 1800 File System Related Rules File system related rules (disk usage) are evaluated separately. Property\/Description Set By User Unit Default com.unraveldata.monitoring.fs_rules.eval.interval Frequency, in seconds, to evaluate filesystem specific monitoring rules. 0: disables evaluation. sec 60 DB Status Monitoring Property\/Description Set By User Unit Default com.unraveldata.monitoring.db.status.check.interval Frequency, in seconds, database basic status is queried 0: disables evaluation. sec 30 DB Performance Monitoring Property\/Description Set By User Unit Default com.unraveldata.monitoring.db.performance.check.interval Frequency, in seconds, database performance metrics are gathered. 0: disables evaluation. sec 30 com.unraveldata.monitoring.db.performance.query Default query; select count(*) from (select 1 from blackboards limit 1000) b see note Zookeeper Monitoring Property\/Description Set By User Unit Default com.unraveldata.monitoring.zookeeper.check.interval Frequency, in seconds, Zookeeper metrics should be queried. 0: disables monitoring. sec 30 com.unraveldata.monitoring.zookeeper.history.size How many data sets consider in computing average values (for historical data). integer 5 Kafka Monitoring Property\/Description Set By User Unit Default com.unraveldata.monitoring.kafka.check.interval Frequency, in seconds,Kafka metrics should be queried. 0: disables monitoring. sec 30 com.unraveldata.monitoring.kafka.ignore.topics Zookeeper topics to be ignored during Kafka monitoring. Topics can be ignored from different reasons, e.g. all internal topics are ignored. Use a comma separated list to specify more than one. CSL __consumer_offsets, connect-configs, connect-offsets, connect-status com.unraveldata.monitoring.kafka.history.size Number of data sets stored in the memory and used as historical data. integer 5 ElasticSearch Monitoring Property\/Description Set By User Unit Default com.unraveldata.monitoring.elastic.check.interval Frequency, in seconds,ElasticSearch metrics should be queried. 0: disables monitoring. sec 30 com.unraveldata.monitoring.elastic.history.size Number of data sets stored in the memory and used as historical data. integer 5 " }, 
{ "title" : "Disk Monitoring", 
"url" : "adv/unravel-monitoring-service/disk-monitoring.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Monitoring Service \/ Disk Monitoring", 
"snippet" : "For all properties: Time is specified in seconds. Setting a timing rule to 0 disables it. Size as percentages, and Lists as comma separated lists File system monitoring rule This rule sets the timing interval for partition (volume) and folder (directory) monitoring Property Description Default com.u...", 
"body" : "For all properties: Time is specified in seconds. Setting a timing rule to 0 disables it. Size as percentages, and Lists as comma separated lists File system monitoring rule This rule sets the timing interval for partition (volume) and folder (directory) monitoring Property Description Default com.unraveldata.monitoring.fs_rules.eval.interval Frequency to evaluate file system rule. 90 Partitions (volumes) monitoring Given list of partitions are monitored. Once partition usage exceeds given threshold (configured as high watermark limit) email alert is sent. Another email is not sent until disk usage goes below low watermark limit and above high watermark limit again. Property Description Default com.unraveldata.monitoring.fs.partitions.check.interval Frequency to check monitored partitions. 60 com.unraveldata.monitoring.fs.partitions.csv Comma separated list of monitored partitions. Symbolic links are supported. \/srv\/unravel,\/usr\/local\/unravel com.unraveldata.monitoring.fs.partitions.high.watermark Trigger alert if disk usage is over this limit. Do not trigger next alert until disk usage is below low watermark limit. 85 com.unraveldata.monitoring.fs.partitions.low.watermark If disk usage goes below this limit then disk alert can be triggered again. 70 Deprecated Partition Properties The following properties have been deprecated and replaced with the above partition properties. We strongly suggest remove these deprecated properties from unravel.properties and replace them with the above properties. If you have both the new and deprecated properties defined, the value of the deprecated property is used. For instance, if you have the defined both: new: com.unraveldata.monitoring.fs.partitions.high.watermark=80 deprecated: com.unraveldata.kafka.monitor.disk.high.watermark=60 Unravel uses the deprecated property value, so triggering occurs when disk usage is > 60% not 80% Property Replaced by com.unraveldata.filesystem.volumes.csv com.unraveldata.monitoring.fs.partitions.csv com.unraveldata.kafka.monitor.disk.high.watermark com.unraveldata.monitoring.fs.partitions.high.watermark com.unraveldata.kafka.monitor.disk.low.watermark com.unraveldata.monitoring.fs.partitions.low.watermark Folders monitoring Property Note Default com.unraveldata.monitoring.fs.folders.check.interval Frequency to check monitored folders (directories). 0 com.unraveldata.monitoring.fs.folder_limit.pairs.csv Comma separated list of monitored folders and their size limits, <foldername>:<folderlimit>[KB|MB|GB], e.g., \/srv\/unravel:100GB,\/usr\/local\/unravel:200GB. The folder must be a fully qualified and may be a symbolic link. If no size unit is specified, size is evaluated as bytes. \/srv\/unravel:100GB com.unraveldata.monitoring.fs.folders.low.watermark Percentage of folder limit above. Once an alert is triggered, a new alert is only triggered if the size first drops below this limit and then rises above the folder limit. The purpose is to prevent false and repeated alerts. Example: \/srv\/unravel:100GB with a low water mark of 80% The first time (\/srv\/unravel size > 100GB) the alert is triggered. No new alert is triggered unless (\/srv\/unravel size drops < 80GB) and then 80 " }, 
{ "title" : "JMX Client", 
"url" : "adv/unravel-monitoring-service/jmx-client.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Monitoring Service \/ JMX Client", 
"snippet" : "Monitoring service has an associated JMX client which can be used mainly for development and testing purposes. JMX client can be found in the unravel-data\/core-unravel monitoring\/monitor\/src\/main\/javascript\/unravel-monitoring-client\/ It is enough to start index.html config.js index.html JMX client d...", 
"body" : "Monitoring service has an associated JMX client which can be used mainly for development and testing purposes. JMX client can be found in the unravel-data\/core-unravel monitoring\/monitor\/src\/main\/javascript\/unravel-monitoring-client\/ It is enough to start index.html config.js index.html JMX client displays monitoring beans. In the right panel below JSON response data is associated REST API call by which given data can be retrieved. " }, 
{ "title" : "Monitors REST API", 
"url" : "adv/unravel-monitoring-service/monitors-rest-api.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Monitoring Service \/ Monitors REST API", 
"snippet" : "sdfa Table of Contents PartitionInfo Monitor DbStatus Monitor DbPerformance Monitor Zookeeper Monitor Kafka Monitor Elastic Monitor Each monitor can be accessed by REST API (which is exposed by Jolokia). Example of Jolokia response { \"request\": { \"mbean\": \"com.unraveldata:group=Database,name=DbStatu...", 
"body" : "sdfa Table of Contents PartitionInfo Monitor DbStatus Monitor DbPerformance Monitor Zookeeper Monitor Kafka Monitor Elastic Monitor Each monitor can be accessed by REST API (which is exposed by Jolokia). Example of Jolokia response {\n \"request\": {\n \"mbean\": \"com.unraveldata:group=Database,name=DbStatus,type=Monitoring\",\n \"attribute\": \"MBeanStatus\",\n \"type\": \"read\"\n },\n \"value\": {\n \"lastUpdated\": \"2018-05-15T08:31:36\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n },\n \"timestamp\": 1526373099,\n \"status\": 200\n} There is value Set MONITOR_HOST Since we can have more unravel nodes I used term \"monitor_host\" to point to node where monitoring service is running PartitionInfo Monitor MBean Status http:\/ MONITOR_HOST MBeanStatus - Output {\n \"lastUpdated\": \"2018-05-14T11:30:17\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Partitions Info http:\/\/ MONITOR_HOST PartitionInfo - Output {\n \"\\\/home\": {\n \"diskUsagePercentage\": 18,\n \"lastUpdated\": \"2018-05-14T11:30:57\",\n \"partition\": \"\\\/home\",\n \"freeSpace\": 812976791552,\n \"totalSpace\": 990715518976\n },\n \"\\\/\": {\n \"diskUsagePercentage\": 18,\n \"lastUpdated\": \"2018-05-14T11:30:57\",\n \"partition\": \"\\\/\",\n \"freeSpace\": 812976791552,\n \"totalSpace\": 990715518976\n },\n \"\\\/tmp\": {\n \"diskUsagePercentage\": 18,\n \"lastUpdated\": \"2018-05-14T11:30:57\",\n \"partition\": \"\\\/tmp\",\n \"freeSpace\": 812976791552,\n \"totalSpace\": 990715518976\n }\n} DbStatus Monitor MBean Status http:\/\/ MONITOR_HOST MBeanStatus - Output {\n \"lastUpdated\": \"2018-05-14T11:31:49\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Connection Ok http:\/\/ MONITOR_HOST ConnectionOk - Output true DbPerformance Monitor MBean Status http:\/\/ MONITOR_HOST MBeanStatus - Output {\n \"lastUpdated\": \"2018-05-14T11:45:06\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Connection Ok http:\/\/ MONITOR_HOST ConnectionOk - Output true Last Query Duration http:\/\/ MONITOR_HOST LastQueryDuration - Output 14 Query Timed Out http:\/\/ MONITOR_HOST QueryTimedOut false Query Exception http:\/\/ MONITOR_HOST QueryException - Output RuntimeException: Cannot read configuration Zookeeper Monitor MBean Status http:\/\/ MONITOR_HOST MBeanStatus {\n \"lastUpdated\": \"2018-05-14T11:33:32\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Recent Data http:\/\/ MONITOR_HOST Response value [\n {\n \"latencyMax\": 0,\n \"leader\": false,\n \"follower\": false,\n \"created\": \"2018-05-14T11:33:52\",\n \"connectionsCount\": 0,\n \"mode\": null,\n \"latencyAvg\": 0,\n \"latencyMin\": 0,\n \"port\": 2000,\n \"outstandingCount\": 0,\n \"host\": \"localhost\",\n \"nodeCount\": 0,\n \"ok\": false\n }\n] Historical Data http:\/\/ MONITOR_HOST Response value [\n {\n \"latencyMinAvg\": 0,\n \"connectionsCountTrend\": 0,\n \"nodeCountTrend\": 0,\n \"latencyMinTrend\": 0,\n \"outstandingCountTrend\": 0,\n \"latencyAvg\": 0,\n \"latencyMaxTrend\": 0,\n \"port\": 2000,\n \"isOkCount\": 0,\n \"host\": \"localhost\",\n \"outstandingCountAvg\": 0,\n \"connectionsCountAvg\": 0,\n \"leaderCount\": 0,\n \"followerCount\": 0,\n \"latencyAvgTrend\": 0,\n \"latencyMaxAvg\": 0,\n \"nodeCountAvg\": 0,\n \"isNotOkCount\": 5\n }\n] Kafka Monitor MBean Status http:\/\/ MONITOR_HOST {\n \"lastUpdated\": \"2018-05-14T11:55:24\",\n \"lastUpdateSuccessful\": true,\n \"errorMessage\": null,\n \"initialized\": true\n} Recent Data http:\/\/ MONITOR_HOST {\n \"consumerGroups\": [\n {\n \"groupName\": \"0_1790010567376612\",\n \"consumerTopicList\": [\n {\n \"consumerHost\": \"172.16.1.111\",\n \"clientId\": \"unravel_diag_meta\",\n \"lag\": 0,\n \"currentOffset\": 0,\n \"partitionId\": 0,\n \"consumerId\": \"unravel_diag_meta-1971faac-1d57-462b-b73e-c45e9c3cee52\",\n \"topicName\": \"meta\",\n \"logEndOffset\": 0\n }\n ]\n }\n ],\n \"created\": \"2018-05-14T12:06:38\",\n \"kafkaRunning\": true,\n \"topicList\": [\n \"aa2\",\n \"event\",\n \"hitdoc\",\n \"hive\",\n \"hive-hook\",\n \"hive-hook-emr\",\n \"jc\",\n \"live\",\n \"meta\",\n \"metrics\",\n \"mr\",\n \"spark\",\n \"workflow\"\n ],\n \"topicsWithoutConsumer\": [\n \"aa2\",\n \"event\",\n \"hitdoc\",\n \"hive\",\n \"hive-hook\",\n \"hive-hook-emr\",\n \"jc\",\n \"live\",\n \"metrics\",\n \"mr\",\n \"spark\",\n \"workflow\"\n ]\n} Historical Data http:\/\/ MONITOR_HOST Response value N RecentData com.unraveldata.monitoring.kafka.history.size Elastic Monitor MBean Status http:\/\/ MONITOR_HOST MBeanStatus {\n \"lastUpdated\": \"2018-05-14T12:15:02\",\n \"lastUpdateSuccessful\": false,\n \"errorMessage\": \"Cannot retrieve ElasticSearch data: Cannot get ElasticSearch data. Address: sako1:4171\",\n \"initialized\": true\n} RecentData http:\/\/ MONITOR_HOST RecentData {\n \"running\": true,\n \"nodes\": [\n {\n \"indices\": {\n \"search\": {\n \"fetchTimeInMillis\": 0,\n \"queryTimeInMillis\": 0,\n \"scrollTimeInMillis\": 0\n },\n \"docs\": {\n \"deleted\": 0,\n \"count\": 0\n },\n \"indexing\": {\n \"noopUpdateTotal\": 0,\n \"indexTimeInMillis\": 0,\n \"throttleTimeInMillis\": 0,\n \"indexCurrent\": 0,\n \"deleteTimeInMillis\": 0,\n \"deleteCurrent\": 0,\n \"indexTotal\": 0,\n \"indexFailed\": 0,\n \"deleteTotal\": 0,\n \"throttled\": false\n },\n \"get\": {\n \"missingTimeInMillis\": 0,\n \"existsTimeInMillis\": 0,\n \"timeInMillis\": 0\n },\n \"store\": {\n \"sizeInBytes\": 0,\n \"throttleTimeInMillis\": 0\n }\n },\n \"roles\": [\n \"master\",\n \"data\",\n \"ingest\"\n ],\n \"name\": \"unravel_s_1\",\n \"timestamp\": 1526327791715\n }\n ],\n \"port\": 4171,\n \"created\": \"2018-05-14T19:56:33\",\n \"host\": \"sako1\",\n \"clusterHealth\": {\n \"activeShardsPercentAsNumber\": 100,\n \"numberOfPendingTasks\": 0,\n \"numberOfInFlightFetch\": 0,\n \"timedOut\": false,\n \"activePrimaryShards\": 0,\n \"unassignedShards\": 0,\n \"numberOfFailedNodes\": 0,\n \"numberOfNodes\": 1,\n \"taskMaxWaitingInQueueMillis\": 0,\n \"initializingShards\": 0,\n \"numberOfDataNodes\": 1,\n \"relocatingShards\": 0,\n \"clusterName\": \"unravel18679\",\n \"activeShards\": 0,\n \"delayedUnassignedShards\": 0,\n \"numberOfSuccessfulNodes\": 1,\n \"status\": \"green\"\n }\n} Historica lData http:\/\/ MONITOR_HOST Response value N RecentData com.unraveldata.monitoring.elastic.history.size " }, 
{ "title" : "Unravel Properties", 
"url" : "adv/unravel-prop.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties", 
"snippet" : "Unravel properties, unless otherwise, are located in \/usr\/local\/unravel\/etc\/unravel.properties You may change properties, but you should be cautious, contact support@unraveldata.com The Set By User : when you must supply a value Req : when it's optional Opt is blank if there is a default value Unit ...", 
"body" : "Unravel properties, unless otherwise, are located in \/usr\/local\/unravel\/etc\/unravel.properties You may change properties, but you should be cautious, contact support@unraveldata.com The Set By User : when you must supply a value Req : when it's optional Opt is blank if there is a default value Unit Abbreviations : comma separated list CSL : milliseconds ms : minutes min : nanoseconds ns : seconds sec : fully qualified directory path, e.g., \/tmp\/dir\/lower path : percentage, e.g., 1.2, .5 percent " }, 
{ "title" : "Basic Properties", 
"url" : "adv/unravel-prop/adv-un-properties-basic-required.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Basic Properties", 
"snippet" : "Name\/Description Set By User Unit Default unravel.jdbc.username Unravel database user string - unravel.jdbc.password password for unravel.jdbc.username string - unravel.jdbc.url MySQL mariadb string (path) jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod jdbc:mariadb:\/\/127.0.0.1:3306\/unravel_mysql_pro...", 
"body" : " Name\/Description Set By User Unit Default unravel.jdbc.username Unravel database user string - unravel.jdbc.password password for unravel.jdbc.username string - unravel.jdbc.url MySQL mariadb string (path) jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod jdbc:mariadb:\/\/127.0.0.1:3306\/unravel_mysql_prod com.unraveldata.hive.hdfs.dir \/user\/unravel\/HOOK_RESULT_DIR Required - com.unraveldata.customer.organization Optional - com.unraveldata.tmpdir string (path) \/srv\/unravel\/tmp com.unraveldata.login.admins Admins who can write in the Unravel UI, e.g., update\/add auto actions. Use a comma separated list to add multiple users string admin com.unraveldata.login.admins.readonly Admins who only have read-only. These have the same access as a read\/write Admin except in read-only mode. comma separated list of users string - com.unraveldata.login.mode mode to use for login ldap: uses ldap entries for login saml: uses saml open: users logs directly into Unravel UI Required - com.unraveldata.zk.quorum comma separated list - com.unraveldata.es.cluster Unravel elastic search cluster name, e.g., unravel21650 - com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. Required string - com.unraveldata.history.maxSize.weeks Number of weeks retained for search results in Elastic Search. integer - com.unraveldata.retention.max.days Number of days to keep the heaviest data (such as error logs and drill-down details) in the SQL Database integer - com.unraveldata.kerberos.principal Spark event logs directory String (path) com.unraveldata.kerberos.keytab.path An HDFS path that helps locate MR job logs to process String (path) " }, 
{ "title" : "General Login", 
"url" : "adv/unravel-prop/adv-un-properties-general-login.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ General Login", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.login.mode mode to use for login ldap: uses ldap entries for login saml: uses saml open: users logs directly into Unravel UI Required string - com.unraveldata.kerberos.principal Spark event logs directory string (path) - com.unraveldata.kerbe...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.login.mode mode to use for login ldap: uses ldap entries for login saml: uses saml open: users logs directly into Unravel UI Required string - com.unraveldata.kerberos.principal Spark event logs directory string (path) - com.unraveldata.kerberos.principal An HDFS path that helps locate MR job logs to process string (path) - " }, 
{ "title" : "General properties for all platforms", 
"url" : "adv/unravel-prop/adv-un-properties-general-all-platforms.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ General properties for all platforms", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.job.collector.done.log.base HDFS path to \"done\" directory of MR logs string HDP CONFIGURE FOR DEFCLOUDERA - HDP com.unraveldata.spark.eventlog.location Spark event logs directory string maprfs:\/\/\/apps\/spark com.unraveldata.job.collector.log.a...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.job.collector.done.log.base HDFS path to \"done\" directory of MR logs string HDP CONFIGURE FOR DEFCLOUDERA - HDP com.unraveldata.spark.eventlog.location Spark event logs directory string maprfs:\/\/\/apps\/spark com.unraveldata.job.collector.log.aggregation.base An HDFS path that helps locate MR job logs to process string \/tmp\/logs\/*\/logs\/ com.unraveldata.max.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a successful application. byte 500000000 (~500 MB) com.unraveldata.max.failed.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that will be imported and processed by the Spark worker for a failed application. byte 2000000000 (~2GB) com.unraveldata.min.job.duration.for.attempt.log Minimum duration of failed\/killed application for which executor logs will be processed (in milliseconds). ms 60000 (10 mins) com.unraveldata.attempt.log.max.containers Maximum number of containers for the application. If application has more that configured number of containers then the aggregated executor log will not be processed for the application ms 6000 (1 min) com.unraveldata.com.unraveldata.spark.master Maximum number of containers for the application. If application has more that configured number of containers then the aggregated executor log will not be processed for the application integer 500 com.unraveldata,spark.master Default master for spark applications. (Used to download executor log using correct APIs)Valid Options: yarn, mesos, standalone yarn " }, 
{ "title" : "Airflow", 
"url" : "adv/unravel-prop/adv-un-properties-airflow.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Airflow", 
"snippet" : "Property\/Description Set By User Unit Default com.unraveldata.airflow.available Notes if airflow is currently available : not available false : available true Required boolean - com.unraveldata.airflow.server.url Full URL of the airflow server, starting with http:\/\/ https:\/\/ Required url - com.unrav...", 
"body" : " Property\/Description Set By User Unit Default com.unraveldata.airflow.available Notes if airflow is currently available : not available false : available true Required boolean - com.unraveldata.airflow.server.url Full URL of the airflow server, starting with http:\/\/ https:\/\/ Required url - com.unraveldata.airflow.protocol Type of connection, e.g., https or http https com.unraveldata.airflow.login.name Airflow login username. Required string - com.unraveldata.airflow.login.password Password for login username. Required string - com.unraveldata.airflow.status.timeout.sec Set Airflow workflow status timeout in Unravel. sec 3600 com.unraveldata.airflow.http.max.body.size.byte Set maximum number of bytes Unravel fetches data from Airflow web UI. Default unlimited. bytes 0 airflow.look.back.num.days Date range for workflows, specified in days to look back. The value must start with a minus, -5 is past 5 days, -5 " }, 
{ "title" : "Auto Actions", 
"url" : "adv/unravel-prop/adv-un-properties-autoactions.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Auto Actions", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.auto.action.publish.internal.metrics.enabled Enables the ability to receive alerts when an application runs over. true: enables alerts false: disables alerts boolean true com.unraveldata.auto.action.default.snooze.period.ms The time repeated ...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.auto.action.publish.internal.metrics.enabled Enables the ability to receive alerts when an application runs over. true: enables alerts false: disables alerts boolean true com.unraveldata.auto.action.default.snooze.period.ms The time repeated violations are be ignored for the violator, i.e., app, user. If the violation is still occurring when awakened the Auto Action executes the action(s) and the violator is once again snoozed.An auto action containing a kill or move action is never snoozed. 0: snooze is turned off &amp;gt; 0: snooze is on, there is no upper bound ms 3,600,000 (1 hour) " }, 
{ "title" : "Azure Data Lake", 
"url" : "adv/unravel-prop/adv-un-properties-azure-lake.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Azure Data Lake", 
"snippet" : "These property are required if you are using Azure Data Lake. Property\/Description Set By User Unit Default com.unraveldata.adl.accountFQDN The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net string - com.unraveldata.adl.clientld An application ID. An application regis...", 
"body" : "These property are required if you are using Azure Data Lake. Property\/Description Set By User Unit Default com.unraveldata.adl.accountFQDN The data lake fully qualified domain name, e.g., mydatalake.azuredatalakestore.net string - com.unraveldata.adl.clientld An application ID. An application registration has to be created in the Azure Active Directory string - com.unraveldata.adl.clientKey An application access key which can be created after registering an application string - com.unraveldata.adl.accessTokenEndpoint The OAUTH 2.0 Access Token Endpoint. It is obtained from the application registration tab on Azure portal string - com.unraveldata.adl.clientRootPath The path in the Data lake store where the target cluster has been given access. URL - " }, 
{ "title" : "Celery Configurations", 
"url" : "adv/unravel-prop/adv-un-properties-celery.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Celery Configurations", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.ngui.proxy.celery http:\/\/localhost:5000 unravel.celery.broker.url If this is not configured, the DB_CONNECTION_STRING is constructed from the jdbc related parameters from Unravel properties. Example: :\/\/unravel:unraveldata@127.0.0.1:3306\/unra...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.ngui.proxy.celery http:\/\/localhost:5000 unravel.celery.broker.url If this is not configured, the DB_CONNECTION_STRING is constructed from the jdbc related parameters from Unravel properties. Example: :\/\/unravel:unraveldata@127.0.0.1:3306\/unravel_mysql_prod sqla+mysql Optional - unravel.celery.result.backend If this is not configured, the DB_CONNECTION_STRING is constructed from the jdbc related parameters from Unravel properties. Example: :\/\/unravel:unraveldata@127.0.0.1:3306\/unravel_mysql_prod db+mysql+pymysql Optional - " }, 
{ "title" : "Cluster Properties", 
"url" : "adv/unravel-prop/adv-un-properties-cluster.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Cluster Properties", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.cluster.name Cluster to connect to if multiple options exist. Required in multi-cluster environments only. Will first attempt to match on the cluster ID, and then fall-back to matching on the display name. For Ambari, the cluster ID and the d...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.cluster.name Cluster to connect to if multiple options exist. Required in multi-cluster environments only. Will first attempt to match on the cluster ID, and then fall-back to matching on the display name. For Ambari, the cluster ID and the display name are equivalent, which is the \"cluster_name\" attribute from the \"\/clusters\" endpoint. E.g., http:\/\/HOST:8080\/api\/v1\/clusters\/ For Cloudera Manager, the cluster id is the \"name\" attribute from the \"\/clusters\" endpoint. E.g., http:\/\/HOST:7180\/api\/v17\/clusters\/ com.unraveldata.cluster.type Possible values are HDP, CDH, or MAPR " }, 
{ "title" : "", 
"url" : "adv/unravel-prop/adv-un-properties-cluster-manager.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ ", 
"snippet" : "Cluster Manager Properties These properties need to be set for Cloud Reports Forecasting Report Impala The following properties are defined by Cluster tool. Be sure to only set the properties for your cluster manager tool. {ManagerName}: ambari or cloudera Name\/Description Set By User Unit Default {...", 
"body" : "Cluster Manager Properties These properties need to be set for Cloud Reports Forecasting Report Impala The following properties are defined by Cluster tool. Be sure to only set the properties for your cluster manager tool. {ManagerName}: ambari or cloudera Name\/Description Set By User Unit Default {ManagerName}.manager.url URL of Cluster Manager, e.g., http:\/\/$clouderaserver:7180, https:\/\/$ambariserver:8083 If the Cloudera Manager URL does not contain a port you must define port below Required string - {ManagerName}.manager.username username for the manager Required string - {ManagerName}.manager.password This is required only for Cloudera when port is missing from the URLpassword Required string - cloudera.manager.port Optional integer - cloudera.manager.api_version Optional and only valid for Cloudera Manager in order to override the API version number to use, such as \"17\" Optional integer - " }, 
{ "title" : "Custom UI Banner\/Notification", 
"url" : "adv/unravel-prop/adv-un-properties-custom-banner.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Custom UI Banner\/Notification", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.custom.banner.display Displays a banner at the top of the Unravel UI. : banner displays text until true end.date : no change to UI false boolean false com.unraveldata.custom.banner.text Text to display when display A banner displays the text ...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.custom.banner.display Displays a banner at the top of the Unravel UI. : banner displays text until true end.date : no change to UI false boolean false com.unraveldata.custom.banner.text Text to display when display A banner displays the text until end.date The text and end.date must both be defined for the banner to be displayed Optional string - com.unraveldata.custom.banner.end.date Date and Time to stop displaying the banner. There is no date\/time limit. Format: YYYYMMDDTHHMMSSZ-000000 The text and end.date must both be defined for the banner to be displayed. Optional string (date) - " }, 
{ "title" : "Email Properties", 
"url" : "adv/unravel-prop/adv-un-properties-email.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Email Properties", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.com.unraveldata. Enables email alerts. true: enables alerts false: disables boolean true com.unraveldata.report.user.email.domain Default email domain used for email alerts. localhost.local Optional string - FIX_UP ADD LOGIN ADMINS ONCE CREAT...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.com.unraveldata. Enables email alerts. true: enables alerts false: disables boolean true com.unraveldata.report.user.email.domain Default email domain used for email alerts. localhost.local Optional string - FIX_UP ADD LOGIN ADMINS ONCE CREATED - mail.smtp.from Used for email \"from\" and \"reply-to\" headers Required string - mail.smtp2.from Used for email \"from\" and \"reply-to\" headers Optional string - mail.smtp.port Port integer 25 mail.smtp.auth Enable\/ SMTP authentication. : If true then mail.smtp.user and mail.smtp.pw must be set as they are used when connecting. Note boolean false mail.smtp.starttls.enable Use start-TLS. boolean false mail.smtp.ssl.enable Use SSL right from the start.string boolean false mail.smtp.user Username for SMTP authentication : If mail.smtp.auth=true you must set this property. Note Optional string - mail.smtp.pw Password for SMTP authentication : If mail.smtp.auth=true you must set this property. Note Optional string - mail.smtp.host Host for SMTP server string localhost mail.smtp.localhost A domain name for apparent sender; must have at least one dot (e.g. organization.com) string localhost.local mail.smtp.debug Enable debug mode. boolean false " }, 
{ "title" : "HBase", 
"url" : "adv/unravel-prop/adv-un-properties-hbase.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ HBase", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.hbase.source.type Source of metrics. Supported values - AMBARI, CDH, and JMX. Required - com.unraveldata.hbase.clusters Cluster names to monitor. If source.type=CDH|AMBARI, this must match cluster name(s) as per rest api Format: clustername1,...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.hbase.source.type Source of metrics. Supported values - AMBARI, CDH, and JMX. Required - com.unraveldata.hbase.clusters Cluster names to monitor. If source.type=CDH|AMBARI, this must match cluster name(s) as per rest api Format: clustername1,clustername2,... Required CSL - com.unraveldata.hbase.metric.poll.interval Polling interval in minutes integer 5 com.unraveldata.hbase.http.read.timeout Polling read timeout in seconds integer 5 com.unraveldata.hbase.http.poll.parallelism Polling parallelism, no. of cores integer 10 com.unraveldata.hbasealert.average.threshold Threshold factor above average value for alerts alerts integer 2 com.unraveldata.hbase. 1.2 (120%) HBase - source.type=JMX Name\/Description Set By User Unit Default com.unraveldata.hbase.{clustername}.node.http.apis HBase node web UI. Format: http[s]:\/\/host:port,http[s]:\/\/host:port,... Example: http:\/\/your.master.server:16010, http:\/\/your.region.server:16030\/ Required CSL - [empty] HBase - source.type=AMBARI | CDH Name\/Description Set By User Unit Default com.unraveldata.hbase.rest.url Ambari or Cloudera Manager base URL. You must specify a port if you are not using the default port (http=80 and https=443) Format: http[s]:\/\/your.ambari.server[:port]\" Example: http:\/\/your.ambari.server http:\/\/your.ambari.server:88 Required integer - com.unraveldata.hbase.rest.user Username for rest api Required string com.unraveldata.hbase.rest.pwd Password for rest api Required string - com.unraveldata.hbase.rest.ssl.enabled hbase.ssl.enabled property value boolean true com.unraveldata.hbase.master.port hbase.master.info.port property value For AMBARI: 16010 For CDH: 60010 Optional integer - com.unraveldata.hbase.regionserver.port hbase.master.info.port property value For AMBARI: 16030 For CDH: 60030 Optional integer com.unraveldata.hbase.service.name HBase service name if not the default - “HBASE” Format: clustername1=servicename1,clustername2=servicename2,... Optional CSL {clustername}=hbase " }, 
{ "title" : "Hive Hook SSL", 
"url" : "adv/unravel-prop/adv-un-properties-hivehook.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Hive Hook SSL", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.live.logreceiver.port.https HTTPS server port(negative value means HTTPS server is disabled) number -1 com.unraveldata.server.ssl.cert_path KeyStore file path, e.g., \/usr\/local\/unravel\/cert.jks Required string - com.unraveldata.server.ssl.cer...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.live.logreceiver.port.https HTTPS server port(negative value means HTTPS server is disabled) number -1 com.unraveldata.server.ssl.cert_path KeyStore file path, e.g., \/usr\/local\/unravel\/cert.jks Required string - com.unraveldata.server.ssl.cert_password KeyStore password Required string - Set these properties in hive-site-xml or in Hive CLI (using --hiveconf Name\/Description Set By User Unit Default hive.hook.insecure.ssl false: SSL certificate is issued and signed by a trusted signing authority or certificate is self-signed and must be added into trust store true: certificate is not validated, trust store not needed> boolean false com.unraveldata.server.ssl.cert_path boolean false com.unraveldata.server.ssl.cert_password Enables SSL string - hive.hook.ssl.trust_store Trust store string - hive.hook.ssl.trust_store_password Path to file of containing Trust store password. If both this and the trust_store_password are set. The password in this file takes precedence string - port number 4043 " }, 
{ "title" : "Hive Metastore Access", 
"url" : "adv/unravel-prop/adv-un-properties-hivemetastore.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Hive Metastore Access", 
"snippet" : "Required for Data Insights tab to populate its information correctly. Property Set By User Unit Default javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store containing the metadata. Examples: postgresql: org.postgresql.Driver mariadb: org.mariadb.jdbc.Driver MySql: org.mys...", 
"body" : "Required for Data Insights tab to populate its information correctly. Property Set By User Unit Default javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store containing the metadata. Examples: postgresql: org.postgresql.Driver mariadb: org.mariadb.jdbc.Driver MySql: org.mysql.jdbc.Driver Req string - javax.jdo.option.ConnectionPassword Password used to access the data store. Req string - javax.jdo.option.ConnectionUserName Username used to access the data store. Req string - javax.jdo.option.ConnectionURL JDBC connection string for the data store containing the metadata of the form: jdbc:{ DB_Driver} {HOST} : {PORT} Examples: } postgresql: jdbc: postgresql:\/\/congo21.unraveldata.com:7432\/hive Req url " }, 
{ "title" : "HiveServer2", 
"url" : "adv/unravel-prop/adv-un-properties-hiveserver2.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ HiveServer2", 
"snippet" : "Property\/Description Set By User Unit Default unravel.hive.server2.host FQDN or IP-Address of the HiveServer2 instance unravel.hive.server2.port Port for the HiveServer2 instance number 10000 unravel.hive.server2.authentication KERBEROS, LDAP, or CUSTOM When set to KERBEROS you must - - unravel.hive...", 
"body" : " Property\/Description Set By User Unit Default unravel.hive.server2.host FQDN or IP-Address of the HiveServer2 instance unravel.hive.server2.port Port for the HiveServer2 instance number 10000 unravel.hive.server2.authentication KERBEROS, LDAP, or CUSTOM When set to KERBEROS you must - - unravel.hive.server2.kerberos.service.name Set only This must hive string - unravel.hive.server2.password Use only when unravel.hive.server2.authentication=LDAP or CUSTOM string - unravel.hive.server2.thrift.transport for \"TTransportBase\" for custom advanced usage - com.unraveldata.kerberos.principal Required when unravel.hive.server2.authentication=KERBEROS onlye.g., user1@xyz.com string - com.unraveldata.kerberos.keytab.path This keytab file will be used to init and renew Kerberos Tickets path - " }, 
{ "title" : "Impala", 
"url" : "adv/unravel-prop/adv-un-properties-impala.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Impala", 
"snippet" : "Property\/Definition Set By User Unit Default com.unraveldatadata.source Can be cm or impalad Opt cm com.unraveldataimpalad.nodes Node list in the form of IP Address:Port Req CSL com.unraveldataimpala.ddl Controls whether DDL statements will be imported boolean false Cloudera Manager Properties cloud...", 
"body" : " Property\/Definition Set By User Unit Default com.unraveldatadata.source Can be cm or impalad Opt cm com.unraveldataimpalad.nodes Node list in the form of IP Address:Port Req CSL com.unraveldataimpala.ddl Controls whether DDL statements will be imported boolean false Cloudera Manager Properties cloudera.manager.impalanum.queries.limit Maximum number of queries that will be returned by a poll to the Cloudera Manager API integer 1000 cloudera.manager.impalapoll.interval.millis Interval between consecutive polls to the Cloudera Manager API measured in milliseconds. ms 60000 cloudera.manager.impalalook.back.minutes Number of minutes to look back when polling the Cloudera Manager API min -5 cloudera.manager.impalaskip.duration.millis Queries with duration shorter than this threshold will get captured but not analyzed ms 1000 com.unraveldata.cloudera.manager.read.timeout.millis HTTP Read timeout for Cloudera Manager connections ms 5000 com.unraveldata.cloudera.manager.connect.timeout.millis HTTP Connect timeout for Cloudera Manager connections ms 30000 The following properties defaults should be fine and shouldn't need to be changed. hitdoc.impala.operator.info.length 20480 impala.events.stalestats.threshold.bytes bytes 1000 impala.events.stalestats.ratio percent 0.2 impala.events.longop.time.millis ms 2000 impala.events.longop.ratio percent 0.2 impala.events.cost.diff.bytes bytes 500000000 impala.events.skew.time.millis ms 500000000 impala.events.skew percent 1.5 " }, 
{ "title" : "JDBC", 
"url" : "adv/unravel-prop/adv-un-properties-jdbc.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ JDBC", 
"snippet" : "You may optionally configure the following properties to manage the Hive Metastore JDBC connection pooling. Unravel uses the c3p0 library to manage the pooling. Property Set By User Unit Default com.unraveldata.metastore.db.c3p0.acquireRetryAttempts Controls how many times c3p0 tries to obtain an co...", 
"body" : "You may optionally configure the following properties to manage the Hive Metastore JDBC connection pooling. Unravel uses the c3p0 library to manage the pooling. Property Set By User Unit Default com.unraveldata.metastore.db.c3p0.acquireRetryAttempts Controls how many times c3p0 tries to obtain an connection from the database before giving up. count 30 com.unraveldata.metastore.db.c3p0.acquireRetryDelay Controls how much waiting time is between each retry attempts in milliseconds. ms 1000 com.unraveldata.metastore.db.c3p0.breakafteracquirefailure Allows you to mark data source as broken and permanently be closed if a connection cannot be obtained from database. The default value is false bool false com.unraveldata.metastore.db.c3p0.maxconnectionage The maximum number of seconds any connections were forced to be released from the pool. If the default value (0) is usedthe connections will never be released. sec 0 com.unraveldata.metastore.db.c3p0.maxidletimeexcessconnections The number of seconds that connections are permitted to remain idle in the pool before being released. If the default value (0) is usedthe connections will never be released. sec 0 com.unraveldata.metastore.db.c3p0.maxpoolsize The maximum connections in the connection pool. count 5 com.unraveldata.metastore.db.c3p0.idleconnectiontestperiod Opt 0 com.unraveldata.metastore.databasePattern Opt string dname * com.unraveldata.metastore.print.metastore.stats Opt bool false " }, 
{ "title" : "HDInsight", 
"url" : "adv/unravel-prop/adv-un-properties-hdinsight.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ HDInsight", 
"snippet" : "Property\/Description Set By User Unit Default com.unraveldata.hdinsight.storage-account-name-1 Optional for Spark when HDInsight uses a blob storage storage account name for the HDInsight cluster string - com.unraveldata.hdinsight.primary-access-key Primary storage account key string - com.unravelda...", 
"body" : " Property\/Description Set By User Unit Default com.unraveldata.hdinsight.storage-account-name-1 Optional for Spark when HDInsight uses a blob storage storage account name for the HDInsight cluster string - com.unraveldata.hdinsight.primary-access-key Primary storage account key string - com.unraveldata.hdinsight.storage-account-name-2 Optional for Spark when HDInsight using blob storage Storage account name for the HDInsight cluster (same as account-name-1 string - com.unraveldata.hdinsight.secondary-access-key Secondary storage account key string - " }, 
{ "title" : "Kafka", 
"url" : "adv/unravel-prop/adv-un-properties-kafka.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Kafka", 
"snippet" : "Property\/Definition Set By User Unit Default com.unraveldata.ext.kafka.clusters Cluster list. These user-defined names are used to clearly identify the Kafka cluster(s) in the Unravel UI. Use a comma a separated list for multiple clusters. Required CSL - com.unraveldata.ext.kafka.{cluster}.bootstrap...", 
"body" : " Property\/Definition Set By User Unit Default com.unraveldata.ext.kafka.clusters Cluster list. These user-defined names are used to clearly identify the Kafka cluster(s) in the Unravel UI. Use a comma a separated list for multiple clusters. Required CSL - com.unraveldata.ext.kafka.{cluster}.bootstrap_servers List of brokers that is used to retrieve initial information about the kafka cluster. For each cluster in com.unraveldata.ext.kafka.clusters you must defined the associated brokers. Use a comma a separated list for multiple brokers. e.g., com.unraveldata.ext.kafka.East.bootstrap_servers=localhost:9092,localhost:9093 Required CSL - com.unraveldata.ext.kafka.{cluster}.jmx_servers Aliases for each kafka nodes in the clusters with JMX ports exposed. You must assign aliases the cluster nodes. Use a comma a separated list for multiple nodes. e.g., unraveldata.ext.kafka.East.jmx_servers=kNode1, kNode2 Required CSL - com.unraveldata.ext.kafka.{cluster}.jmx.{kafkaNode-1}.host The host for a node in the cluster. You must define a host for each node in each cluster. e.g., com.unraveldata.ext.kafka.East.kNode1=localhost com.unraveldata.ext.kafka.East.kNode2=localhost Required - To locate Kafka and JMX ports: . Navigate to: Clusters → Kafka → Configuration → Ports and Addresses. Cloudera Manager Alternatively, you may lookup up that information in the broker nodes of Zookeeper CLI. : For Protocol and broker port navigate to: Kafka → Configs → Kafka Broker HDP JMX port navigate to: Kafka → Configs → Advanced kafka-env → kafka-env template com.unraveldata.ext.kafka.{cluster}.jmx.{kafkaNode-1}.port For each node in each cluster you mush assign a port. e.g., com.unraveldata.ext.kafka.East.jmx.kNode1.port=5005 Required number - com.unraveldata.ext.kafka.servers Use a omma separated list for multiple servers. Required CSL - com.unraveldata.ext.kafka.insight.interval_min min 15 com.unraveldata.ext.kafka.insight.sw_size integer 30 com.unraveldata.ext.kafka.insight.num_ignored_intervals integer 2 com.unraveldata.ext.kafka.insight.lag_threshold integer 100 com.unraveldata.monitoring.kafka.check.interval sec 30 com.unraveldata.monitoring.kafka.ignore.topics Topics to ignore, Use a comma separated list for multiple topics. Optional CSL - com.unraveldata.monitoring.kafka.history.size integer 5 " }, 
{ "title" : "LDAP Properties", 
"url" : "adv/unravel-prop/adv-un-properties-ldap.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ LDAP Properties", 
"snippet" : "These properties are required when com.unraveldata.login.mode Name\/Description Set By User Unit Default com.unraveldata.ldap.domain com.unraveldata.ldap.customLDAPQuery Optional - . com.unraveldata.ldap.groupClassKey Optional - . com.unraveldata.ldap.groupDNPattern Optional - . com.unraveldata.ldap....", 
"body" : "These properties are required when com.unraveldata.login.mode Name\/Description Set By User Unit Default com.unraveldata.ldap.domain com.unraveldata.ldap.customLDAPQuery Optional - . com.unraveldata.ldap.groupClassKey Optional - . com.unraveldata.ldap.groupDNPattern Optional - . com.unraveldata.ldap.groupFilter Optional - . com.unraveldata.ldap.groupMembershipKey Optional - . com.unraveldata.ldap.guidKey=uid Optional - . com.unraveldata.ldap.mailAttribute The mail attribute name in the LDAP response that Unravel server uses to extract the ldap user's email address. If not configured, Unravel server uses the attribute name \"mail\". Optional - . com.unraveldata.ldap.userDNPattern Optional - . com.unraveldata.ldap.userFilter Optional - . com.unraveldata.ldap.url Optional - " }, 
{ "title" : "Oozie", 
"url" : "adv/unravel-prop/adv-un-properties-oozie.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Oozie", 
"snippet" : "Property\/Description Set By User Unit Default oozie.server.url The Oozie server URL to be monitored by Unravel Required path oozie.server.username Optional string oozie.server.password Optional string oozie.log.length The maximum number of characters in Oozie workflow log that Unravel fetches. Any l...", 
"body" : " Property\/Description Set By User Unit Default oozie.server.url The Oozie server URL to be monitored by Unravel Required path oozie.server.username Optional string oozie.server.password Optional string oozie.log.length The maximum number of characters in Oozie workflow log that Unravel fetches. Any log longer than this number x will be trimmed from the beginning and only last x characters are kept. count 1000000 com.unraveldata.oozie.disable Whether to disable bringing in Oozie workflows into Unravel. The underlying jobs will not be affected. bool false com.unraveldata.oozie.fetch.num Number of workflows to pull in each API call. count 100 com.unraveldata.oozie.fetch.interval.sec sec 120 com.unraveldata.oozie.retry.sec sec 600 " }, 
{ "title" : "Role Based Access Control (RBAC)", 
"url" : "adv/unravel-prop/adv-un-properties-rbac.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Role Based Access Control (RBAC)", 
"snippet" : "For additional non-RBAC specific properties that affect RBAC see RBAC Configuration Property\/Description Set By User Unit Default com.unraveldata.rbac.enabled Enables Role Based Access Control true: RBAC turned on false: RBAC turned off bool true com.unraveldata.rbac.default Determines how the End-u...", 
"body" : "For additional non-RBAC specific properties that affect RBAC see RBAC Configuration Property\/Description Set By User Unit Default com.unraveldata.rbac.enabled Enables Role Based Access Control true: RBAC turned on false: RBAC turned off bool true com.unraveldata.rbac.default Determines how the End-user's views are filtered when no specific tags are set for a end-user. string userName com.unraveldata.rbac.tagcmd string is the value of {Mode} com.unraveldata.login.mode com.unraveldata.login.admins.{ Mode Grants read\/write admin access to an AD user who belongs to a specified group(s). Value: a common separated list of groups. CLS - com.unraveldata.login.admins.readonly.{ Mode Grants read-only admin access to an AD user who belongs to a specified group(s). Value: a common separated list of groups. CLS - com.unraveldata.rbac.{ Mode A comma separated list of the prefix of LDAP\/saml group to be used as the PROJECT { Mode - CLS - com.unraveldata.rbac.{ Mode Tag} Defines regular expression used to parse LDAP\/saml groups for generating the TENANTs PROJECT. Value = { Tag} -(REGEX) PROJECT com.unraveldata.rbac.{Mode}.tags. If you have defined more than one group in your LDAP\/saml group definition only the first prefix will be processed with the remaining ignored. The best practice is to define each Note: PROJECT - CLS - " }, 
{ "title" : "SAML Configuration Properties", 
"url" : "adv/unravel-prop/adv-un-properties-saml.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ SAML Configuration Properties", 
"snippet" : "These properties are required when com.unraveldata.login.mode Name\/Description Set By User Unit Default com.unraveldata.login.saml.config Fully qualified path to saml configuration file Optional string path) The following properties are set in the login.saml.config file specified above. entryPoint I...", 
"body" : " These properties are required when com.unraveldata.login.mode Name\/Description Set By User Unit Default com.unraveldata.login.saml.config Fully qualified path to saml configuration file Optional string path) The following properties are set in the login.saml.config file specified above. entryPoint Identity provider entry point, It must be specified in order to be spec-compliant when the request is signed. e.g., \"http:\/\/c24.unravel.com:9080\/simplesaml\/saml2\/idp\/SSOService.php\" Optional - issuer Issuer string to supply to identity provider (Environment name). Should match the name configured in ldp e.g., “Congo24”, “Localhost” , Optional - cert IDP's public signing certificate. e.g., Idp Cert String Optional - cert IDP's public signing certificate. e.g., Idp Cert String Optional - unravel_mapping Mapping SAML attributes to Unravel attributes. Specific to unravel Integration. e.g., \n{\n \"username\":\"userid\",\n \"groups\":\"ds_groups\"\n}\n - " }, 
{ "title" : "Small Files and File Reports", 
"url" : "adv/unravel-prop/adv-un-properties-smallnfiles-reports.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Small Files and File Reports", 
"snippet" : "You must restart the unravel_ondemand unravel_ngui Name\/Description Set By User Unit Default com.unraveldata.ngui.sfhivetable.schedule.enabled Controls whether to schedule periodic fsimage fetch and process. : Small Files feature is enabled true : Small Files disabled. false boolean true com.unravel...", 
"body" : " You must restart the unravel_ondemand unravel_ngui Name\/Description Set By User Unit Default com.unraveldata.ngui.sfhivetable.schedule.enabled Controls whether to schedule periodic fsimage fetch and process. : Small Files feature is enabled true : Small Files disabled. false boolean true com.unraveldata.ngui.sfhivetable.schedule.enabled Controls the frequency with which Unravel fetches fsimage from the cluster. For 4.5.0.0, it is recommended to use the default setting. Using this setting, Unravel triggers fetch fsimage at 00:00 UTC every day. day 1 You must restart the unravel_ondemand Name\/Description Set By User Unit Default unravel.python.reporting.files.disable Enables or disables Unravel ability to generate Small Files and File Reports. : disables the functionality in the Backend and UI. true : enables the functionality in the Backend and UI to generate the Small Files\/File Reports. false boolean false unravel.python.reporting.files.skip_fetch_fsimage If hdfs admin privileges can not be granted, then setting this to true allows an externally fetched fsimage for use by unravel Ondemand process. : Ondemand etl_fsimage process does not fetch fsimage from name node. Instead, the fsimage is expected to be available in directory specified by unravel.python.reporting.files.external_fsimage_dir true boolean false unravel.python.reporting.files.external_fsimage_dir Directory for fsimage when skip_fetch_fsimage=true. The fsimage externally fetched is expected to be in this directory. Unravel uses the latest file in this directory which starts with \" fsimage_\". This directory must be different than the Unravel's internal directory, i.e., \/srv\/unravel\/tmp\/reports\/fsimage. string - unravel.python.reporting.files.hive_database Hive Database where Ondemand creates 5 hive tables (4 temporary, 1 permanent) for Small Files\/File Reports. When not set, tables are created in the default Hive Database. In addition, the hive queries used for this feature run against default MR queue. It must point to a valid Hive database. string default database unravel.python.reporting.files.hive_mr_queue The hive queries ran by Ondemand process run against this MR queue. It must point to a valid MR queue. string default Small Files and File Reports have equivalent \"local\" properties which take precedence if they are set. Should you unset\/delete any of the below properties or their equivalent properties Unravel has hard-coded values to ensure your reports are generated. Name\/Description Set By User Unit Default unravel.python.reporting.files.files_use_avg_file_size_flag : average of all the files is used against the threshold criteria and either all the files are  accepted\/counted or rejected\/not counted as per the criteria. true :  absolute file size is used against the threshold criteria and a file is accepted\/counted or rejected\/not counter as per the criteria. false boolean - unravel.python.reporting.files.min_parent_dir_depth Directory depth to end search at. For instance, if depth=2, search begins below 2 levels, i.e. starts with directory two count - unravel.python.reporting.files.max_parent_dir_depth Directory depth to end search at. Maximum is 50 For instance, if depth=5 given HDFS_root\/one\/two\/three\/four\/five\/six\/seven the search ends at five count - unravel.python.reporting.drill_down_subdirs_flag When set a file is accounted (listed) for all it's ancestors.   : a file is accounted in only it's immediate parent. This allows Unravel to find a specific directory with maximum number of files matching the size criteria. false : lists each file with it's ancestors true For example given the directory structure is \/one\/two : false \/ - lists files in \/ \/one - lists files in one \/one\/two - lists files in \/one\/two : true \/ - lists files in \/, \/one, and \/one\/two. \/one - lists files in \/one, and \/one\/two \/one\/two - lists files in \/one\/two boolean - " }, 
{ "title" : "Small Files", 
"url" : "adv/unravel-prop/adv-un-properties-small-files.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Small Files", 
"snippet" : "You must restart the unravel_ondemand Property\/Description Set By User Unit Default unravel.python.reporting.small_files_use_avg_file_size_flag : average of all the files is used against the threshold criteria and either all the files are  accepted\/counted or rejected\/not counted as per the criteria...", 
"body" : " You must restart the unravel_ondemand Property\/Description Set By User Unit Default unravel.python.reporting.small_files_use_avg_file_size_flag : average of all the files is used against the threshold criteria and either all the files are  accepted\/counted or rejected\/not counted as per the criteria. true :  absolute file size is used against the threshold criteria and a file is accepted\/counted or rejected\/not counter as per the criteria. false true unravel.python.reporting.files.small_files_min_parent_dir_depth Directory depth to end search at. For instance, if depth=2, search begins below 2 levels, i.e. starts with directory two 0 unravel.python.reporting.files.small_files_max_parent_dir_depth Directory depth to end search at. Maximum is 50 For instance, if depth=5 given HDFS_root\/one\/two\/three\/four\/five\/six\/seven the search ends at five 10 unravel.python.reporting.files.small_files_drill_down_subdirs_flag When set a file is accounted (listed) for all it's ancestors.   : a file is accounted in only it's immediate parent. This allows Unravel to find a specific directory with maximum number of files matching the size criteria. false : lists each file with it's ancestors true For example given the directory structure is \/one\/two : false \/ - lists files in \/ \/one - lists files in one \/one\/two - lists files in \/one\/two : true \/ - lists files in \/, \/one, and \/one\/two. \/one - lists files in \/one, and \/one\/two \/one\/two - lists files in \/one\/two boolean - " }, 
{ "title" : "File Reports", 
"url" : "adv/unravel-prop/adv-un-properties-file-reports.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ File Reports", 
"snippet" : "Name\/Description Set By User Unit Default unravel.python.reporting.files.huge_files_threshold_size file size &gt;= threshold_size bytes 100GB unravel.python.reporting.files.huge_files_min_files number of files in directory &gt;= min_files integer 1 unravel.python.reporting.files.huge_files_top_n_dir...", 
"body" : " Name\/Description Set By User Unit Default unravel.python.reporting.files.huge_files_threshold_size file size &gt;= threshold_size bytes 100GB unravel.python.reporting.files.huge_files_min_files number of files in directory &gt;= min_files integer 1 unravel.python.reporting.files.huge_files_top_n_dirs number of files in directory &gt;= min_files integer 10 unravel.python.reporting.files.medium_files_max_threshold_size file size &lt;= max_threshold_size bytes 10GB unravel.python.reporting.files.medium_files_min_threshold_size file size &gt;= min_threshold_size integer 5GB unravel.python.reporting.files.medium_files_min_files number of files in directory &gt;= min_files integer 5 unravel.python.reporting.files.medium_files_top_n_dirs maximum number of directories to display integer 20 unravel.python.reporting.files.tiny_files_threshold_size file size &lt;= threshold_size bytes 100KB unravel.python.reporting.files.tiny_files_min_files maximum number of directories to display integer 10 unravel.python.reporting.files.tiny_files_top_n_dirs maximum number of directories to display integer 30 unravel.python.reporting.files.empty_files_min_files number of files in directory &gt;= min_files integer 10 unravel.python.reporting.files.empty_files_top_n_dirs maximum number of directories to display integer 3 The following four properties are defined per file size type; Size Name\/Description Set By User Unit Default {Size}_files_use_avg_file_size_flag : average of all the files is used against the threshold criteria and either all the files are  accepted\/counted or rejected\/not counted as per the criteria. true :  absolute file size is used against the threshold criteria and a file is accepted\/counted or rejected\/not counter as per the criteria. false boolean false {Size}_files_min_parent_dir_depth Directory depth to end search at. For instance, if depth=2, search begins below 2 levels, i.e. starts with directory two integer 0 {Size}_files_max_parent_dir_depth Directory depth to end search at. Maximum is 50 For instance, if depth=5 given HDFS_root\/one\/two\/three\/four\/five\/six\/seven the search ends at five integer 10 {Size}_files_drill_down_subdirs_flag When set a file is accounted (listed) for all it's ancestors.   : a file is accounted in only it's immediate parent. This allows Unravel to find a specific directory with maximum number of files matching the size criteria. false : lists each file with it's ancestors true For example given the directory structure is \/one\/two : false \/ - lists files in \/ \/one - lists files in one \/one\/two - lists files in \/one\/two : true \/ - lists files in \/, \/one, and \/one\/two. \/one - lists files in \/one, and \/one\/two \/one\/two - lists files in \/one\/two booleans false " }, 
{ "title" : "Queue Analysis", 
"url" : "adv/unravel-prop/adv-un-properties-q-analysis.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Queue Analysis", 
"snippet" : "Property\/Description Set By User Unit Default com.unraveldata.report.queue.metrics.sensor.enabled Enables or disables queue metric sensor. boolean true com.unraveldata.report.queue.poll.interval.msec How often queue metric sensor polls polls YARN Resource Manager. ms 60000 com.unraveldata.report.que...", 
"body" : " Property\/Description Set By User Unit Default com.unraveldata.report.queue.metrics.sensor.enabled Enables or disables queue metric sensor. boolean true com.unraveldata.report.queue.poll.interval.msec How often queue metric sensor polls polls YARN Resource Manager. ms 60000 com.unraveldata.report.queue.http.timeout.msec YARN Resource Manager HTTP connection timeout. ms 10000 com.unraveldata.report.queue.http.retries YARN Resource Manager HTTP connection retries. count 3 com.unraveldata.report.queue.http.retry.period.msec YARN Resource Manager HTTP connection retry wait period. ms 0 unravel.python.queueanalysis.metrics.scale UI rendered graph metrics scale factor. number 1000 unravel.python.queueanalysis.daterange.span UI report date picker range. days 30 " }, 
{ "title" : "Sessions", 
"url" : "adv/unravel-prop/adv-un-properties-session.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Sessions", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.session.enabled Enables On-demand Sessions features tab in the UI true: Sessions enabled false: Sessions disabled boolean true com.unraveldata.session.max.autotune.runs Maximum number of runs allowed in an auto-tune session. count 8...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.session.enabled Enables On-demand Sessions features tab in the UI true: Sessions enabled false: Sessions disabled boolean true com.unraveldata.session.max.autotune.runs Maximum number of runs allowed in an auto-tune session. count 8 " }, 
{ "title" : "Spark", 
"url" : "adv/unravel-prop/adv-un-properties-spark.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Spark", 
"snippet" : "Property\/Description Set By User Unit Default com.unraveldata.spark.live.pipeline.enabled Specifies when to process stages and jobs. Job\/stage data is sent by the Unravel sensor. : process as soon as the job\/stage completes execution. Allows user to see the progress\/completion percentage of jobs in ...", 
"body" : " Property\/Description Set By User Unit Default com.unraveldata.spark.live.pipeline.enabled Specifies when to process stages and jobs. Job\/stage data is sent by the Unravel sensor. : process as soon as the job\/stage completes execution. Allows user to see the progress\/completion percentage of jobs in the Spark APM. true : process after the application completes and the event log file has been processed false boolean true com.unraveldata.spark.live.pipeline.maxStoredStages Maximum number of jobs\/stages stored in the DB. If an application has jobs\/stages > maxStoredStages maxStoredStages This setting affects only the live pipeline integer 1000 com.unraveldata.spark.master Default spark master mode to be used if not available from Sensor. yarn Event log processing com.unraveldata.spark.eventlog.location All the possible locations of the event log files. Multiple locations are supported as a comma separated list of values. This property is used only when string hdfs:\/\/\/user\/spark\/applicationHistory\/ com.unraveldata.spark.eventlog.maxSize Maximum sizeof the event log file that will be processed by the Spark worker daemon. Event logs larger than MaxSize bytes 1000000000 (~1GB) com.unraveldata.spark.eventlog.appDuration.mins Maximum duration (in minutes) of application to pull Spark event log. sec 1440 (1 day) com.unraveldata.spark.hadoopFsMulti.useFilteredFiles Specifies how to search the event log files. : prefix search true : prefix + suffix search false Prefix + suffix search is faster as it avoidslistFiles()API which may take a long time for large directories on HDFS. This search requires that all the possible suffixes com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes boolean false com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes Specifies suffixes used for prefix+suffix search of the event logs when com.unraveldata. spark.hadoopFsMulti.useFilteredFiles : the empty suffix (,,) be part of this value for uncompressed event log files. NOTE CSL ,,.lz4,.snappy,.inprogress Maximum number of attempts for loading the event log file from HDFS\/S3\/ ADL\/WASB etc. 3 com.unraveldata.spark.appLoading.delayForRetry Delay used among consecutive retries when loading the event log files. The actual delay is not constant, it increases progressively by 2^attempt delayForRetry ms 2000 (2 s) com.unraveldata.spark.tasks.inMemoryLimit Number of tasks to be kept in memory and DB per stage. All stats are calculated for all the task attempts but only the configured number of tasks will be kept in memory\/DB. count 1000 Events Related com.unraveldata.spark.events.enableCaching Enables logic for executing caching events. boolean false " }, 
{ "title" : "Tagging", 
"url" : "adv/unravel-prop/adv-un-properties-tagging.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Tagging", 
"snippet" : "Property\/Description Set By User Unit Default com.unraveldata.tagging.enabled Enables tagging functionality. boolean true com.unraveldata.tagging.script.enabled Enables tagging. boolean false com.unraveldata.tagging.script.path Specifies tagging script path to use when enabled=true string (path) \/us...", 
"body" : " Property\/Description Set By User Unit Default com.unraveldata.tagging.enabled Enables tagging functionality. boolean true com.unraveldata.tagging.script.enabled Enables tagging. boolean false com.unraveldata.tagging.script.path Specifies tagging script path to use when enabled=true string (path) \/usr\/local\/unravel\/etc\/apptag.py com.unraveldata.tagging.script.method.name The name of the method in the python script which generates the tagging dictionary. string generate_unravel_tags " }, 
{ "title" : "Tez", 
"url" : "adv/unravel-prop/adv-un-properties-tez.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Tez", 
"snippet" : "Property\/Descripton Set By User Unit Default yarn.ats.webapp.username Username used for Application Timeline Server if authentication is required Optional string yarn.ats.webapp.password Password used for Application Timeline Server if authentication is required Optional string com.unraveldata.yarn....", 
"body" : " Property\/Descripton Set By User Unit Default yarn.ats.webapp.username Username used for Application Timeline Server if authentication is required Optional string yarn.ats.webapp.password Password used for Application Timeline Server if authentication is required Optional string com.unraveldata.yarn.timeline-service.webapp.address Hostname of Application Timeline Server, e.g., http:\/\/$atshostname Required string (url) com.unraveldata.yarn.timeline-service.port HTTP port of Application Timeline Server integer 8188 com.unraveldata.tez.app.ats.connect.timeout.millis HTTP Connect timeout for ATS connections ms 30000 com.unraveldata.tez.app.ats.read.timeout.millis HTTP Read timeout for ATS connections ms 5000 com.unraveldata.tez.app.ats.poll.timeout.millis Controls the timeout after which we will stop trying to poll ATS if the polling is failing ms 120000 com.unraveldata.tez.ats.poll.interval.millis Interval between consecutive polls of ATS if the polling fails. ms 10000 com.unraveldata.tez.ats.poll.max.retries Maximum number of retries if the polling of ATS fails integer 30 The following properties defaults shouldn't need to be changed. com.unraveldata.tez.events.low.tasks integer 25 com.unraveldata.tez.events.low.tasks integer 50 com.unraveldata.tez.events.min.task.millis ms 2000 com.unraveldata.tez.events.max.task.millis ms 50000 com.unraveldata.tez.events.task.percentage percent 0.2 " }, 
{ "title" : "Top X", 
"url" : "adv/unravel-prop/adv-un-properties-topx.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Top X", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.ngui.topx.enabled : Top X reports are enabled. true : Top X reports are disabled. false boolean true...", 
"body" : " Name\/Description Set By User Unit Default com.unraveldata.ngui.topx.enabled : Top X reports are enabled. true : Top X reports are disabled. false boolean true " }, 
{ "title" : "Sensors", 
"url" : "adv/unravel-prop/adv-sensors.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Sensors", 
"snippet" : "Table of Contents All Sensors Resource Usage Sensor Spark Sensor Most Sensor properties are set via JVM arguments, when they can be set in the file the file name is noted. All Sensors Property Description Set By User Unit Default Specified by adding -D<propertyName>=<propertyValue> to the list of JV...", 
"body" : " Table of Contents All Sensors Resource Usage Sensor Spark Sensor Most Sensor properties are set via JVM arguments, when they can be set in the file the file name is noted. All Sensors Property Description Set By User Unit Default Specified by adding -D<propertyName>=<propertyValue> to the list of JVM args, e.g.-Dclient.rest.conntimeout.ms=0 prepend client.rest.queue The queue length for outgoing REST HTTP requests 20000 client.rest.retryfail Cool-down period after unsuccessful attempt to make REST HTTP request in nanoseconds ns 30 seconds client.rest.conn.timeout.ms REST HTTP request timeout in milliseconds ms 100 client.rest.shutdown.ms Maximum time to wait for orderly shutdown of the REST client (if exceeded some messages still in the queue will be lost) ms 10 client.rest.dns.ttl The period to refresh the DNS info in milliseconds - IP is pre-resolved and kept till the next refresh if no failures are observed ms 6 hours client.rest.priority.retries Certain critical messages have priority flag and their transmission will be reattempted this many times 5 no prepend unravel.server.hostport Unravel server host:port information - Resource Usage Sensor Property Description Set By User Unit Default Specified by adding -D<propertyName>=<propertyValue> to the list of JVM args, e.g.-Dclient.rest.conntimeout.ms=0 prepend agent.metrics.enabled_keys Comma separated list of metric type names which are enabled for collection CLS availableMemory,cpuUtilization, processCpuLoad,systemCpuLoad, maxHeap, usedHeap, vmRss,gcLoad no prepend unravel.metrics.factor Sampling period scale down factor 1 Agent Argument Definition Set By User Unit Default Agent arguments are added directly to the agent definition - anything after -javaagent:btrace-agent.jar= . no prepend metricsCaptureFilter Format allow specifying single ordinals for component IDs as well as ranges and enumerations - e.g.,metricsCaptureFilter=1,2,5-10,turns on metrics collection for components 1, 2, 5 to 10 0-1500 Spark Sensor Property Definition Set By User Unit Default Specified by adding -D<propertyName>=<propertyValue> to the list of JVM args, e.g.-Dclient.rest.conntimeout.ms=0 prepend enableLiveUpdates Enable live updates for Spark apps boolean False enableCachingInfo Enable tracking caching info for Spark apps boolean False enableSampling Enable data sampling between operators for Spark apps boolean False Agent Argument Definition Set By User Unit Default Agent arguments are added directly to the agent definition - anything after -javaagent:btrace-agent.jar= . prepend clusterID The cluster ID - currently only used in Spark " }, 
{ "title" : "Configurations for OnDemand Reports", 
"url" : "adv/unravel-prop/configurations-for-ondemand-reports.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Properties \/ Configurations for OnDemand Reports", 
"snippet" : "Table of Contents General Celery Configurations HiveServer2 Sessions Small Files and File Reports Small Files File Reports Queue Analysis There are no Top X specific properties. Set By User Column notes whether you are required to set the property (Req) or optionally can set the property (Opt) Gener...", 
"body" : " Table of Contents General Celery Configurations HiveServer2 Sessions Small Files and File Reports Small Files File Reports Queue Analysis There are no Top X specific properties. Set By User Column notes whether you are required to set the property (Req) or optionally can set the property (Opt) General Property\/Description Description Set By User Unit Default unravel.server.ip FQDN or IP-Address of Unravel Server Req - com.unraveldata.python.enabled Enable\/disable all ondemand reports and Sessions features in UI. (This property is configured during the ondemand installation.) boolean true Celery Configurations Property Description Set By User Unit Default Celery Configurations com.unraveldata.ngui.proxy.celery http:\/\/localhost:5000 unravel.celery.broker.url If this is not configured, the DB_CONNECTION_STRING is constructed from the jdbc related parameters from Unravel properties. Example: sqla+mysql - unravel.celery.result.backend If this is not configured, the DB_CONNECTION_STRING is constructed from the jdbc related parameters from Unravel properties. Example: db+mysql+pymysql - HiveServer2 Property Description Set By User Unit Default prepend: host FQDN or IP-Address of the HiveServer2 instance port Port for the HiveServer2 instance number 10000 authentication \"KERBEROS\" or \"LDAP\" or \"CUSTOM\" When set to KERBEROS you must kerberos.service.name=hive - - kerberos.service.name This must hive unravel.hive.server2.authentication KERBEROS string - password Use only when unravel.hive.server2.authentication=LDAP CUSTOM string - thrift.transport or \"TTransportBase\" for custom advanced usage - prepend: kerberos.principal Required when unravel.hive.server2.authentication KERBEROS e.g., user1@xyz.com string - kerberos.keytab.path Required only when unravel.hive.server2.authentication=KERBEROS This keytab file will be used to init and renew Kerberos Tickets path - Sessions Property Description Set By User Unit Default prepend: session.enabled Enables On-demand Sessions features tab in the UI true: Sessions enabled false: Sessions disabled bool true session.max.autotune.runs Maximum number of runs allowed in an auto-tune session. count 8 Small Files and File Reports Property Description Set By User Unit Default You must restart the unravel_ondemand and unravel_ngui daemons for any changes in the two properties given below to take effect NOTE: : com.unraveldata.ngui.sfhivetable.schedule. prepend enabled Controls whether to schedule periodic fsimage fetch and process. : Small Files feature is enabled true : Small Files disabled. false boolean true interval Controls the frequency with which Unravel fetches fsimage from the cluster. For 4.5.0.0, it is recommended to use the default setting. With this setting, Unravel triggers fetch fsimage at 00:00 UTC day 1d You must restart the unravel_ondemand daemon for any changes to take effect NOTE: : unravel.python.reporting.files prepend disable Enables or disables Unravel ability to generate Small Files and File Reports. : disables the functionality in the Backend and UI. true : enables the functionality in the Backend and UI to generate the Small Files\/File Reports. false boolean false skip_fetch_fsimage If hdfs admin privileges can not be granted, then setting this to true allows an externally fetched fsimage for use by unravel Ondemand process. Ondemand etl_fsimage process does not fetch fsimage from name node. Instead, the fsimage is expected to be available in directory specified by unravel.python.reporting.files.external_fsimage_dir true: boolean false external_fsimage_dir Directory for fsimage when skip_fetch_fsimag e This directory Note: must be different string - hive_database Hive Database where Ondemand creates 5 hive tables (4 temporary, 1 permanent) for Small Files\/File Reports. When not set, tables are created in the default Hive Database. In addition, the hive queries used for this feature run against default MR queue. It must string default Hive database hive_mr_queue The hive queries ran by Ondemand process run against this MR queue. It must string default The following properties apply to Small Files and File Reports; each has equivalent \"local\" properties. If the equivalent property is not files_use_avg_file_size_flag : average of all the files is used against the threshold criteria and either all the files are accepted\/counted or rejected\/not counted as per the criteria. true : absolute file size is used against the threshold criteria and a file is accepted\/counted or rejected\/not counter as per the criteria. false boolean - min_parent_dir_depth Directory depth to start search at. If depth=2, search begins below 2 levels i.e. starts with directory such as HDFS_root\/one\/two count - max_parent_dir_depth Directory depth to end search at. Maximum is 50. : Example Depth=5: search ends at HDFS_root\/one\/two\/three\/four\/five Depth=6: search ends at HDFS_root\/one\/two\/three\/four\/five\/six count - drill_down_subdirs_flag When set a file is accounted (listed) for all it's ancestors. : a file is accounted in only it's immediate parent. This allows Unravel to find a specific directory with maximum number of files matching the size criteria. false t rue : given directory list is \/one\/two Example \/ - lists files in \/ false: \/one - lists files in one \/one\/two - lists files in \/one\/two \/ - lists files in \/, \/one and \/one\/two. true: \/one - lists files in \/one and \/one\/two \/one\/two - lists files in \/one\/two boolean - SmallFiles Property Description Set By User Unit Default NOTE: prepend small_files_use_avg_file_size_flag : average of all the files is used against the threshold criteria and either all the files are accepted\/counted or rejected\/not counted as per the criteria. : absolute file size is used against the threshold criteria and a file is accepted\/counted or rejected\/not counter as per the criteria. false true true files.small_files_min_parent_dir_depth If depth=2, search begins below 2 levels i.e. starts with directory such as HDFS_root\/one\/two Directory depth to start search at. 0 files.small_files_max_parent_dir_depth Depth=6: search ends at HDFS_root\/one\/two\/three\/four\/five\/six Depth=5: search ends at HDFS_root\/one\/two\/three\/four\/five : Example Directory depth to end search at. Maximum is 50. 10 files.small_files_drill_down_subdirs_flag When set a file is accounted (listed) for all it's ancestors. : a file is accounted in only it's immediate parent. This allows Unravel to find a specific directory with maximum number of files matching the size criteria. false t rue : given directory list is \/one\/two Example \/ - lists files in \/ false: \/one - lists files in one \/one\/two - lists files in \/one\/two \/ - lists files in \/, \/one and \/one\/two. true: \/one - lists files in \/one and \/one\/two \/one\/two - lists files in \/one\/two true File Reports Property Description Set By User Unit Default NOTE: unravel_ondemand prepend: huge_files_threshold_size file size >= threshold_size bytes 100GB huge_files_min_files number of files in directory >= min_files count 1 huge_files_top_n_dirs maximum number of directories to display count 10 medium_files_max_threshold_size file size <= max_threshold_size bytes 10GB medium_files_min_threshold_size file size >= min_threshold_size bytes 5GB medium_files_min_files number of files in directory >= min_files count 5 medium_files_top_n_dirs maximum number of directories to display count 20 tiny_files_threshold_size file size <= threshold_size bytes 100KB tiny_files_min_files maximum number of directories to display count 10 tiny_files_top_n_dirs maximum number of directories to display count 30 empty_files_min_files number of files in directory >= min_files count 10 empty_files_top_n_dirs maximum number of directories to display count 03 prepend: The following four properties are defined per file size Size Size See here boolean false Size See here count 0 Size See here count 10 Size See here boolean false Queue Analysis Property Definition Set By User Default Default prepend metrics.sensor.enabled Enables or disables queue metric sensor. boolean true poll.interval.msec How often queue metric sensor polls polls YARN Resource Manager. ms 60000 http.timeout.msec YARN Resource Manager HTTP connection timeout. ms 10000 http.retries YARN Resource Manager HTTP connection retries. count 3 http.retry.period.msec YARN Resource Manager HTTP connection retry wait period. ms 0 prepend metrics.scale UI rendered graph metrics scale factor. number 1000 daterange.span UI report date picker range. days 30 Forecasting Reports & Cloud Reports Property Definition Set By User Unit Default unravel.python.reporting.cloudreport.enable Enable\/disable all ondemand cloud reports boolean true Set cluster properties. Set cluster manager properties. Be sure to only set the properties for your cloud provider, either Cloudera or Ambari. App Parameter Defaults Report N\/A " }, 
{ "title" : "Unravel Servers and Sensors", 
"url" : "adv/unravel-servers-and-sensors.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Servers and Sensors", 
"snippet" : "Installing Sensors Individual Applications Submitted Through spark-submit Individual Hive Queries Upgrading the Unravel Server and Sensors Uploading Spark Programs to Unravel Uninstalling Unravel Server...", 
"body" : " Installing Sensors Individual Applications Submitted Through spark-submit Individual Hive Queries Upgrading the Unravel Server and Sensors Uploading Spark Programs to Unravel Uninstalling Unravel Server " }, 
{ "title" : "Installing Sensors", 
"url" : "adv/unravel-servers-and-sensors/installing-sensors.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Installing Sensors", 
"snippet" : "Individual Applications Submitted Through spark-submit Individual Hive Queries...", 
"body" : " Individual Applications Submitted Through spark-submit Individual Hive Queries " }, 
{ "title" : "Individual Applications Submitted Through spark-submit", 
"url" : "adv/unravel-servers-and-sensors/installing-sensors/individual-applications-submitted-through-spark-submit.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Installing Sensors \/ Individual Applications Submitted Through spark-submit", 
"snippet" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications only (in other words, per-application profiling cluster-wide profiling The information here applies to Spark versions 1.3.x through 2.0.x. HIGHLIGHTED UNRAVEL_HOST_IP Obtain the Sensor. The...", 
"body" : "Introduction This topic explains how to set up Unravel Sensor for Spark to profile specific Spark applications only (in other words, per-application profiling cluster-wide profiling The information here applies to Spark versions 1.3.x through 2.0.x. HIGHLIGHTED UNRAVEL_HOST_IP Obtain the Sensor. The Unravel Sensor is included in the Unravel Server RPM installation. After installing the Unravel Server RPM on UNRAVEL_HOST_IP http:\/\/UNRAVEL_HOST_IP:3000\/hh\/unravel-agent-pack-bin.zip To obtain Unravel Sensor from the filesystem on the Unravel Server host: Go to the \/usr\/local\/unravel\/webapps\/ROOT\/hh\/ Locate the sensor file: unravel-agent-pack-bin.zip Run the Sensor to Intercept Spark Apps. Option A: If You Run Spark Apps in yarn-cluster Mode (Default) Put the sensor on the host node(s) from which you will run spark-submit by first creating a destination directory that is readable by all users. We suggest that UNRAVEL_SENSOR_PATH \/usr\/local\/unravel-spark If spark-submit # mkdir {UNRAVEL_SENSOR_PATH}\n# cd {UNRAVEL_SENSOR_PATH}\n# wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unravel-agent-pack-bin.zip If spark-submit UNRAVEL_SENSOR_PATH hdfs:\/\/\/tmp # mkdir {UNRAVEL_SENSOR_PATH}\n# cd {UNRAVEL_SENSOR_PATH}\n# wget http:\/\/{UNRAVEL_HOST_IP}3000\/hh\/unravel-agent-pack-bin.zip\n# cd {UNRAVEL_SENSOR_PATH}\n# hdfs fs -copyFromLocal unravel-agent-pack-bin.zip \/tmp\n# set UNRAVEL_SENSOR_PATH=\"hdfs:\/\/\/tmp\" Define spark.driver.extraJavaOptions and spark.executor.extraJavaOptions as part of your spark-submit command. To use the example below, substitute your local values for: (UNRAVEL_SENSOR_PATH} unravel-agent-pack-bin.zip (UNRAVEL_SENSOR_PATH} (UNRAVEL_SERVER_IP_PORT} unravel_lr IP:PORT 10.0.0.142:4043 {SPARK_EVENT_LOG_DIR} {PATH_TO_SPARK_EXAMPLE_JAR} Absolute spark-submit {SPARK_VERSION} 1.3 1.5 1.6 2.0 export UNRAVEL_SENSOR_PATH={UNRAVEL_SENSOR_PATH}\nexport UNRAVEL_SERVER_IP_PORT={UNRAVEL_SERVER_IP_PORT}\nexport SPARK_EVENT_LOG_DIR={SPARK_EVENT_LOG_DIR}\nexport PATH_TO_SPARK_EXAMPLE_JAR={PATH_TO_SPARK_EXAMPLE_JAR}\nexport SPARK_VERSION={SPARK_VERSION}\n\nexport ENABLED_SENSOR_FOR_DRIVER=\"spark.driver.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=driver\"\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=executor\"\n\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --master yarn-cluster \\\n --archives $UNRAVEL_SENSOR_PATH\/unravel-agent-pack-bin.zip \\\n --conf \"$ENABLED_SENSOR_FOR_DRIVER\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR Option B: If You Run Spark Apps in yarn-client Mode. To intercept Spark apps running in yarn-client UNZIPPED_ARCHIVE_DEST \/usr\/local\/unravel-spark Important Please keep the original unravel-agent-pack-bin.zip UNZIPPED_ARCHIVE_DEST If you use multiple hosts as clients, on each client. # mkdir {UNZIPPED_ARCHIVE_DEST}\n# cd {UNZIPPED_ARCHIVE_DEST} \n# wget http:\/\/{UNRAVEL_HOST_IP}:3000\/hh\/unravel-agent-pack-bin.zip\n# unzip unravel-agent-pack-bin.zip Define spark.executor.extraJavaOptions as part of your spark-submit command. To use the example below, substitute your local values for: (UNZIPPED_ARCHIVE_DEST} (UNRAVEL_SERVER_IP_PORT} unravel_lr IP:PORT 10.0.0.142:4043 {SPARK_EVENT_LOG_DIR} {PATH_TO_SPARK_EXAMPLE_JAR} Absolute spark-submit {SPARK_VERSION} 1.3 1.5 1.6 2.0 export UNZIPPED_ARCHIVE_DEST={UNZIPPED_ARCHIVE_DEST}\nexport UNRAVEL_SERVER_IP_PORT={UNRAVEL_SERVER_IP_PORT}\nexport SPARK_EVENT_LOG_DIR={SPARK_EVENT_LOG_DIR}\nexport PATH_TO_SPARK_EXAMPLE_JAR={PATH_TO_SPARK_EXAMPLE_JAR}\nexport SPARK_VERSION={SPARK_VERSION}\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=executor\"\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --master yarn-client \\\n --archives $UNZIPPED_ARCHIVE_DEST\/unravel-agent-pack-bin.zip \\\n --driver-java-options \"-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=driver,libs=spark-$SPARK_VERSION\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR " }, 
{ "title" : "Individual Hive Queries", 
"url" : "adv/unravel-servers-and-sensors/installing-sensors/individual-hive-queries.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Installing Sensors \/ Individual Hive Queries", 
"snippet" : "The Unravel JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-agent-pack-bin.zip HIGHLIGHTED Set UNRAVEL_HOST_IP When you enable this sensor, it uses the standard Map...", 
"body" : "The Unravel JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-agent-pack-bin.zip HIGHLIGHTED Set UNRAVEL_HOST_IP When you enable this sensor, it uses the standard MapReduce profiling extension. You can copy and paste the following configuration snippets for a quick bootstrap: Option A: Installing the MapReduce JVM Sensor on Cloudera Option B: Installing the MapReduce JVM Sensor on Cloudera Manager for Hive Option C: Installing the MapReduce JVM Sensor on HDFS Option D: Installing the MapReduce JVM Sensor on the Local File System Option A: Installing the MapReduce JVM Sensor on Cloudera Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5; set mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set mapreduce.task.profile.params=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport=UNRAVEL_HOST_IP:4043; Enable the JVM agent for application master: set yarn.app.mapreduce.am.command-opts=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP Option B: Installing the MapReduce JVM Sensor on Cloudera Manager for Hive See Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide Step 2: Install Unravel Sensor and Configure Impala Option C: Installing the MapReduce JVM Sensor on HDFS Change your *init Set the path to the JVM sensor archive set mapreduce.job.cache.archives=path_in_hdfs\/unravel-agent-pack-bin.zip; Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5; set mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set mapreduce.task.profile.params=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP Enable the JVM agent for application master: set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP Option D: Installing the MapReduce JVM Sensor on the Local File System Add the JVM sensor archive to the PATH environment variable. Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5; set mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set mapreduce.task.profile.params=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP Enable the JVM agent for application master: set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= UNRAVEL_HOST_IP " }, 
{ "title" : "Upgrading the Unravel Server and Sensors", 
"url" : "adv/unravel-servers-and-sensors/upgrading-the-unravel-server-and-sensors.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Upgrading the Unravel Server and Sensors", 
"snippet" : "See Unravel Software Versions Contact Unravel Support at support@unraveldata.com Unravel Server Unravel Sensor Upgrade sensors on CDH cluster Upgrade sensors on HDP cluster Upgrade sensors on MAPR cluster Unravel Server Be sure to read the release note regarding upgrading the unravel server; in some...", 
"body" : "See Unravel Software Versions Contact Unravel Support at support@unraveldata.com \n Unravel Server \n Unravel Sensor \n Upgrade sensors on CDH cluster \n Upgrade sensors on HDP cluster \n Upgrade sensors on MAPR cluster Unravel Server Be sure to read the release note regarding upgrading the unravel server; in some cases it is mandatory to upgrade the sensors for the new unravel server version. This topic explains how to upgrade the Unravel Server RPM in a single or multi-host environment. For single Unravel gateway or client host, run only the commands that are marked as host1 Lines beginning with '\/\/' are comments. Copy the new RPM to each Unravel host. Stop each host simultaneously. \/\/ host1 \n# sudo \/etc\/init.d\/unravel_all.sh stop \n\/\/ host1 \n# sudo \/etc\/init.d\/unravel_all.sh stop \n\/\/ host3\n# sudo \/etc\/init.d\/unravel_all.sh stop Upgrade the RPM on each host simultaneously. \/\/ host1\n# sudo rpm -U unravel-4.*.x86_64.rpm* \n\/\/ host2\n# sudo rpm -U unravel-4.*.x86_64.rpm* \n\/\/ host3\n# sudo rpm -U unravel-4.*.x86_64.rpm* Add your license key to unravel.properties. You must enter add license key to unravel.properties After all the RPM upgrades finish, restart Unravel Server on each host simultaneously. \/\/ host1\n# sudo \/etc\/init.d\/unravel_all.sh start \n\/\/ host2\n# sudo \/etc\/init.d\/unravel_all.sh start \n\/\/ host23\n# sudo \/etc\/init.d\/unravel_all.sh start Complete any deployment-specific upgrade steps. Unravel Sensor \n HIGHLIGHTED \n UNRAVEL_HOST_IP \n SPARK_VERSION _X.Y.Z \n HIVE_VERSION_X.Y.Z Upgrade sensors on CDH cluster Only activate the sensors when there are no Hive or Spark jobs running as the restart may affect running jobs. Check you current sensor version. Log in to the Cloudera Manager and click the parcel () in the top menu bar. Click the Check for New Parcels button and look for UNRAVEL_SENSOR entries. If newer sensors are available, it will be shown as another entry like below. Click the Download and then Distribute button, then activate for the newer version of the sensors. When activating the new sensors, you will be notified that Hive and Spark services must be restarted. Once new sensor activation is completed, the old version is automatically disabled. Upgrade sensors on HDP cluster Only activate the sensors when there are no Hive or Spark jobs running as the restart may affect running jobs. Login to the unravel server node, and all the cluster nodes that have unravel sensors deployed. Backup the old sensor folders then remove the sensors folders. # cd \/usr\/local\/\n# sudo tar -cvf unravel-sensors-`date +%m%d%y`.tar unravel-agent unravel_client\n# sudo rm -rf \/usr\/local\/unravel-agent\n# sudo rm -rf \/usr\/local\/unravel_client Run the sensor package script on unravel server node. # cd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/\n# sudo .\/unravel_hdp_setup.sh install -y --unravel-server {UNRAVEL_HOST_IP}:3000 --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z} Running the sensor package script creates the sensor packages in \/usr\/local\/unravel-agent and \/usr\/local\/unravel_client folders. Tar these two folders and scp to all cluster nodes. # cd \/usr\/local\/\n# tar -cvf unravel-new-sensors.tar unravel-agent unravel_client\n# scp unravel-new-sensors.tar ${CLUSTER_NODE}:\/usr\/local\/ On the cluster node, extract the copied unravel-new-sensors.tar file on the cluster nodes. # cd \/usr\/local\/ \n# tar -xvf unravel-new-sensors.tar Upgrade sensors on MAPR cluster Only activate the sensors when there are no Hive or Spark jobs running as the restart may affect running jobs. Login to the unravel server node, and all the cluster nodes that have unravel sensors deployed. Backup the old sensor folders then remove the sensors folders. # cd \/usr\/local\/\n# sudo tar -cvf unravel-sensors-`date +%m%d%y`.tar unravel-agent unravel_client\n# sudo rm -rf \/usr\/local\/unravel-agent\n# sudo rm -rf \/usr\/local\/unravel_client\n# cd \/opt\/mapr\/spark\/spark-{SPARK_VERSION_X.Y.Z}\/conf\/\n# sudo mv spark-defaults.conf.pre_unravel spark-defaults.conf.pre_unravel.copy Run the sensor package script on unravel server node. # cd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/\n# sudo .\/unravel_mapr_setup.sh install -y --unravel-server {UNRAVEL_HOST_IP}:3000 --spark-version {SPARK_VERSION_X.Y.Z} --hive-version {HIVE_VERSION_X.Y.Z}\n Running the sensor package script creates the sensor packages in \/usr\/local\/unravel-agent and \/usr\/local\/unravel_client folders. Tar these two folders and scp to all cluster nodes. # cd \/usr\/local\/\n# tar -cvf unravel-new-sensors.tar unravel-agent unravel_client\n# scp unravel-new-sensors.tar ${CLUSTER_NODE}:\/usr\/local\/ On the cluster node, extract the copied unravel-new-sensors.tar file on the cluster nodes. # cd \/usr\/local\/ \n# tar -xvf unravel-new-sensors.tar " }, 
{ "title" : "Uploading Spark Programs to Unravel", 
"url" : "adv/unravel-servers-and-sensors/uploading-spark-programs-to-unravel.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Uploading Spark Programs to Unravel", 
"snippet" : "Unravel can display your uploaded Spark programs in the UI. Spark programs must be submitted as Java, Scala or Python source code; not as JVM byte code. You have two options for uploading Spark programs: HIGHLIGHTED Option 1: Upload Individual Source Files Upload Spark source files and specify their...", 
"body" : "Unravel can display your uploaded Spark programs in the UI. Spark programs must be submitted as Java, Scala or Python source code; not as JVM byte code. You have two options for uploading Spark programs: \n HIGHLIGHTED Option 1: Upload Individual Source Files Upload Spark source files and specify their location on the spark-submit Example: In yarn-client mode local \n --conf \"spark.unravel.program.dir=$PROGRAM_DIR\" spark-submit export PROGRAM_DIR={FULLY_QUALIFIED_PATH_TO_LOCAL_FILE_DIRECTORY} \nexport PATH_TO_SPARK_EXAMPLE_JAR={FULLY_QUALIFIED_JAR_PATH} \n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --files {Comma separated list of files} \\\n --conf \"spark.unravel.program.dir=$PROGRAM_DIR\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR The default value of spark.unravel.program.dir Example: In yarn-cluster mode \n --files {comma-separated-list-of-source-files} spark-submit export PROGRAM_DIR={FULLY_QUALIFIED_PATH_TO_SOURCE_FILE_DIRECTORY}\nexport PATH_TO_SPARK_EXAMPLE_JAR={FULLY_QUALIFIED_JAR_PATH}\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --files {comma-separated-list-of-source-files} \\\n $PATH_TO_SPARK_EXAMPLE_JAR Option 2: Upload a Zip Archive Package all relevant source files into a zip archive. It's advisable to keep the archive small by including only the relevant driver source files. Example: In yarn-client mode local \n --conf \"spark.unravel.program.zip=$SRC_ZIP_PATH\" spark-submit export PROGRAM_DIR={FULLY_QUALIFIED_ZIP_PATH}\nexport SRC_ZIP_PATH=$PROGRAM_DIR\/{SRC_ZIP_NAME}\nexport PATH_TO_SPARK_EXAMPLE_JAR={FULLY_QUALIFIED_JAR_PATH}\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --conf \"spark.unravel.program.zip=$SRC_ZIP_PATH\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR Example: In yarn-cluster mode \n --files $SRC_ZIP_PATH --conf \"spark.unravel.program.zip={SRC_ZIP_NAME}\" spark-submit export PROGRAM_DIR={FULLY_QUALIFIED_ZIP_PATH}\nexport SRC_ZIP_PATH=$PROGRAM_DIR\/{SRC_ZIP_NAME}\nexport PATH_TO_SPARK_EXAMPLE_JAR={FULLY_QUALIFIED_JAR_PATH} \n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --files $SRC_ZIP_PATH \\\n --conf \"spark.unravel.program.zip={SRC_ZIP_NAME}\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR Unravel searches for source files in this order: \n spark.unravel.program.dir Application home directory (Option 1) Zip archive provided as spark.unravel.program.zip After the Spark application has completed, you can see the Spark program(s) in Unravel UI under Applications Program " }, 
{ "title" : "Uninstalling Unravel Server", 
"url" : "adv/unravel-servers-and-sensors/uninstalling-unravel-server.html", 
"breadcrumbs" : "Unravel 4.5 \/ Advanced Topics \/ Unravel Servers and Sensors \/ Uninstalling Unravel Server", 
"snippet" : "Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel # sudo rpm -e unravel # sudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm...", 
"body" : " Disable and uninstall cluster instrumentation before uninstalling Unravel Server. The Unravel Server RPM has the service name unravel # sudo rpm -e unravel\n# sudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl All data in the bundled database is deleted by the rm " }, 
{ "title" : "Appendices", 
"url" : "appx.html", 
"breadcrumbs" : "Unravel 4.5 \/ Appendices", 
"snippet" : "Server Daemon Reference HBASE Alerts and Metrics...", 
"body" : " Server Daemon Reference HBASE Alerts and Metrics " }, 
{ "title" : "Server Daemon Reference", 
"url" : "appx/appx-server-daemon.html", 
"breadcrumbs" : "Unravel 4.5 \/ Appendices \/ Server Daemon Reference", 
"snippet" : "This reference covers both the on-premises and Unravel for EMR editions. Some daemons and properties do not apply to the EMR edition....", 
"body" : "This reference covers both the on-premises and Unravel for EMR editions. Some daemons and properties do not apply to the EMR edition. " }, 
{ "title" : "Unravel Server Daemons", 
"url" : "appx/appx-server-daemon.html#UUID-25c1a395-8c9e-021a-d153-21687bfe7636_id_ServerDaemonReference-_bwaxof3tb14eUnravelServerDaemons", 
"breadcrumbs" : "Unravel 4.5 \/ Appendices \/ Server Daemon Reference \/ Unravel Server Daemons", 
"snippet" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_db bundled db (on a custom port) unravel_ds Datastore REST API HTTP server unravel_ew_N Event Worker unra...", 
"body" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh start stop restart _N Daemon Logical Name Description unravel_db bundled db (on a custom port) unravel_ds Datastore REST API HTTP server unravel_ew_N Event Worker unravel_hhwe Hive Hook Worker EMR unravel_hl Hitdoc Loader unravel_hostN Host monitor unravel_ja \"Job Analyzer\" summarizes jobs unravel_jcs2 Job Collector Sensor YARN unravel_jcse2 Job Collector Sensor YARN for EMR unravel_jcw2_N Job Collector Sensor Worker YARN unravel_k bundled Kafka (on a custom port) unravel_km Kafka Monitor unravel_lr Log Receiver unravel_ma_N Metrics Analyzer unravel_ngui aNGular web UI unravel_os4 Oozie v4 Sensor unravel_pw Partition Worker unravel_s_N Elasticsearch unravel_sw_N Spark Worker unravel_tc bundled TomCat (port 4020), internal REST API unravel_td \"Tidy Dir\" cleans up and archives hdfs directories, db retention cleaner unravel_tw Table Worker unravel_ud User Digest (report generator) unravel_us_N Universal sensor \\ Impala unravel_zk_N bundled Zookeeper (on a custom port) " }, 
{ "title" : "Unravel Server Adjustable Properties", 
"url" : "appx/appx-server-daemon.html#UUID-25c1a395-8c9e-021a-d153-21687bfe7636_id_ServerDaemonReference-_ranitvt6e5pgUnravelServerAdjustableProperties", 
"breadcrumbs" : "Unravel 4.5 \/ Appendices \/ Server Daemon Reference \/ Unravel Server Adjustable Properties", 
"snippet" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Name\/Description Property Type\/ Default Value General Unravel com.unraveldata.logdir Do not set directly; set UNRAVEL_LOG_DIR etc\/unravel.ext.sh \/usr\/local\/unravel\/logs com.unraveldata.tmpdir Determines where daemons will put temporary file...", 
"body" : "The file \/usr\/local\/unravel\/etc\/unravel.properties Property Name\/Description Property Type\/ Default Value General Unravel com.unraveldata.logdir Do not set directly; set UNRAVEL_LOG_DIR etc\/unravel.ext.sh \/usr\/local\/unravel\/logs com.unraveldata.tmpdir Determines where daemons will put temporary files. Depending on the daemon role, up to 10GB might be used temporarily. \/srv\/unravel\/tmp or $UNRAVEL_DATA_DIR\/tmp HDFS com.unraveldata.hdfs.batch.monitoring.interval.sec Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for batch visibility; should be between 300 and 1800 (inclusive) 300 com.unraveldata.hdfs.interactive.monitoring.interval.sec Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for interactive visibility; should be between 5 and 60 (inclusive) 30 JDBC unravel.jdbc.username MySQL (embedded or external) username for db unravel unravel.jdbc.password MySQL (embedded or external) password for db random generated for bundled MySQL unravel.jdbc.url This is JDBC URL without username and password jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prodc Kafka com.unraveldata.kafka.broker_list embedded 127.0.0.1:4091 mapreduce com.unraveldata.longest.job.duration.days Number of days for the longest running Map Reduce job ever expected; should be between 2 and 7 (inclusive) 2 Oozie com.unraveldata.oozie.fetch.interval.sec seconds between intervals for fetching Oozie workflow status 120 com.unraveldata.oozie.fetch.num Max number of jobs to fetch during an interval 100 oozie.server.url URL for accessing Oozie to track workflows http:\/\/localhost:11000\/oozie Zookeeper com.unraveldata.zk.quorum embedded Zookeeper ensemble in form host1:port1,host2:port2, 127.0.0.1:4181 " }, 
{ "title" : "Adjustable Environment Settings", 
"url" : "appx/appx-server-daemon.html#UUID-25c1a395-8c9e-021a-d153-21687bfe7636_id_ServerDaemonReference-_dhjhvxwiog21AdjustableEnvironmentSettings", 
"breadcrumbs" : "Unravel 4.5 \/ Appendices \/ Server Daemon Reference \/ Adjustable Environment Settings", 
"snippet" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source' Env Variable Description Default JAVA_HOME The standard way to specify the home directory of Oracle Java so that $JAVA_HOME\/bin\/java Should use update-alternatives to make correct Java first choice JAVA_EXT_OPTS Last chance arguments to...", 
"body" : "The optional file \/usr\/local\/unravel\/etc\/unravel.ext.sh source' Env Variable Description Default JAVA_HOME The standard way to specify the home directory of Oracle Java so that $JAVA_HOME\/bin\/java Should use update-alternatives to make correct Java first choice JAVA_EXT_OPTS Last chance arguments to jvm to override other settings unset HADOOP_CONF_DIR The directory containing the hadoop config files core-site.xml hdfs-site.xml mapred-site.xml as discovered by running \"hadoop fs -ls \" UNRAVEL_DATA_DIR A base dir. owned by user unravel, during installation unravel creates multiple sub dirs for holding persistent data ( db_data k_data, zk_data tmp_data com.unraveldata.tmpdir \/srv\/unravel UNRAVEL_LISTEN_PORT The Web UI port on the primary or standalone Unravel installation ( service unravel_ngui 3000 UNRAVEL_LOG_DIR A destination directory owned by run-as user for log files \/usr\/local\/unravel\/logs " }, 
{ "title" : "Adjustable Root Environment Settings", 
"url" : "appx/appx-server-daemon.html#UUID-25c1a395-8c9e-021a-d153-21687bfe7636_id_ServerDaemonReference-AdjustableRootEnvironmentSettings", 
"breadcrumbs" : "Unravel 4.5 \/ Appendices \/ Server Daemon Reference \/ Adjustable Root Environment Settings", 
"snippet" : "The optional file \/etc\/unravel_ctl Env Variable Description Default if not set RUN_AS The \/etc\/init.d\/unravel_* unravel USE_GROUP The primary group membership of the user that runs the daemons unravel...", 
"body" : "The optional file \/etc\/unravel_ctl Env Variable Description Default if not set RUN_AS The \/etc\/init.d\/unravel_* unravel USE_GROUP The primary group membership of the user that runs the daemons unravel " }, 
{ "title" : "Unravel Server Directories and Files", 
"url" : "appx/appx-server-daemon.html#UUID-25c1a395-8c9e-021a-d153-21687bfe7636_id_ServerDaemonReference-_37le18boksr8UnravelServerDirectoriesandFiles", 
"breadcrumbs" : "Unravel 4.5 \/ Appendices \/ Server Daemon Reference \/ Unravel Server Directories and Files", 
"snippet" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path\/Purpose Expected Size Notes \/etc\/unravel_ctl control file for run-as n\/a Must be owned by root for security reasons. \/etc\/init.d\/unravel_all.sh convenience script for Unravel start, restart, status, and ...", 
"body" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path\/Purpose Expected Size Notes \/etc\/unravel_ctl control file for run-as n\/a Must be owned by root for security reasons. \/etc\/init.d\/unravel_all.sh convenience script for Unravel start, restart, status, and stop ; controls daemons in proper order n\/a Note for multi-host installations: run this script on both primary and secondary Unravel host \/usr\/local\/unravel Unravel Server installation directory 1-2.5GB This directory is created by installing the Unravel RPM; this is a fixed destination \/usr\/local\/unravel\/etc\/unravel.ext.sh an optional file for overriding JAVA_HOME n\/a Optional; example syntax: export JAVA_HOME=\/path \/usr\/local\/unravel\/etc\/unravel.properties site-specific settings for Unravel n\/a Keep a \"golden\" copy of this file; rpm -U \/usr\/local\/unravel\/etc\/unravel.version.properties version-specific values for Unravel like version number, build timestamp n\/a Updated during upgrades; do not modify this reference file in order to preserve traceability \/usr\/local\/unravel\/logs Logs written by Unravel daemons ~3.5GB max Each daemon will have a maximum of 100MB of logs, auto-rolled; use a symlink to put on another partition. \/srv\/unravel\/ Base directory for Unravel server data kept separately from installation directory; contains messaging data for process coordination, bundled db, Elasticsearch indexes, temporary files 2-900GB ; depending on activity level, retention This directory or subdirectories can be a symlinks to other volumes for disk io performance reasons to distribute load over multiple volumes. If this is an EBS on AWS, then it must be provisioned for max available IOPS and the Unravel server must be EBS optimized. " }, 
{ "title" : "HBASE Alerts and Metrics", 
"url" : "appx/appx-hbase.html", 
"breadcrumbs" : "Unravel 4.5 \/ Appendices \/ HBASE Alerts and Metrics", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Alerts", 
"url" : "appx/appx-hbase.html#UUID-df21037e-b075-152c-375a-30ebe127ae3f_id_HBASEAlertsandMetrics-Alerts", 
"breadcrumbs" : "Unravel 4.5 \/ Appendices \/ HBASE Alerts and Metrics \/ Alerts", 
"snippet" : "Alerts generated and stored along with metrics. UI plots this information as appropriate. Category Alert Suggested Action Data availability Table offline Run \"hbase hbck\" to see if your HBase cluster has corruptions and use -repair flag if required. Check master logs for more information Region offl...", 
"body" : "Alerts generated and stored along with metrics. UI plots this information as appropriate. Category Alert Suggested Action Data availability Table offline Run \"hbase hbck\" to see if your HBase cluster has corruptions and use -repair flag if required. Check master logs for more information Region offline Run \"hbase hbck\" to see if your HBase cluster has corruptions and use -repair flag if required. Check master logs for more information Region in transition beyond threshold period. If a region server is dead, this is common. If not run \"hbase hbck\" to see if your HBase cluster has corruptions. Server availability Dead region servers Check region server logs for more information Performance Region servers with reads > 20% of avg Region server hotspotting. Split regions or randomize the keys. Region servers with writes > 20% of avg Region server hotspotting. Split regions or randomize the keys. Regions within a table with reads > 20% of avg for that table Table hotspotting - Split regions or randomize the keys Regions within a table with writes > 20% of avg for that table Table hotspotting - Split regions or randomize the keys Regions within a regionserver with reads > 20% of avg for that table Region server hotspotting - Split regions or randomize the keys Regions within a regionserver with writes > 20% of avg for that table Region server hotspotting - Split regions or randomize the keys Load, osload > 20% of avg Check for compactions, regions in transition and server logs Balancer not running Enable Blancer # of compactions and length of compaction Disable periodic automatic major compactions by setting - hbase.hregion.majorcompaction to 0 Storage Regionservers with storage (storefilesie sum) > 20% of avg Split or randomize the keys Regions within a table with storage (storefilesie sum) > 20% of avg for that table Split or randomize the keys Temporal e.g. requests > 20% higher for the last 1 hour as compared to the prior 3 hours (just an example) Check master and region server alerts or environment issues which could be slowing down the read\/write " }, 
{ "title" : "Metrics", 
"url" : "appx/appx-hbase.html#UUID-df21037e-b075-152c-375a-30ebe127ae3f_id_HBASEAlertsandMetrics-HBaseClusterMetrics", 
"breadcrumbs" : "Unravel 4.5 \/ Appendices \/ HBASE Alerts and Metrics \/ Metrics", 
"snippet" : "Master\/Cluster & JMX Metrics Metric Description Unit averageLoad Average number of Regions per Region Server percentage clusterRequests Number of read and write requests across Cluster count masterActiveTime Master Active Time epoch in milliseconds masterStartTime Master Start Time epoch in millisec...", 
"body" : " Master\/Cluster & JMX Metrics Metric Description Unit averageLoad Average number of Regions per Region Server percentage clusterRequests Number of read and write requests across Cluster count masterActiveTime Master Active Time epoch in milliseconds masterStartTime Master Start Time epoch in milliseconds numDeadRegionServers Number of dead Region Servers count numRegionServers Number of live Region Servers count ritCount The number of regions in transition count ritCountOverThreshold The number of regions that have been in transition longer than a threshold time seconds ritOldestAge The age of the longest region in transition, in milliseconds millliseconds OS Metrics (Ambari Only) OS Metrics Description Unit jvm_* jvm metrics number rpc_* rpc metrics number " }, 
{ "title" : "Region Server Metrics", 
"url" : "appx/appx-hbase.html#UUID-df21037e-b075-152c-375a-30ebe127ae3f_id_HBASEAlertsandMetrics-RegionServerMetrics", 
"breadcrumbs" : "Unravel 4.5 \/ Appendices \/ HBASE Alerts and Metrics \/ Region Server Metrics", 
"snippet" : "JMX Metrics JMX Metrics Description Unit compactionQueueLength Current depth of the compaction request queue. If increasing, we are falling behind with storefile compaction. count hlogFileSie Sie of all WAL Files bytes percentFilesLocal Percent of store file data that can be read from the local Data...", 
"body" : " JMX Metrics JMX Metrics Description Unit compactionQueueLength Current depth of the compaction request queue. If increasing, we are falling behind with storefile compaction. count hlogFileSie Sie of all WAL Files bytes percentFilesLocal Percent of store file data that can be read from the local DataNode, 0-100 percentage readRequestCount The number of read requests received count regionCount The number of regions hosted by the regionserver count slowOPCount The number of operations we thought were slow. OP: delete, get, put, increment, append count storeFileSize Aggregate size of the store files on disk bytes writeRequestCount The number of write requests received count OS Metrics (Ambari Only) OS Metrics Description Unit cpu_user cpu percentage disk.disk_free Amount of free disk space bytes disk.write_bps Number of bytes written per second to disk. bytes per second disk.read_bps Number of bytes read per second to disk. bytes per second load.load_one load number memory.mem_free Percentage of free memory. percentage network.bytes_in Total number incoming bytes to network. bytes network.bytes_out Total number outgoing bytes to network. bytes " }, 
{ "title" : "Table\/Region Metrics", 
"url" : "appx/appx-hbase.html#UUID-df21037e-b075-152c-375a-30ebe127ae3f_id_HBASEAlertsandMetrics-TableRegionMetrics", 
"breadcrumbs" : "Unravel 4.5 \/ Appendices \/ HBASE Alerts and Metrics \/ Table\/Region Metrics", 
"snippet" : "Table and Region Metrics Description Unit tableSize Total table size in the region server bytes regionCount Number of regions count averageRegionSize (Table only) Average region size over the region server including memstore and storefile sizes bytes storeFileSize Size of storefiles being served byt...", 
"body" : " Table and Region Metrics Description Unit tableSize Total table size in the region server bytes regionCount Number of regions count averageRegionSize (Table only) Average region size over the region server including memstore and storefile sizes bytes storeFileSize Size of storefiles being served bytes readRequestCount Number of read requests this region server has answered count writeRequestCount Number of mutation requests this region server has answered count " }, 
{ "title" : "Troubleshooting", 
"url" : "troubleshooting.html", 
"breadcrumbs" : "Unravel 4.5 \/ Troubleshooting", 
"snippet" : "Sending Diagnostics to Unravel Support <In progress> If you can't reach Unravel Server, (i) ping LANS_DNS, (ii) try this workaround:...", 
"body" : " Sending Diagnostics to Unravel Support <In progress> If you can't reach Unravel Server, (i) ping LANS_DNS, (ii) try this workaround: " }, 
{ "title" : "Sending Diagnostics to Unravel Support", 
"url" : "troubleshooting/sending-diagnostics-to-unravel-support.html", 
"breadcrumbs" : "Unravel 4.5 \/ Troubleshooting \/ Sending Diagnostics to Unravel Support", 
"snippet" : "In the upper right corner of Unravel Web UI, click the pull-down menu, and select Manage Wait for the page to fully load. Select the Diagnostics Click Send Diagnostics to Unravel Support This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com....", 
"body" : " In the upper right corner of Unravel Web UI, click the pull-down menu, and select Manage Wait for the page to fully load. Select the Diagnostics Click Send Diagnostics to Unravel Support This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com.unraveldata.login.admins If you don't have access to push the bundle through the Web UI. On the Unravel Host bundle the diagnostic information. # \/usr\/local\/unravel\/install_bin\/diag_dump.sh Email the bundle to the Unravel support team. " }, 
{ "title" : "Release Notes", 
"url" : "release-notes.html", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes", 
"snippet" : "v4.5.0.4 Release Notes v4.5.0.3 Release Notes v4.5.0.2 Release Notes v4.5.0 Release Notes v4.5.x - Upgrade Instructions v4.5.x - Updates to Unravel Properties...", 
"body" : " v4.5.0.4 Release Notes v4.5.0.3 Release Notes v4.5.0.2 Release Notes v4.5.0 Release Notes v4.5.x - Upgrade Instructions v4.5.x - Updates to Unravel Properties " }, 
{ "title" : "v4.5.0.4 Release Notes", 
"url" : "release-notes/rn-4504.html", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.4 Release Notes", 
"snippet" : "Contents Software Version Software Upgrade Support Certified Platforms Supported OS\/Software Supported Browsers Unravel Sensor Upgrade New Features Improvements Bug Fixes Known Issues v4.5.x - Updates to Unravel Properties...", 
"body" : " Contents Software Version Software Upgrade Support Certified Platforms Supported OS\/Software Supported Browsers Unravel Sensor Upgrade New Features Improvements Bug Fixes Known Issues v4.5.x - Updates to Unravel Properties " }, 
{ "title" : "Software Version", 
"url" : "release-notes/rn-4504.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-SoftwareVersion", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ Software Version", 
"snippet" : "Release Date: 02\/11\/2019 For details on downloading updates see here...", 
"body" : "Release Date: 02\/11\/2019 For details on downloading updates see here " }, 
{ "title" : "Software Upgrade Support", 
"url" : "release-notes/rn-4504.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-SoftwareUpgradeSupport", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ Software Upgrade Support", 
"snippet" : "Upgrade from 4.3 Upgrade from 4.4.0...", 
"body" : " Upgrade from 4.3 Upgrade from 4.4.0 " }, 
{ "title" : "Certified Platforms", 
"url" : "release-notes/rn-4504.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-CertifiedPlatforms", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ Certified Platforms", 
"snippet" : "Please also review the compatibility matrix CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 3.0, 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0...", 
"body" : "Please also review the compatibility matrix CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 3.0, 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0 " }, 
{ "title" : "Supported OS\/Software", 
"url" : "release-notes/rn-4504.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-SupportedOSSoftware", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ Supported OS\/Software", 
"snippet" : "OS RedHat\/Centos: 6.6-7.5 SELinux with Enforcement mode on Red Hat Enterprise Linux Server release 7.4 (Maipo) Targeted Policy v.28 Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47...", 
"body" : " OS RedHat\/Centos: 6.6-7.5 SELinux with Enforcement mode on Red Hat Enterprise Linux Server release 7.4 (Maipo) Targeted Policy v.28 Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47 " }, 
{ "title" : "SupportedBrowsers", 
"url" : "release-notes/rn-4504.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-SupportedBrowsers", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ SupportedBrowsers", 
"snippet" : "Chrome: 70.x Internet Explorer: IE 11...", 
"body" : " Chrome: 70.x Internet Explorer: IE 11 " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "release-notes/rn-4504.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-SensorUnravelSensorUpgrade", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ Unravel Sensor Upgrade", 
"snippet" : "Current *Sensor* Version Current CDH \/ HDP \/ MAPR Version Upgrade to 4.5.0.4Needed? 4.5.0.3 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.2 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.0 CDH 5.15\/C...", 
"body" : " Current *Sensor* Version Current CDH \/ HDP \/ MAPR Version Upgrade to 4.5.0.4Needed? 4.5.0.3 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.2 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.0 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.0rc0005) No 4.4.x CDH 5.15 (Parcel version:1.0.4400001) HDP 2.6.5 (Sensor version latest: 4.4.2.0b0007) Yes 4.3.x CDH 5.14 (Parcel version:1.0.65) HDP 2.6.5 Yes " }, 
{ "title" : "New Features", 
"url" : "release-notes/rn-4504.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-NewFeaturesNewFeatures", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ New Features", 
"snippet" : "Supports TLS for connections from Hive hook to Unravel edge node. (CUSTOMER-363) Improvements and Bug Fixes Improvements AutoActions: Improved navigability from History of Runs, including links to offending apps. (CUSTOMER-623 \/ CUSTOMER-630) All YARN jobs (MR, TEZ, Spark) can be killed\/moved in run...", 
"body" : " Supports TLS for connections from Hive hook to Unravel edge node. (CUSTOMER-363) Improvements and Bug Fixes Improvements AutoActions: Improved navigability from History of Runs, including links to offending apps. (CUSTOMER-623 \/ CUSTOMER-630) All YARN jobs (MR, TEZ, Spark) can be killed\/moved in running state from application page. (CUSTOMER-337) Bug Fixes RBAC tagcmd does not always work in all environments. (CUSTOMER-592) LDAP AUTH failing due to null pointer exception. (CUSTOMER-600) Tagged Spark jobs not showing in workflow view. (CUSTOMER-608) Diagnostics files are either empty or not complete. (CUSTOMER-621) OnDemand reports runs \"successfully\" but not showing in UI. (CUSTOMER-635) Queue analysis report generation fails with a \"x not in list\" error. (REPORT-277) Hbase is not working when clusterID contains a space, e.g., cluster 25. (HBASE-83) " }, 
{ "title" : "Known Issues", 
"url" : "release-notes/rn-4504.html#UUID-d05e00ab-aebc-d781-3a97-fe4de9d7cc23_id_v4504ReleaseNotes-KnownKnownIssues", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.4 Release Notes \/ Known Issues", 
"snippet" : "HDP 3.0 Does not support the Small Files Report. (REPORT-315) MR: Actions: Move to Queue does not work. (PLATFORM-1195) PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. HBASE OS Metrics currently does not support CDH. Special Characte...", 
"body" : " HDP 3.0 Does not support the Small Files Report. (REPORT-315) MR: Actions: Move to Queue does not work. (PLATFORM-1195) PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. HBASE OS Metrics currently does not support CDH. Special Characters in passwords are not supported. (CUSTOMER-440, CUSTOMER-552) fs image download will not trigger after install. (REPORT-262) Queue Analysis graph zooming and resetting not working properly in Edge. (UIX-1589) Cloud Reports: You must select an instance type Queue analysis data can be missing or skewed if queues were created or deleted in a middle of the report range. (REPORT-316) Top X Report: Hive-on-Tez apps will show vcoreseconds and memory seconds values as zero. (TEZLLAP-249) The group-by pie charts in Cluster Discovery may be empty if don't have enough historic data (CLOUD-221) Cloud discovery functionality in cloud reports may fail with \"UDObject\" object has no attribute \"warn\" Add support for custom SSL cert for Ambari and Cloudera Manager in ondemand reports (CUSTOMER-649) Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.5.0.3 Release Notes", 
"url" : "release-notes/rn-4503.html", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes", 
"snippet" : "Contents Software Version Software Upgrade Support Certified Platforms Supported OS\/Software Supported Browsers Unravel Sensor Upgrade New Features Improvements and Bug Fixes Improvements Bug Fixes Known Issues v4.5.x - Updates to Unravel Properties...", 
"body" : " Contents Software Version Software Upgrade Support Certified Platforms Supported OS\/Software Supported Browsers Unravel Sensor Upgrade New Features Improvements and Bug Fixes Improvements Bug Fixes Known Issues v4.5.x - Updates to Unravel Properties " }, 
{ "title" : "Software Version", 
"url" : "release-notes/rn-4503.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-SoftwareVersion", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Software Version", 
"snippet" : "Release Date: 01\/28\/2019 For details on downloading updates see here...", 
"body" : "Release Date: 01\/28\/2019 For details on downloading updates see here " }, 
{ "title" : "Software Upgrade Support", 
"url" : "release-notes/rn-4503.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-SoftwareUpgradeSupport", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Software Upgrade Support", 
"snippet" : "Upgrade from 4.3 Upgrade from 4.4.0...", 
"body" : " Upgrade from 4.3 Upgrade from 4.4.0 " }, 
{ "title" : "Certified Platforms", 
"url" : "release-notes/rn-4503.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-CertifiedPlatforms", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Certified Platforms", 
"snippet" : "Please also review the compatibility matrix CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0...", 
"body" : "Please also review the compatibility matrix CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0 " }, 
{ "title" : "Supported OS\/Software", 
"url" : "release-notes/rn-4503.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-SupportedOSSoftware", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Supported OS\/Software", 
"snippet" : "OS RedHat\/Centos: 6.6-7.5 SELinux with Enforcement mode on Red Hat Enterprise Linux Server release 7.4 (Maipo) Targeted Policy v.28 Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47...", 
"body" : " OS RedHat\/Centos: 6.6-7.5 SELinux with Enforcement mode on Red Hat Enterprise Linux Server release 7.4 (Maipo) Targeted Policy v.28 Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47 " }, 
{ "title" : "SupportedBrowsers", 
"url" : "release-notes/rn-4503.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-SupportedBrowsers", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ SupportedBrowsers", 
"snippet" : "Chrome: 70.x Internet Explorer: IE 11...", 
"body" : " Chrome: 70.x Internet Explorer: IE 11 " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "release-notes/rn-4503.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-SensorUnravelSensorUpgrade", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Unravel Sensor Upgrade", 
"snippet" : "Current *Sensor* Version Current CDH \/ HDP \/ MAPR Version Upgrade to 4.5.0.3Needed? 4.5.0.2 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.0 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.0rc0005) No 4.4.x CDH 5.15 (Pa...", 
"body" : " Current *Sensor* Version Current CDH \/ HDP \/ MAPR Version Upgrade to 4.5.0.3Needed? 4.5.0.2 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.2rc0006) No 4.5.0.0 CDH 5.15\/CDH 6.0 (Parcel version:1.0.4500006) HDP 2.6.5 (Sensor version: 4.5.0.0rc0005) No 4.4.x CDH 5.15 (Parcel version:1.0.4400001) HDP 2.6.5 (Sensor version latest: 4.4.2.0b0007) Yes 4.3.x CDH 5.14 (Parcel version:1.0.65) HDP 2.6.5 Yes " }, 
{ "title" : "New Features", 
"url" : "release-notes/rn-4503.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-NewFeaturesNewFeatures", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ New Features", 
"snippet" : "NONE...", 
"body" : "NONE " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "release-notes/rn-4503.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-KnownImprovementsandBugFixes", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Improvements and Bug Fixes", 
"snippet" : "Improvements Auth tokens for REST API expiration date extended along with ability to invalidate them. (CUSTOMER-495) Airflow now supports HTTP. (CUSTOMER-456) Support added for encrypted password from unravel.properties for OnDemand. (CUSTOMER-594) Support for SELinux is enabled on Unravel Edge Node...", 
"body" : "Improvements Auth tokens for REST API expiration date extended along with ability to invalidate them. (CUSTOMER-495) Airflow now supports HTTP. (CUSTOMER-456) Support added for encrypted password from unravel.properties for OnDemand. (CUSTOMER-594) Support for SELinux is enabled on Unravel Edge Node. (CUSTOMER-419) Bug Fixes Impala Chargeback shows \"User\" field data in report. (CUSTOMER-605) Unravel Spark sensor supports clusterId with spaces. (CUSTOMER-595) Airflow connectivity fixed. (CUSTOMER-575) Airflow Workflow are now displayed on Workflow page. (CUSTOMER-458) unravel_ondemand daemon is now included in unravel_all.sh. (CUSTOMER-434, CUSTOMER-375) Queue Analysis collects metrics for nested queue on HDP when configured to use Fair Scheduler. (REPORT-294) Queue metric sensor works with remote clusters if RM is not accessible via HTTP or if TLS with authentication is enabled, (PLATFORM-927, PLATFORM-1052) " }, 
{ "title" : "Known Issues", 
"url" : "release-notes/rn-4503.html#UUID-99ae137c-a9fa-7714-af57-23b6585a4448_id_v4503ReleaseNotes-KnownKnownIssues", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.3 Release Notes \/ Known Issues", 
"snippet" : "PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. RBAC tagcmd does not always work in all environments. (CUSTOMER-592) Special Characters in passwords are not supported. (CUSTOMER-440, CUSTOMER-552) fs image download will not trigger a...", 
"body" : " PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. RBAC tagcmd does not always work in all environments. (CUSTOMER-592) Special Characters in passwords are not supported. (CUSTOMER-440, CUSTOMER-552) fs image download will not trigger after install. (REPORT-262) Queue Analysis graph zooming and resetting not working properly in Edge.(UIX-1589) Cloud Reports: You must select an instance type Hbase is not working when clusterID contains a space, e.g., cluster 25. (HBASE-83) Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.5.0.2 Release Notes", 
"url" : "release-notes/rn-4502.html", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes", 
"snippet" : "Contents Software Version Software Upgrade Support Certified Platforms Supported OS\/Software Supported Browsers Unravel Sensor Upgrade New Features None Improvements and Bug Fixes Improvements Bug Fixes Known Issues v4.5.x - Updates to Unravel Properties...", 
"body" : " Contents Software Version Software Upgrade Support Certified Platforms Supported OS\/Software Supported Browsers Unravel Sensor Upgrade New Features None Improvements and Bug Fixes Improvements Bug Fixes Known Issues v4.5.x - Updates to Unravel Properties " }, 
{ "title" : "Software Version", 
"url" : "release-notes/rn-4502.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-SoftwareVersion", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Software Version", 
"snippet" : "Release Date: 01\/21\/2019 For details on downloading updates see here...", 
"body" : "Release Date: 01\/21\/2019 For details on downloading updates see here " }, 
{ "title" : "Software Upgrade Support", 
"url" : "release-notes/rn-4502.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-SoftwareUpgradeSupport", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Software Upgrade Support", 
"snippet" : "Upgrade from 4.3 Upgrade from 4.4.0...", 
"body" : " Upgrade from 4.3 Upgrade from 4.4.0 " }, 
{ "title" : "Certified Platforms", 
"url" : "release-notes/rn-4502.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-CertifiedPlatforms", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Certified Platforms", 
"snippet" : "Please also review the compatibility matrix CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0...", 
"body" : "Please also review the compatibility matrix CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0 " }, 
{ "title" : "Supported OS\/Software", 
"url" : "release-notes/rn-4502.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-SupportedOSSoftware", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Supported OS\/Software", 
"snippet" : "OS RedHat\/Centos: 6.6-7.5 Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47...", 
"body" : " OS RedHat\/Centos: 6.6-7.5 Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47 " }, 
{ "title" : "SupportedBrowsers", 
"url" : "release-notes/rn-4502.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-SupportedBrowsers", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ SupportedBrowsers", 
"snippet" : "Chrome: 70.x Internet Explorer: IE 11...", 
"body" : " Chrome: 70.x Internet Explorer: IE 11 " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "release-notes/rn-4502.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-SensorUnravelSensorUpgrade", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Unravel Sensor Upgrade", 
"snippet" : "Sensor upgrade is only required for Spark Live View \/ APP Actions Feature....", 
"body" : " Sensor upgrade is only required for Spark Live View \/ APP Actions Feature. " }, 
{ "title" : "New Features", 
"url" : "release-notes/rn-4502.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-NewFeaturesNewFeatures", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ New Features", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "None", 
"url" : "release-notes/rn-4502.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-None", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ None", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "release-notes/rn-4502.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-KnownImprovementsandBugFixes", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Improvements and Bug Fixes", 
"snippet" : "Improvements None Bug Fixes Removed \"NoneType and \"float\" error Cloud Mapping Per Instance or Cloud Mapping Per Host. (CUSTOMER-593) Unravel_km es_clear.sh...", 
"body" : "Improvements None Bug Fixes Removed \"NoneType and \"float\" error Cloud Mapping Per Instance or Cloud Mapping Per Host. (CUSTOMER-593) Unravel_km es_clear.sh " }, 
{ "title" : "Known Issues", 
"url" : "release-notes/rn-4502.html#UUID-c60afffb-e788-20ec-aaa4-3a23c850ff50_id_v4502ReleaseNotes-KnownKnownIssues", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0.2 Release Notes \/ Known Issues", 
"snippet" : "PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. RBAC tagcmd not working in Equifax environment. (CUSTOMER-592) Use of encrypted password (Cloudera manager password) from unravel.properties Special Characters in passwords are not supp...", 
"body" : " PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. RBAC tagcmd not working in Equifax environment. (CUSTOMER-592) Use of encrypted password (Cloudera manager password) from unravel.properties Special Characters in passwords are not supported, (CUSTOMER-440, CUSTOMER-552) Unravel Spark sensor doesn't support clusterId with spaces, (CUSTOMER-59 fs image download will not trigger after install. (REPORT-262 Queue Analysis graph zooming and resetting not working properly in Edge(UIX-1589) may fail to collect metrics for nested queue on HDP when configured to use Fair Scheduler due to a bug in HDP (REPORT-294) Queue metric sensor does not work with remote clusters if RM is not accessible via HTTP or if TLS with authentication is enabled (PLATFORM-927, PLATFORM-1052) Sessions: Auto tune fails to apply recommendations: Error: Failed to retrieve recommendations Cloud Reports: You must select an instance type Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.5.0 Release Notes", 
"url" : "release-notes/rn-4500.html", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes", 
"snippet" : "Contents Software Version Software Upgrade Support Certified Platforms Supported OS\/Software Supported Browsers Unravel Sensor Upgrade New Features Improvements and Bug Fixes Improvements Bug Fixes Known Issues v4.5.x - Updates to Unravel Properties...", 
"body" : " Contents Software Version Software Upgrade Support Certified Platforms Supported OS\/Software Supported Browsers Unravel Sensor Upgrade New Features Improvements and Bug Fixes Improvements Bug Fixes Known Issues v4.5.x - Updates to Unravel Properties " }, 
{ "title" : "Software Version", 
"url" : "release-notes/rn-4500.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-SoftwareVersion", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Software Version", 
"snippet" : "Release Date: 01 \/14\/2019 For details on downloading updates see here...", 
"body" : "Release Date: 01 \/14\/2019 For details on downloading updates see here " }, 
{ "title" : "Software Upgrade Support", 
"url" : "release-notes/rn-4500.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-SoftwareUpgradeSupport", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Software Upgrade Support", 
"snippet" : "Upgrade from 4.3 Upgrade from 4.4.0...", 
"body" : " Upgrade from 4.3 Upgrade from 4.4.0 " }, 
{ "title" : "Certified Platforms", 
"url" : "release-notes/rn-4500.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-CertifiedPlatforms", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Certified Platforms", 
"snippet" : "Please also review the compatibility matrix CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0...", 
"body" : "Please also review the compatibility matrix CDH: On-premise CDH 6.0, 5.15, 5.14 HDP: On-premise HDP 2.6.5, 2.6.4 MapR: 6.1.0, 6.0.0, 5.2.0 " }, 
{ "title" : "Supported OS\/Software", 
"url" : "release-notes/rn-4500.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-SupportedOSSoftware", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Supported OS\/Software", 
"snippet" : "OS RedHat\/Centos: 6.6-7.5 Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47...", 
"body" : " OS RedHat\/Centos: 6.6-7.5 Database MySql: 5.5, 5.6, 5.7(recommended) Database Driver MySql: 5.1.47 " }, 
{ "title" : "Supported Browsers", 
"url" : "release-notes/rn-4500.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-SupportedBrowsers", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Supported Browsers", 
"snippet" : "Chrome: 70.x Internet Explorer: IE 11...", 
"body" : " Chrome: 70.x Internet Explorer: IE 11 " }, 
{ "title" : "Unravel Sensor Upgrade", 
"url" : "release-notes/rn-4500.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-SensorUnravelSensorUpgrade", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Unravel Sensor Upgrade", 
"snippet" : "Sensor upgrade is only required for Spark Live View \/ APP Actions Feature....", 
"body" : " Sensor upgrade is only required for Spark Live View \/ APP Actions Feature. " }, 
{ "title" : "New Features", 
"url" : "release-notes/rn-4500.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-NewFeaturesNewFeatures", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes \/ New Features", 
"snippet" : "Applications Ability to interactively 'Kill' and 'Move' YARN Live view Live update of Total, Completed, Skipped, Active, and Pending Stages for every Spark Job in a Spark application. (CUSTOMER-239) Real-time visibility of running Spark jobs show completed stage, running stage and overall progress. ...", 
"body" : " Applications Ability to interactively 'Kill' and 'Move' YARN Live view Live update of Total, Completed, Skipped, Active, and Pending Stages for every Spark Job in a Spark application. (CUSTOMER-239) Real-time visibility of running Spark jobs show completed stage, running stage and overall progress. (CUSTOMER-111) Insights New Data, operator and SQL-level insights for Impala. New efficiency and failure events for Hive\/Tez. Cloud Reports Added several new API end points. Retrieve app recommendations. (CUSTOMER-396) Get finished and total number of jobs that ran on a particular date range . (CUSTOMER-367) Get status, error, logs, summary of individual Apps. (CUSTOMER-253) * Preview features are in beta and is subject to change. The design and code is less mature than official GA features and is being provided as-is with no warranties. Preview features are not subject to the support SLA of official GA features. We do not recommend you deploy Preview features in production environment " }, 
{ "title" : "Improvements and Bug Fixes", 
"url" : "release-notes/rn-4500.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-KnownImprovementsandBugFixes", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Improvements and Bug Fixes", 
"snippet" : "Improvements Auto-Actions You can add more than 2 rulesets in auto action policy. (CUSTOMER-325) UI shows list of triggered auto actions Link to the offending application is contained in email notification. (CUSTOMER-251) Violation email is now sent to the owner using LDAP email address field. (CUST...", 
"body" : "Improvements Auto-Actions You can add more than 2 rulesets in auto action policy. (CUSTOMER-325) UI shows list of triggered auto actions Link to the offending application is contained in email notification. (CUSTOMER-251) Violation email is now sent to the owner using LDAP email address field. (CUSTOMER-350) Rules for catching named workflows that have missed their SLAs are applied are no longer indiscriminately to all running workflows. (CUSTOMER-400) Operations Chargeback Reports now support Impala queries. (IMPALA-118) Top-X Report now supports Spark and Hive-On-Tez. (REPORT-241) Sessions Improved Spark's session recommendations for bigger containers, effective broadcast size & shuffle partition size. (SESS-128, SESS-129, SESS-130) Spark com.unraveldata.spark.maste com.unraveldata.spark.eventlog.appDuration Reporting OnDemand is now supported via PostgreSQL DB. (REPORT-229) Added support to disable Small Files. (REPORT-204) Small File Report generation is faster. (REPORT-181) Small Files\/ File Reports can run even without dfsadmin privileges. (REPORT-143) Queue Analysis now is interactive, supports MapR disk metrics, and has UI presentation and performance improvements (REPORT-105) Tez Improvements in map tasks recommendations for Hive\/Tez apps (TEZLLAP-47) Removed recommendations to increase the number of tasks for ORDER BY reducer vertices. (TEZLLAP-35) Improvements in hive.tez.auto.reducer.parallelism recommendations for Hive\/Tez apps. (TEZLLAP-37) Improvements in failure events for Hive\/Tez apps. (TEZLLAP-80) Added support for Hive\/Tez on MapR 5.2 and 6.0. (TEZLLAP-103) A failure event is now triggered when a Tez DAG fails. (TEZLLAP-145) Hive queries executed as Tez apps are now connected with their Tez parent app in the UI. (TEZLLAP-199) Hive query metrics are now published to the Auto-Actions framework. (TEZLLAP-206) Impala Input tables used by Impala queries are now displayed in the Data Page. (IMPALA-121) DML statements are now captured. ( IMPALA-166) Cluster name is now available when using impalad as the data source. (IMPALA-78) UIX UI shows list of triggered auto actions on mouse hover over the AA2 badge. (UIX-1249) Queue Analysis Graphs are now interactive and clicking on any of the graphs will open up the cluster view for that point in time with a list of all apps that were running at that time (UIX-1443) Average values for each queue metric is now displayed next to its value when selecting a point on a graph (UIX-1493) Average run apps, used memory, used vcores, and used disk (MaprR only) now are included as columns in the list of analysed queues and can be sorted by these values (UIX-1535) All queue metric labels now have descriptions, when hovering over the metric label a popup window will be shown giving a complete description (UIX-1444) Raw queue metrics now have a retention policy to preventing possibility of the database running out of space, default is 90 days (PLATFORM-945) MapR disk metrics are supported, the 4th graph \"Disk Usage\" will be displayed alongside with Apps, Memory, VCore usage when monitoring MapR clusters (REPORT-220) Rendering performance of graphs was greatly improved in 4.5.0.0 and users should notice much less lag when opening queue metric graphs (UIX-1515) Queue metric collector sensor and queue analysis reports now fully support PostgreSQL as backend storage database alongside with MySQL (REPORT-274) HiveHook Sensors HiveHook Support for Hive 2.2.0 & Hive 2.3.0 Support for Single HiveHook class Bug Fixes Customer Fixes Provide 'path' filter options in Small Files Report. (CUSTOMER-371) Support for `com.unraveldata.rbac.tagcmd` property. (CUSTOMER-545) For inbound HTTPS connections to Unravel UI, provide support to restrict TLS protocols. (CUSTOMER-515) com.unraveldata.ldap.bind_pw=bigsecret needs to support encrypted passwords (CUSTOMER-473) Unravel using global resources to make connections to internet (CUSTOMER-449) Support for RHEL6 with OnDemand Framework (CUSTOMER-435) Spark Spark application\/job progress (CUSTOMER-475) Spark “Load Logs” and “Application Diagnostics” (CUSTOMER-508) Spark-shell is generating unwanted warnings (CUSTOMER-407) Spark execution graph display issue (CUSTOMER-283) Execution tab needs to show unique Stage IDs (CUSTOMER-236) Kafka Kafka Consumer Groups not displaying (CUSTOMER-366) Kafka page not showing all partitions in UI (CUSTOMER-544) Application Page Filters - add ability to select multiple users and multiple queues (CUSTOMER-491) Sorting doesn't take into account MB vs GB vs TB (CUSTOMER-383) Airflow Monitoring does not work consistently (CUSTOMER-304) Workflow compare panel only showing 24 hours worth of data (CUSTOMER-291) Queue analysis reports is empty and sometimes hits 1000 field limit in ES (CUSTOMER-471) Sessions Recommendations generated by Application are now c onsistent Inside and Outside Sessions (SESS-138) CMP API issue picks up data ~4 days later than the earliest data point viewable. ( SESS-60) Auto Actions Auto actions are not triggering for long running Hive apps with Tez as Execution Engine (PLATFORM-643) Reporting TOP-X UI Stability Fixes (REPORT-263) Error handling Improvements for Small Files \/ File Reports (REPORT-235) Small Files Data Inconsistencies Improvements (REPORT-209) DFS Forecasting Report has the wrong AMS URL. (REPORT-246) Tez Tez apps are now visible in the UI regardless of missing or incorrect ATS configuration (TEZLLAP-50) A Hive\/Tez query is no longer associated with multiple DAGs in the Unravel UI. (TEZLLAP-198) Fixed NullPointerException raised in unravel_ew_1.log Fixed NumberFormatException raised when tez.am.grouping.max-size tez.grouping.max-size Fixed NullPointerException raised when dagMap is not populated. (TEZLLAP-219) Fixed NumberFormatException raised in MapR 6.0 when tez.grouping.min-size Tez apps are no longer stuck in running state in MapR 6.0. (TEZLLAP-231) Impala HTTP connection to CM will no longer wait indefinitely (IMPALA-139) Short-running Impala queries are no longer missed (IMPALA-165) Start and end query times use the correct timezone when using impalad as the data source (IMPALA-164) UIX Queue Analysis graphs are taking long time to render (UIX-1515) Added Report Archives Support for PostgresSQL. (REPORT-257) Added appsRun metric on queue analysis graph (UIX-1476) Global search no longer returns multiple results for single Hive Query ID. (UIX-1224) Delay when opening up a new workflow instance panel by clicking on Compare graph. (UIX-1357) Spark Major performance improvement in compressed Event log parsing. (USPARK-164 ) Queue Analysis MapR queue analysis report is missing Disk metrics. (REPORT-220) Queue metric sensor is not collecting any data if PostgreSQL is used. (REPORT-274) ES reports index limit was increased to 10000. (PLATFORM-821) Queue Analysis report stuck at putting data to ES. (REPORT-291) " }, 
{ "title" : "Known Issues", 
"url" : "release-notes/rn-4500.html#UUID-7081db48-e69a-eb5d-605b-8ed45bfb5920_id_v450ReleaseNotes-KnownKnownIssues", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.0 Release Notes \/ Known Issues", 
"snippet" : "PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. Special Characters in passwords are not supported (CUSTOMER-440, CUSTOMER-552) fs image download will not trigger after install. (REPORT-262 Queue Analysis graph zooming and resetting n...", 
"body" : " PostgreSQL DB is now usable across all features. However we do not recommend it for production usage at this time. Special Characters in passwords are not supported (CUSTOMER-440, CUSTOMER-552) fs image download will not trigger after install. (REPORT-262 Queue Analysis graph zooming and resetting not working properly in Edge ( UIX-1589 ) may fail to collect metrics for nested queue on HDP when configured to use Fair Scheduler due to a bug in HDP (REPORT-294) Queue metric sensor does not work with remote clusters if RM is not accessible via HTTP or TLS with authentication is enabled (PLATFORM-927, PLATFORM-1052) Sessions: Auto tune fails to apply recommendations: Error: Failed to retrieve recommendations Cloud Reports: You must select an instance type Please write to us at support@unraveldata.com " }, 
{ "title" : "v4.5.x - Upgrade Instructions", 
"url" : "release-notes/rn-upgrade.html", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.x - Upgrade Instructions", 
"snippet" : "Table of Contents Supported Installs\/Upgrade PostgreSQL is now bundled with Unravel instead of MySQL. We recommend using MySQL as Unravel has added a variety of reports Verify the database you are using before proceeding with the upgrade. If you are upgrading from 4.2.x or 4.3.x you should already b...", 
"body" : " Table of Contents Supported Installs\/Upgrade PostgreSQL is now bundled with Unravel instead of MySQL. We recommend using MySQL as Unravel has added a variety of reports Verify the database you are using before proceeding with the upgrade. If you are upgrading from 4.2.x or 4.3.x you should already be using MySQL either as an internal or external database 4.4.x you might be using with PostgreSQL which was bundled " }, 
{ "title" : "The following upgrade options arenotsupported:", 
"url" : "release-notes/rn-upgrade.html#UUID-d493bd0e-f376-fd43-d0ea-31b44aef8c12_id_v45x-UpgradeInstructions-Thefollowingupgradeoptionsarenotsupported", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.x - Upgrade Instructions \/ The following upgrade options arenotsupported:", 
"snippet" : "Upgrade from 4.3.x embedded MySQL to 4.5 using embedded PostgreSQL external MySQL to 4.5 using embedded PostgreSQL Upgrade from 4.4.x embedded PostgreSQL to 4.5 using external MySQL external MySQL to 4.5 using embedded PostgreSQL...", 
"body" : " Upgrade from 4.3.x embedded MySQL to 4.5 using embedded PostgreSQL external MySQL to 4.5 using embedded PostgreSQL Upgrade from 4.4.x embedded PostgreSQL to 4.5 using external MySQL external MySQL to 4.5 using embedded PostgreSQL " }, 
{ "title" : "Supported Installs\/Upgrade", 
"url" : "release-notes/rn-upgrade.html#UUID-d493bd0e-f376-fd43-d0ea-31b44aef8c12_id_v45x-UpgradeInstructions-SupportedInstallsUpgrade", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.x - Upgrade Instructions \/ Supported Installs\/Upgrade", 
"snippet" : "Fresh Install Using MySQL (recommended) Follow these pre-install instructions to install and configure MySQL Using PostgreSQL (bundled) proceed with install Download and Install RPM Download RPM Install RPM Post-Install steps if using MySQL Follow these post-install instructions to configure Unravel...", 
"body" : "Fresh Install Using MySQL (recommended) Follow these pre-install instructions to install and configure MySQL Using PostgreSQL (bundled) proceed with install Download and Install RPM Download RPM Install RPM Post-Install steps if using MySQL Follow these post-install instructions to configure Unravel for MySQL Upgrade from 4.4.x If your database is MySQL If the MySQL data has not been migrated to the new partitioned tables, then follow all the steps in Upgrade from 4.3.1.X If MySQL data is already migrated to partitions, then follow all the steps in Upgrade from 4.3.1.X section but skip the \" MySql Partitioning and Data Migration To check if the tables are already partitioned, execute the following command: # echo \"show table status like '%blackboards%'\" | \/usr\/local\/unravel\/install_bin\/db_access.sh If your database is PostgreSQL Install RPM If MySQL server is running as unravel_db unravel_all.sh unravel_db sed -i 's\/unravel_pg\/unravel_db\/g' \/etc\/init.d\/unravel_all.sh Upgrade from 4.3.x Embedded\/external MySQL to 4.5 using external MySQL Unravel 4.5 uses partitioned MySQL tables to manage the disk space and you must prepare for the upgrade. Prepare for upgrade Check amount of disk space used by MySQL (Space_Used) via the CLI with the command Embedded MySQL # du -sh \/srv\/unravel\/db_data External MySQL # grep \"^datadir=\" \/etc\/my.cnf | awk -F\"[=]\" '{print $2}' | xargs du -sh Check amount of disk space available in the database partition (Space_Available) via the CLI with the command Embedded MySQL # df -h \/srv\/unravel\/db_data External MySQL # grep \"^datadir=\" \/etc\/my.cnf | awk -F\"[=]\" '{print $2}' | xargs df -h If Space_Used > Space_Available then follow instructions to move MySQL Copy the MySQL JDBC JAR to \/usr\/local\/unravel\/share\/java\/ # mkdir -p \/usr\/local\/unravel\/share\/java\n# sudo cp \/usr\/local\/unravel\/dlib\/mybatis\/mysql-connector*.jar \/usr\/local\/unravel\/share\/java Upgrade Download RPM Install RPM Complete the instructions at MySql Partitioning and Data Migration RBAC properties have been changed. See here If MySQL server is running as unravel_db unravel_all.sh unravel_db sed -i 's\/unravel_pg\/unravel_db\/g' \/etc\/init.d\/unravel_all.sh Upgrade from 4.2.x 4.2.x (embedded MySQL) -> 4.5 (external MySQL) 4.2.x (external MySQL) -> 4.5 (external MySQL) Upgrade to 4.3.1.7 then follow the instructions above " }, 
{ "title" : "v4.5.x - Updates to Unravel Properties", 
"url" : "release-notes/rn-upgrade-properties.html", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.x - Updates to Unravel Properties", 
"snippet" : "This page is lists Unravel configuration properties added, renamed, removed, and deprecated in 4.5. See Unravel Properties Table of Contents New LDAP Custom Banner HBASE Hive-hook SSL Small Files and File Reports Small Files File Reports Forecasting & Cloud Reports Spark Application Liveness Experim...", 
"body" : " This page is lists Unravel configuration properties added, renamed, removed, and deprecated in 4.5. See Unravel Properties Table of Contents New LDAP Custom Banner HBASE Hive-hook SSL Small Files and File Reports Small Files File Reports Forecasting & Cloud Reports Spark Application Liveness Experimental Renamed\/Replaced Properties Hive RBAC " }, 
{ "title" : "New", 
"url" : "release-notes/rn-upgrade-properties.html#UUID-279a7fca-a033-8e79-9a85-8ae9fb085435_id_v45x-UpdatestoUnravelProperties-New", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.x - Updates to Unravel Properties \/ New", 
"snippet" : "Property Description Default LDAP com.unraveldata.ldap.mailAttribute The mail attribute name in the LDAP response that Unravel server will use to extract the ldap user's email address. If not configured, Unravel server use use the attribute name \"mail\". - com.unraveldata.ldap.customLDAPQuery replace...", 
"body" : " Property Description Default LDAP com.unraveldata.ldap.mailAttribute The mail attribute name in the LDAP response that Unravel server will use to extract the ldap user's email address. If not configured, Unravel server use use the attribute name \"mail\". - com.unraveldata.ldap.customLDAPQuery replaced hive.server2.authentication.ldap.customLDAPQuery com.unraveldata.ldap.groupFilter replaced hive.server2.authentication.ldap.groupFilter com.unraveldata.ldap.groupDNPattern replaced hive.server2.authentication.ldap.groupDNPattern com.unraveldata.ldap.guidKey replaced hive.server2.authentication.ldap.guidKey=uid com.unraveldata.ldap.userDNPattern replaced hive.server2.authentication.ldap.userDNPattern com.unraveldata.ldap.userFilter replaced hive.server2.authentication.ldap.userFilter com.unraveldata.ldap.groupMembershipKey replaced hive.server2.authentication.ldap.groupMembershipKey com.unraveldata.ldap.groupClassKey replaced hive.server2.authentication.ldap.groupClassKey Custom Banner com.unraveldata.custom.banner.display Displays a banner at the top of the Unravel UI. True text end.date False - com.unraveldata.custom.banner.text Text to display when display - com.unraveldata.custom.banner.end.date Date and Time to stop displaying the custom banner Format: YYYYMMDD T Z - HBASE (See here com.unraveldata.hbase. clustername HBase node web UI. Format: http[s]:\/\/host:port,http[s]:\/\/host:port,... * Example: http:\/\/your.master.server:16010,http:\/\/your.region.server:16030 - Hive-hook SSL (See here com.unraveldata.live.logreceiver.port.https HTTPS server port (negative value means disabled HTTPS server) -1 com.unraveldata.server.ssl.cert_path KeyStore file path, e.g., \/usr\/local\/unravel\/cert.jks - com.unraveldata.server.ssl.cert_password KeyStore password - com.unraveldata.server.ssl.trust_store_path TrustStore file path - com.unraveldata.server.ssl.trust_store_password TrustStore password - Small Files and File Reports (See here unravel.python.files_use_avg_file_size_flag true false - unravel.python.min_parent_dir_depth Directory depth to start search at. - unravel.python.max_parent_dir_depth Directory depth to end search at. Maximum is 50. - unravel.python.drill_down_subdirs_flag When set a file is accounted (listed) for all it's ancestors. false - Small Files (See here unravel.python.reports.files.small_files_use_avg_file_size_flag See here true unravel.python.reports.files.small_files_min_parent_dir_depth See here 0 unravel.python.reports.files.small_files_max_parent_dir_depth See here 10 reports.files.small_files_drill_down_subdirs_flag See here true File Reports (See here The following four properties are defined per file size Size unravel.python.reports.files. Size See here false unravel.python.reports.files. Size See here 0 unravel.python.reports.files. Size See here 10 unravel.python.reports.files. Size See here false Forecasting & Cloud Reports (see here com.unraveldata.ambari.manager.url URL of cloud manger, e.g., http:\/\/$clouderaserver:7180, http:\/\/$ambariserver:8080 For Cloudera, if the URL does not contain a port you must define manager.port - com.unraveldata.ambari.manager.username Username to log into the manager - com.unraveldata.ambari.manager.password Password for the username. - Spark com.unraveldata.spark.master Default spark master mode to be used if not available from Sensor com.unraveldata.spark.eventlog.appDuration If application duration is more than configured value load the event log is not loaded. 1440 mins Application Liveness com.unraveldata.appstatus.refresh.mins Time interval in minutes to scan for running applications and marking the stale ones. 5 mins com.unraveldata.appstatus.stale_limit.mins Maximum number of minutes since the latest app update from RM before it is marked as stale. 10 mins Experimental Experimental features (not to be used in production) com.unraveldata.cluster_access.host Cluster Access Service host (where the service will be bound) 0.0.0.0 com.unraveldata.cluster_access.port Cluster Access Service port 4020 com.unraveldata.sregistry.hostport Service Registry host:port ${com.unraveldata.zk.quorum} com.unraveldata.sensor.polling.secs The base polling period of Unravel reactive sensors in seconds. 30s com.unraveldata.appevents.emitters.exclude.list Comma separated list of the application event emitter IDs which will be disabled\/excluded. - com.unraveldata.multicluster.enabled Allow Unravel to operate in multi-cluster mode. In this mode a service registry will be used to discover and access all registered (local and remote) clusters. false " }, 
{ "title" : "Renamed\/Replaced Properties", 
"url" : "release-notes/rn-upgrade-properties.html#UUID-279a7fca-a033-8e79-9a85-8ae9fb085435_id_v45x-UpdatestoUnravelProperties-RenamedReplacedProperties", 
"breadcrumbs" : "Unravel 4.5 \/ Release Notes \/ v4.5.x - Updates to Unravel Properties \/ Renamed\/Replaced Properties", 
"snippet" : "Hive (See LDAP Property Replaced with Hive authentication hive.server2.authentication.ldap.customLDAPQuery com.unraveldata.ldap.customLDAPQuery hive.server2.authentication.ldap.groupFilter com.unraveldata.ldap.groupFilter hive.server2.authentication.ldap.groupDNPattern com.unraveldata.ldap.groupDNPa...", 
"body" : "Hive (See LDAP Property Replaced with Hive authentication hive.server2.authentication.ldap.customLDAPQuery com.unraveldata.ldap.customLDAPQuery hive.server2.authentication.ldap.groupFilter com.unraveldata.ldap.groupFilter hive.server2.authentication.ldap.groupDNPattern com.unraveldata.ldap.groupDNPattern hive.server2.authentication.ldap.guidKey=uid com.unraveldata.ldap.guidKey hive.server2.authentication.ldap.userDNPattern com.unraveldata.ldap.userDNPattern hive.server2.authentication.ldap.userFilter com.unraveldata.ldap.userFilter hive.server2.authentication.ldap.groupMembershipKey com.unraveldata.ldap.groupMembershipKey hive.server2.authentication.ldap.groupClassKey com.unraveldata.ldap.groupClassKey RBAC You must update these properties manually. Property Replaced with com.unraveldata.rbac.mode com.unraveldata.login.mode com.unraveldata.rbac.user.operations.enabled com.unraveldata.ngui.user.mode " }
]
$(document).trigger('search.ready');
});